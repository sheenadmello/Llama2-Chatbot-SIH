{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsANFlp2qPVG"
      },
      "source": [
        "## **What will this chatbot solve?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6zDUSpNqPVI"
      },
      "source": [
        "- **Research Paper Query Engine** - This feature cuts down the time you might spend searching through endless academic papers, helping you quickly get the information about the data science concepts that's most relevant to your project.\n",
        "\n",
        "- **Graph Paper Relationship Engine** - This tool helps you see the connections between different research studies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ch_Mb5vlqPVJ"
      },
      "source": [
        "# **0. Loading the Gemma model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-12T21:05:44.203201Z",
          "iopub.status.busy": "2024-05-12T21:05:44.202824Z",
          "iopub.status.idle": "2024-05-12T21:06:45.965935Z",
          "shell.execute_reply": "2024-05-12T21:06:45.964653Z",
          "shell.execute_reply.started": "2024-05-12T21:05:44.203174Z"
        },
        "id": "NcRgv9vtqPVJ"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "# # !mamba install cudatoolkit xformers bitsandbytes pytorch pytorch-cuda=11.8 \\\n",
        "# #     -c pytorch -c nvidia -c xformers -c conda-forge -y\n",
        "# !pip install \"unsloth[kaggle] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "# !pip uninstall datasets -y\n",
        "# !pip install datasets\n",
        "\n",
        "# import os\n",
        "# os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-12T21:06:45.968409Z",
          "iopub.status.busy": "2024-05-12T21:06:45.968118Z",
          "iopub.status.idle": "2024-05-12T21:14:22.007833Z",
          "shell.execute_reply": "2024-05-12T21:14:22.006481Z",
          "shell.execute_reply.started": "2024-05-12T21:06:45.968382Z"
        },
        "id": "DzwkblfBqPVK"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "# !pip install xformers==0.0.25.post1\n",
        "# !pip install bitsandbytes\n",
        "# !pip install llama-index\n",
        "# !pip install llama-index-embeddings-huggingface\n",
        "# !pip install llama-index-vector-stores-chroma\n",
        "# !pip install chromadb\n",
        "# !pip install llama-index-llms-huggingface\n",
        "# !pip install llama-index-llms-groq\n",
        "# !pip install sentence-transformers\n",
        "# !pip install pyvis==0.3.1\n",
        "# !pip install torch=2.2.2\n",
        "# !pip install -U wikipedia-api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8JvBI2zqPVL"
      },
      "outputs": [],
      "source": [
        "# from unsloth import FastLanguageModel\n",
        "# import torch\n",
        "# max_seq_length = 1024 # Choose any! We auto support RoPE Scaling internally!\n",
        "# dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "# load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "# # 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "# fourbit_models = [\n",
        "#     \"unsloth/mistral-7b-bnb-4bit\",\n",
        "#     \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
        "#     \"unsloth/llama-2-7b-bnb-4bit\",\n",
        "#     \"unsloth/gemma-7b-bnb-4bit\",\n",
        "#     \"unsloth/gemma-7b-it-bnb-4bit\", # Instruct version of Gemma 7b\n",
        "#     \"unsloth/gemma-2b-bnb-4bit\",\n",
        "#     \"unsloth/gemma-2b-it-bnb-4bit\", # Instruct version of Gemma 2b\n",
        "# ] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "# model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "#     model_name = \"unsloth/gemma-7b-bnb-4bit\", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
        "#     max_seq_length = max_seq_length,\n",
        "#     dtype = dtype,\n",
        "#     load_in_4bit = load_in_4bit,\n",
        "#     # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cVTU52sqPVL"
      },
      "outputs": [],
      "source": [
        "# from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "\n",
        "# from unsloth import FastLanguageModel\n",
        "# import torch\n",
        "# max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "# dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "\n",
        "# gemma_llm = HuggingFaceLLM(model=model, tokenizer=tokenizer, context_window=8192, max_new_tokens=max_seq_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVksB-M0qPVL"
      },
      "source": [
        "Let's test the `gemma_llm`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75uULmRIqPVM"
      },
      "outputs": [],
      "source": [
        "# Markdown(gemma_llm.complete(\"What is data science?\").text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tf-henNjqPVM"
      },
      "source": [
        "# **1. Scientific Research Assistant**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1k4VoLnlqPVM"
      },
      "source": [
        "In this section, we're focusing on creating the first part of our chatbot: a tool that can search through a huge number of research papers on arXiv. The key to this tool is using embeddings, taken from paper abstracts. Think of these as unique IDs that sum up what each paper is about.\n",
        "\n",
        "When you ask the chatbot something, it uses these embeddings to look through the abstracts and find papers that really match what you're looking for, not just by keywords, but by the actual ideas and concepts you're interested in. This is more about understanding the meaning of your question and finding papers that really match.\n",
        "\n",
        "We'll go through everything: picking the right papers from arXiv, getting the abstracts ready, and choosing a way to turn these abstracts into embeddings. Then, we'll set up a smart search that can quickly find the best matches when you ask a question."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPzO6LO7qPVN"
      },
      "source": [
        "## **1.1 Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPzG2HWhqPVN",
        "outputId": "fe09a74e-2c91-44fd-972d-0082fbd1ad6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2459562, 4)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>abstract</th>\n",
              "      <th>categories</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0704.0001</td>\n",
              "      <td>Calculation of prompt diphoton production cros...</td>\n",
              "      <td>A fully differential calculation in perturba...</td>\n",
              "      <td>hep-ph</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0704.0002</td>\n",
              "      <td>Sparsity-certifying Graph Decompositions</td>\n",
              "      <td>We describe a new algorithm, the $(k,\\ell)$-...</td>\n",
              "      <td>math.CO cs.CG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0704.0003</td>\n",
              "      <td>The evolution of the Earth-Moon system based o...</td>\n",
              "      <td>The evolution of Earth-Moon system is descri...</td>\n",
              "      <td>physics.gen-ph</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0704.0004</td>\n",
              "      <td>A determinant of Stirling cycle numbers counts...</td>\n",
              "      <td>We show that a determinant of Stirling cycle...</td>\n",
              "      <td>math.CO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0704.0005</td>\n",
              "      <td>From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...</td>\n",
              "      <td>In this paper we show how to compute the $\\L...</td>\n",
              "      <td>math.CA math.FA</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          id                                              title  \\\n",
              "0  0704.0001  Calculation of prompt diphoton production cros...   \n",
              "1  0704.0002           Sparsity-certifying Graph Decompositions   \n",
              "2  0704.0003  The evolution of the Earth-Moon system based o...   \n",
              "3  0704.0004  A determinant of Stirling cycle numbers counts...   \n",
              "4  0704.0005  From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...   \n",
              "\n",
              "                                            abstract       categories  \n",
              "0    A fully differential calculation in perturba...           hep-ph  \n",
              "1    We describe a new algorithm, the $(k,\\ell)$-...    math.CO cs.CG  \n",
              "2    The evolution of Earth-Moon system is descri...   physics.gen-ph  \n",
              "3    We show that a determinant of Stirling cycle...          math.CO  \n",
              "4    In this paper we show how to compute the $\\L...  math.CA math.FA  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# https://www.kaggle.com/code/matthewmaddock/nlp-arxiv-dataset-transformers-and-umap\n",
        "\n",
        "# This takes about 1 minute.\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "cols = ['id', 'title', 'abstract', 'categories']\n",
        "data = []\n",
        "file_name = '/home/mahita/fyp/input/arxiv-metadata-oai-snapshot.json'\n",
        "\n",
        "\n",
        "with open(file_name, encoding='latin-1') as f:\n",
        "    for line in f:\n",
        "        doc = json.loads(line)\n",
        "        lst = [doc['id'], doc['title'], doc['abstract'], doc['categories']]\n",
        "        data.append(lst)\n",
        "\n",
        "df_data = pd.DataFrame(data=data, columns=cols)\n",
        "\n",
        "print(df_data.shape)\n",
        "\n",
        "df_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C96977BkqPVO"
      },
      "source": [
        "There are a total of almost 2,5M papers on arxiv, that's too much! However, not all of them are about AI, so let's narrow down to the topics we're interested in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYsBIAWaqPVO"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# topics = ['cs.AI', 'cs.CV', 'cs.IR', 'cs.LG', 'cs.CL']\n",
        "\n",
        "# # Create a regular expression pattern that matches any of the topics\n",
        "# # The pattern will look like 'cs.AI|cs.CV|cs.IR|cs.LG|cs.CL'\n",
        "# pattern = '|'.join(topics)\n",
        "\n",
        "# # Filter the DataFrame to include rows where the 'categories' column contains any of the topics\n",
        "# # The na=False parameter makes sure that NaN values are treated as False\n",
        "# df_filtered = df_data[df_data['categories'].str.contains(pattern, na=False)]\n",
        "\n",
        "# # Display the filtered DataFrame\n",
        "# df_filtered"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_x44emPcqPVO"
      },
      "source": [
        "Great! Now we down to about 330K papers. Now, let's clean the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pssXWvIPqPVO"
      },
      "outputs": [],
      "source": [
        "# df_filtered.iloc[110]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3c9rTw7RqPVP",
        "outputId": "4259d7df-0436-43f6-c4a4-dc721efbc1d3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>abstract</th>\n",
              "      <th>categories</th>\n",
              "      <th>prepared_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0704.0001</td>\n",
              "      <td>Calculation of prompt diphoton production cros...</td>\n",
              "      <td>A fully differential calculation in perturbati...</td>\n",
              "      <td>hep-ph</td>\n",
              "      <td>Calculation of prompt diphoton production cros...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0704.0002</td>\n",
              "      <td>Sparsity-certifying Graph Decompositions</td>\n",
              "      <td>We describe a new algorithm, the $(k,\\ell)$-pe...</td>\n",
              "      <td>math.CO cs.CG</td>\n",
              "      <td>Sparsity-certifying Graph Decompositions\\n We ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0704.0003</td>\n",
              "      <td>The evolution of the Earth-Moon system based o...</td>\n",
              "      <td>The evolution of Earth-Moon system is describe...</td>\n",
              "      <td>physics.gen-ph</td>\n",
              "      <td>The evolution of the Earth-Moon system based o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0704.0004</td>\n",
              "      <td>A determinant of Stirling cycle numbers counts...</td>\n",
              "      <td>We show that a determinant of Stirling cycle n...</td>\n",
              "      <td>math.CO</td>\n",
              "      <td>A determinant of Stirling cycle numbers counts...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0704.0005</td>\n",
              "      <td>From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...</td>\n",
              "      <td>In this paper we show how to compute the $\\Lam...</td>\n",
              "      <td>math.CA math.FA</td>\n",
              "      <td>From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          id                                              title  \\\n",
              "0  0704.0001  Calculation of prompt diphoton production cros...   \n",
              "1  0704.0002           Sparsity-certifying Graph Decompositions   \n",
              "2  0704.0003  The evolution of the Earth-Moon system based o...   \n",
              "3  0704.0004  A determinant of Stirling cycle numbers counts...   \n",
              "4  0704.0005  From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...   \n",
              "\n",
              "                                            abstract       categories  \\\n",
              "0  A fully differential calculation in perturbati...           hep-ph   \n",
              "1  We describe a new algorithm, the $(k,\\ell)$-pe...    math.CO cs.CG   \n",
              "2  The evolution of Earth-Moon system is describe...   physics.gen-ph   \n",
              "3  We show that a determinant of Stirling cycle n...          math.CO   \n",
              "4  In this paper we show how to compute the $\\Lam...  math.CA math.FA   \n",
              "\n",
              "                                       prepared_text  \n",
              "0  Calculation of prompt diphoton production cros...  \n",
              "1  Sparsity-certifying Graph Decompositions\\n We ...  \n",
              "2  The evolution of the Earth-Moon system based o...  \n",
              "3  A determinant of Stirling cycle numbers counts...  \n",
              "4  From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def clean_text(x):\n",
        "\n",
        "    # Replace newline characters with a space\n",
        "    new_text = \" \".join([c.strip() for c in x.replace(\"\\n\", \"\").split()])\n",
        "    # Remove leading and trailing spaces\n",
        "    new_text = new_text.strip()\n",
        "\n",
        "    return new_text\n",
        "\n",
        "df_data['title'] = df_data['title'].apply(clean_text)\n",
        "df_data['abstract'] = df_data['abstract'].apply(clean_text)\n",
        "\n",
        "df_data['prepared_text'] = df_data['title'] + '\\n ' + df_data['abstract']\n",
        "df_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qbn-TUeNqPVP"
      },
      "outputs": [],
      "source": [
        "# from llama_index.core import Document\n",
        "\n",
        "# arxiv_documents = [Document(text=prepared_text, doc_id=id) for prepared_text,id in list(zip(df_data['prepared_text'], df_data['id']))]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Otp4b9szqPVP"
      },
      "source": [
        "## **1.2 Creating Index**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDeqq-r6qPVP"
      },
      "source": [
        "The `VectorStoreIndex` is by far the most frequently used type of Index in llamaindex. This class takes your Documents and splits them up into Nodes. Then, it creates `vector_embeddings` of the text of every node. But what is `vector_embedding`?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIuFCr5LqPVQ"
      },
      "source": [
        "Vector embeddings are like turning the essence of your words into a mathematical sketch. Imagine every idea or concept in your text getting its unique numerical fingerprint. This is handy because even if two snippets of text use different words, if they're sharing the same idea, their numerical sketches—or embeddings—will be close neighbors in the numerical space. This magic is done using tools known as embedding models.\n",
        "\n",
        "Choosing the right embedding model is crucial. It's like picking the right artist to paint your portrait; you want the one who captures you best. A great place to start is the MTEB leaderboard, where the crème de la crème of embedding models are ranked. As we have quite a large dataset, the model size matters, we don't want to wait all day for the model to extract all the vector embeddings. When I last checked, the `BAAI/bge-small-en-v1.5` model was leading the pack, especially considering its size. It could be a solid choice if you're diving into the world of text embeddings.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7xljkF3qPVQ"
      },
      "source": [
        "Note: *The embedding process may take a considerable amount of time. To save time, I have precomputed it. If you wish to execute the process from the beginning, uncomment the following code.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lsf8mGwsqPVQ"
      },
      "outputs": [],
      "source": [
        "# from llama_index.core import VectorStoreIndex\n",
        "# from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "# from llama_index.core import Settings\n",
        "# import chromadb\n",
        "# import torch\n",
        "# from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "# from llama_index.core import StorageContext\n",
        "\n",
        "# Settings.llm = None\n",
        "# # Create embed model\n",
        "# device_type = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\", cache_folder=\"../models\", device=device_type)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwnGFsTVqPVR"
      },
      "source": [
        "Great! Now we have to find somewhere to store all of the embeddings extracted by the model, and that's why we need a `vector store`. There are many to choose from, in this tutorial, I will choose the `chroma` vector store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdLMX5G5qPVR"
      },
      "outputs": [],
      "source": [
        "# chroma_client = chromadb.PersistentClient(path=\"../DB/arxiv\")\n",
        "# chroma_collection = chroma_client.get_or_create_collection(\"gemma_assistant_arxiv_papers\")\n",
        "\n",
        "\n",
        "# # Create vector store\n",
        "# vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "# storage_context = StorageContext.from_defaults(vector_store=vector_store)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXqaWsB5qPVR"
      },
      "source": [
        "Note: This part takes quite a lot of time! So I precomputed the embedding and store them into chroma db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKsmtZubqPVR"
      },
      "outputs": [],
      "source": [
        "# index = VectorStoreIndex.from_documents(\n",
        "#     arxiv_documents, storage_context=storage_context, embed_model=embed_model, show_progress=True\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqW7K8qeqPVR"
      },
      "source": [
        "## **1.3 Loading from arxiv vector store**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "En0ms29IqPVR"
      },
      "source": [
        "Now, let's load the precomputed embeddings!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ms9x9VnaqPVS"
      },
      "outputs": [],
      "source": [
        "# !cp -r /kaggle/input/gemma-assistant-db ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSrDZ1pHqPVS"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex, Settings\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "import chromadb\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "from llama_index.core import StorageContext\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UoSKgHOqPVS",
        "outputId": "62926b69-a098-4076-ba13-413d41e71c9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLM is explicitly disabled. Using MockLLM.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/torch/cuda/__init__.py:141: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11060). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
            "  return torch._C._cuda_getDeviceCount() > 0\n",
            "/opt/conda/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/opt/conda/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "Settings.llm = None # Set this to none to make the index only do retrieval\n",
        "# device_type = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\", cache_folder=\"../models\", device=device_type) # must be the same as the previous stage\n",
        "\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\") # must be the same as the previous stage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdBs5eRVqPVS"
      },
      "outputs": [],
      "source": [
        "chroma_client = chromadb.PersistentClient(path=\"/home/mahita/fyp/input/arxiv\")\n",
        "chroma_collection = chroma_client.get_or_create_collection(\"gemma_assistant_arxiv_papers\")\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "# load the vectorstore\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "paper_index = VectorStoreIndex.from_vector_store(vector_store, storage_context=storage_context, embed_model=embed_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIidsw2EqPVT"
      },
      "outputs": [],
      "source": [
        "paper_query_engine = paper_index.as_query_engine(\n",
        "    similarity_top_k=10\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNloF5ghqPVT",
        "outputId": "b1b9ced4-fbda-4714-862d-0c1866bdaee2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context information is below.\n",
            "---------------------\n",
            "fake news detection on social media: a data mining perspective\n",
            " Social media for news consumption is a double-edged sword. On the one hand,its low cost, easy access, and rapid dissemination of information lead peopleto seek out and consume news from social media. On the other hand, it enablesthe wide spread of \"fake news\", i.e., low quality news with intentionally falseinformation. The extensive spread of fake news has the potential for extremelynegative impacts on individuals and society. Therefore, fake news detection onsocial media has recently become an emerging research that is attractingtremendous attention. Fake news detection on social media presents uniquecharacteristics and challenges that make existing detection algorithms fromtraditional news media ineffective or not applicable. First, fake news isintentionally written to mislead readers to believe false information, whichmakes it difficult and nontrivial to detect based on news content; therefore,we need to include auxiliary information, such as user social engagements onsocial media, to help make a determination. Second, exploiting this auxiliaryinformation is challenging in and of itself as users' social engagements withfake news produce data that is big, incomplete, unstructured, and noisy.Because the issue of fake news detection on social media is both challengingand relevant, we conducted this survey to further facilitate research on theproblem. In this survey, we present a comprehensive review of detecting fakenews on social media, including fake news characterizations on psychology andsocial theories, existing algorithms from a data mining perspective, evaluationmetrics and representative datasets. We also discuss related research areas,open problems, and future research directions for fake news detection on socialmedia.\n",
            "\n",
            "fake news early detection: an interdisciplinary study\n",
            " Massive dissemination of fake news and its potential to erode democracy hasincreased the demand for accurate fake news detection. Recent advancements inthis area have proposed novel techniques that aim to detect fake news byexploring how it propagates on social networks. Nevertheless, to detect fakenews at an early stage, i.e., when it is published on a news outlet but not yetspread on social media, one cannot rely on news propagation information as itdoes not exist. Hence, there is a strong need to develop approaches that candetect fake news by focusing on news content. In this paper, a theory-drivenmodel is proposed for fake news detection. The method investigates news contentat various levels: lexicon-level, syntax-level, semantic-level anddiscourse-level. We represent news at each level, relying on well-establishedtheories in social and forensic psychology. Fake news detection is thenconducted within a supervised machine learning framework. As aninterdisciplinary research, our work explores potential fake news patterns,enhances the interpretability in fake news feature engineering, and studies therelationships among fake news, deception/disinformation, and clickbaits.Experiments conducted on two real-world datasets indicate the proposed methodcan outperform the state-of-the-art and enable fake news early detection whenthere is limited content information.\n",
            "\n",
            "fake news detection through graph-based neural networks: a survey\n",
            " The popularity of online social networks has enabled rapid dissemination ofinformation. People now can share and consume information much more rapidlythan ever before. However, low-quality and/or accidentally/deliberately fakeinformation can also spread rapidly. This can lead to considerable and negativeimpacts on society. Identifying, labelling and debunking online misinformationas early as possible has become an increasingly urgent problem. Many methodshave been proposed to detect fake news including many deep learning andgraph-based approaches. In recent years, graph-based methods have yieldedstrong results, as they can closely model the social context and propagationprocess of online news. In this paper, we present a systematic review of fakenews detection studies based on graph-based and deep learning-based techniques.We classify existing graph-based methods into knowledge-driven methods,propagation-based methods, and heterogeneous social context-based methods,depending on how a graph structure is constructed to model news relatedinformation flows. We further discuss the challenges and open problems ingraph-based fake news detection and identify future research directions.\n",
            "\n",
            "some like it hoax: automated fake news detection in social networks\n",
            " In recent years, the reliability of information on the Internet has emergedas a crucial issue of modern society. Social network sites (SNSs) haverevolutionized the way in which information is spread by allowing users tofreely share content. As a consequence, SNSs are also increasingly used asvectors for the diffusion of misinformation and hoaxes. The amount ofdisseminated information and the rapidity of its diffusion make it practicallyimpossible to assess reliability in a timely manner, highlighting the need forautomatic hoax detection systems. As a contribution towards this objective, we show that Facebook posts can beclassified with high accuracy as hoaxes or non-hoaxes on the basis of the userswho \"liked\" them. We present two classification techniques, one based onlogistic regression, the other on a novel adaptation of boolean crowdsourcingalgorithms. On a dataset consisting of 15,500 Facebook posts and 909,236 users,we obtain classification accuracies exceeding 99% even when the training setcontains less than 1% of the posts. We further show that our techniques arerobust: they work even when we restrict our attention to the users who likeboth hoax and non-hoax posts. These results suggest that mapping the diffusionpattern of information can be a useful component of automatic hoax detectionsystems.\n",
            "\n",
            "a review of web infodemic analysis and detection trends across multi-modalities using deep neural networks\n",
            " Fake news and misinformation are a matter of concern for people around theglobe. Users of the internet and social media sites encounter content withfalse information much frequently. Fake news detection is one of the mostanalyzed and prominent areas of research. These detection techniques applypopular machine learning and deep learning algorithms. Previous work in thisdomain covers fake news detection vastly among text circulating online.Platforms that have extensively been observed and analyzed include newswebsites and Twitter. Facebook, Reddit, WhatsApp, YouTube, and other socialapplications are gradually gaining attention in this emerging field.Researchers are analyzing online data based on multiple modalities composed oftext, image, video, speech, and other contributing factors. The combination ofvarious modalities has resulted in efficient fake news detection. At present,there is an abundance of surveys consolidating textual fake news detectionalgorithms. This review primarily deals with multi-modal fake news detectiontechniques that include images, videos, and their combinations with text. Weprovide a comprehensive literature survey of eighty articles presentingstate-of-the-art detection techniques, thereby identifying research gaps andbuilding a pathway for researchers to further advance this domain.\n",
            "\n",
            "fake news detection tools and methods -- a review\n",
            " In the past decade, the social networks platforms and micro-blogging sitessuch as Facebook, Twitter, Instagram, and Weibo have become an integral part ofour day-to-day activities and is widely used all over the world by billions ofusers to share their views and circulate information in the form of messages,pictures, and videos. These are even used by government agencies to spreadimportant information through their verified Facebook accounts and officialTwitter handles, as they can reach a huge population within a limited timewindow. However, many deceptive activities like propaganda and rumor canmislead users on a daily basis. In these COVID times, fake news and rumors arevery prevalent and are shared in a huge number which has created chaos in thistough time. And hence, the need for Fake News Detection in the present scenariois inevitable. In this paper, we survey the recent literature about differentapproaches to detect fake news over the Internet. In particular, we firstlydiscuss fake news and the various terms related to it that have been consideredin the literature. Secondly, we highlight the various publicly availabledatasets and various online tools that are available and can debunk Fake Newsin real-time. Thirdly, we describe fake news detection methods based on twobroader areas i.e., its content and the social context. Finally, we provide acomparison of various techniques that are used to debunk fake news.\n",
            "\n",
            "a survey of fake news: fundamental theories, detection methods, and opportunities\n",
            " The explosive growth in fake news and its erosion to democracy, justice, andpublic trust has increased the demand for fake news detection and intervention.This survey reviews and evaluates methods that can detect fake news from fourperspectives: (1) the false knowledge it carries, (2) its writing style, (3)its propagation patterns, and (4) the credibility of its source. The surveyalso highlights some potential research tasks based on the review. Inparticular, we identify and detail related fundamental theories across variousdisciplines to encourage interdisciplinary research on fake news. We hope thissurvey can facilitate collaborative efforts among experts in computer andinformation sciences, social sciences, political science, and journalism toresearch fake news, where such efforts can lead to fake news detection that isnot only efficient but more importantly, explainable.\n",
            "\n",
            "identifying fake news from twitter sharing data: a large-scale study\n",
            " Social networks offer a ready channel for fake and misleading news to spreadand exert influence. This paper examines the performance of differentreputation algorithms when applied to a large and statistically significantportion of the news that are spread via Twitter. Our main result is that simplecrowdsourcing-based algorithms are able to identify a large portion of fake ormisleading news, while incurring only very low false positive rates formainstream websites. We believe that these algorithms can be used as the basisof practical, large-scale systems for indicating to consumers which news sitesdeserve careful scrutiny and skepticism.\n",
            "\n",
            "exploring fake news detection with heterogeneous social media context graphs\n",
            " Fake news detection has become a research area that goes way beyond a purelyacademic interest as it has direct implications on our society as a whole.Recent advances have primarily focused on textbased approaches. However, it hasbecome clear that to be effective one needs to incorporate additional,contextual information such as spreading behaviour of news articles and userinteraction patterns on social media. We propose to construct heterogeneoussocial context graphs around news articles and reformulate the problem as agraph classification task. Exploring the incorporation of different types ofinformation (to get an idea as to what level of social context is mosteffective) and using different graph neural network architectures indicatesthat this approach is highly effective with robust results on a commonbenchmark dataset.\n",
            "\n",
            "a study of fake news reading and annotating in social media context\n",
            " The online spreading of fake news is a major issue threatening entiresocieties. Much of this spreading is enabled by new media formats, namelysocial networks and online media sites. Researchers and practitioners have beentrying to answer this by characterizing the fake news and devising automatedmethods for detecting them. The detection methods had so far only limitedsuccess, mostly due to the complexity of the news content and context and lackof properly annotated datasets. One possible way to boost the efficiency ofautomated misinformation detection methods, is to imitate the detection work ofhumans. It is also important to understand the news consumption behavior ofonline users. In this paper, we present an eye-tracking study, in which we let44 lay participants to casually read through a social media feed containingposts with news articles, some of which were fake. In a second run, we askedthe participants to decide on the truthfulness of these articles. We alsodescribe a follow-up qualitative study with a similar scenario but this timewith 7 expert fake news annotators. We present the description of both studies,characteristics of the resulting dataset (which we hereby publish) and severalfindings.\n",
            "---------------------\n",
            "Given the context information and not prior knowledge, answer the query.\n",
            "Query: What are some papers about integrating social data analytics in fake news detection?\n",
            "Answer: \n"
          ]
        }
      ],
      "source": [
        "print(paper_query_engine.query(\"What are some papers about integrating social data analytics in fake news detection?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWX0w94wqPVT"
      },
      "source": [
        "## **1.4 Prompt Engineering**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zc1Zv9_qPVT"
      },
      "source": [
        "Prompting is essential for large language models (LLMs) as it guides their responses to be more relevant and contextually appropriate. A well-crafted prompt directs the LLM to focus on specific tasks, ensuring that the output is not only precise but also useful for the user. This specificity is crucial in complex domains where accuracy and detail are key. Effective prompting helps maximize the utility of LLMs by tailoring their vast language capabilities to specific user needs.\n",
        "\n",
        "Let's take a look at llama-index default prompt for the query engine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1vF1kqgqPVU",
        "outputId": "bf20c8b6-491f-463c-d919-06bc15b3cf36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['response_synthesizer:text_qa_template', 'response_synthesizer:refine_template']\n"
          ]
        }
      ],
      "source": [
        "prompts_dict = paper_query_engine.get_prompts()\n",
        "print(list(prompts_dict.keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T97TshDYqPVZ",
        "outputId": "281aeb2d-6a1a-4f2c-dfa8-3f62208d858e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You are an expert Q&A system that is trusted around the world.\n",
            "Always answer the query using the provided context information, and not prior knowledge.\n",
            "Some rules to follow:\n",
            "1. Never directly reference the given context in your answer.\n",
            "2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.\n"
          ]
        }
      ],
      "source": [
        "print(prompts_dict['response_synthesizer:text_qa_template'].conditionals[0][1].message_templates[0].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vb6hZUjAqPVZ"
      },
      "source": [
        "**This is the system prompt. Now look at the user prompt**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnpN9dxyqPVZ",
        "outputId": "8e4c408f-5210-4630-e63b-5f355b2efe78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context information is below.\n",
            "---------------------\n",
            "{context_str}\n",
            "---------------------\n",
            "Given the context information and not prior knowledge, answer the query.\n",
            "Query: {query_str}\n",
            "Answer: \n"
          ]
        }
      ],
      "source": [
        "print(prompts_dict['response_synthesizer:text_qa_template'].conditionals[0][1].message_templates[1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezEDURrQqPVZ"
      },
      "source": [
        "**The default prompts given by llama index is good in general tasks, however we can tailored it to make it specific for our task, which is searching for Data Science papers. Here is how to do it:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5T3koRxqPVa"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import ChatPromptTemplate, PromptTemplate\n",
        "from llama_index.core.llms import ChatMessage, MessageRole\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "You are an expert research assistant specializing in machine learning and artificial intelligence.\n",
        "Your task involves processing a collection of recent research paper abstracts. Perform the following:\n",
        "\n",
        "1. Provide a coherent and comprehensive summary of these abstracts tailored to the user's specific query.\n",
        "2. Identify and summarize the key themes, methodologies, and implications of the research findings, highlighting any significant trends or innovations.\n",
        "\"\"\"\n",
        "\n",
        "user_prompt = \"\"\"\n",
        "Context information is below.\n",
        "---------------------\n",
        "{context_str}\n",
        "---------------------\n",
        "Given the context information and not prior knowledge, answer the query.\n",
        "Query: {query_str}\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "message_template = [\n",
        "    ChatMessage(content=system_prompt, role=MessageRole.SYSTEM),\n",
        "    ChatMessage(content=user_prompt, role=MessageRole.USER)\n",
        "]\n",
        "prompt_template = PromptTemplate(user_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uO8M9stwqPVa"
      },
      "outputs": [],
      "source": [
        "# paper_query_engine = paper_index.as_query_engine(\n",
        "#     llm=gemma_llm,\n",
        "#     similarity_top_k=3,\n",
        "# )\n",
        "\n",
        "\n",
        "paper_query_engine.update_prompts(\n",
        "    {\"response_synthesizer:text_qa_template\": prompt_template}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdWcEvVpqPVa",
        "outputId": "176230ed-2783-4a1a-d861-9b0986743fdc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a survey on video diffusion models\n",
            " The recent wave of AI-generated content (AIGC) has witnessed substantialsuccess in computer vision, with the diffusion model playing a crucial role inthis achievement. Due to their impressive generative capabilities, diffusionmodels are gradually superseding methods based on GANs and auto-regressiveTransformers, demonstrating exceptional performance not only in imagegeneration and editing, but also in the realm of video-related research.However, existing surveys mainly focus on diffusion models in the context ofimage generation, with few up-to-date reviews on their application in the videodomain. To address this gap, this paper presents a comprehensive review ofvideo diffusion models in the AIGC era. Specifically, we begin with a conciseintroduction to the fundamentals and evolution of diffusion models.Subsequently, we present an overview of research on diffusion models in thevideo domain, categorizing the work into three key areas: video generation,video editing, and other video understanding tasks. We conduct a thoroughreview of the literature in these three key areas, including furthercategorization and practical contributions in the field. Finally, we discussthe challenges faced by research in this domain and outline potential futuredevelopmental trends. A comprehensive list of video diffusion models studied inthis survey is available athttps://github.com/ChenHsing/Awesome-Video-Diffusion-Models.\n",
            "frame by familiar frame: understanding replication in video diffusion models\n",
            " Building on the momentum of image generation diffusion models, there is anincreasing interest in video-based diffusion models. However, video generationposes greater challenges due to its higher-dimensional nature, the scarcity oftraining data, and the complex spatiotemporal relationships involved. Imagegeneration models, due to their extensive data requirements, have alreadystrained computational resources to their limits. There have been instances ofthese models reproducing elements from the training samples, leading toconcerns and even legal disputes over sample replication. Video diffusionmodels, which operate with even more constrained datasets and are tasked withgenerating both spatial and temporal content, may be more prone to replicatingsamples from their training sets. Compounding the issue, these models are oftenevaluated using metrics that inadvertently reward replication. In our paper, wepresent a systematic investigation into the phenomenon of sample replication invideo diffusion models. We scrutinize various recent diffusion models for videosynthesis, assessing their tendency to replicate spatial and temporal contentin both unconditional and conditional generation scenarios. Our studyidentifies strategies that are less likely to lead to replication. Furthermore,we propose new evaluation strategies that take replication into account,offering a more accurate measure of a model's ability to generate the originalcontent.\n",
            "vidprom: a million-scale real prompt-gallery dataset for text-to-video diffusion models\n",
            " The arrival of Sora marks a new era for text-to-video diffusion models,bringing significant advancements in video generation and potentialapplications. However, Sora, as well as other text-to-video diffusion models,highly relies on the prompts, and there is no publicly available datasetfeaturing a study of text-to-video prompts. In this paper, we introduceVidProM, the first large-scale dataset comprising 1.67 million uniquetext-to-video prompts from real users. Additionally, the dataset includes 6.69million videos generated by four state-of-the-art diffusion models and somerelated data. We initially demonstrate the curation of this large-scaledataset, which is a time-consuming and costly process. Subsequently, we showhow the proposed VidProM differs from DiffusionDB, a large-scale prompt-gallerydataset for image generation. Based on the analysis of these prompts, weidentify the necessity for a new prompt dataset specifically designed fortext-to-video generation and gain insights into the preferences of real userswhen creating videos. Our large-scale and diverse dataset also inspires manyexciting new research areas. For instance, to develop better, more efficient,and safer text-to-video diffusion models, we suggest exploring text-to-videoprompt engineering, efficient video generation, and video copy detection fordiffusion models. We make the collected dataset VidProM publicly available atGitHub and Hugging Face under the CC-BY- NC 4.0 License.\n",
            "stable video diffusion: scaling latent video diffusion models to large datasets\n",
            " We present Stable Video Diffusion - a latent video diffusion model forhigh-resolution, state-of-the-art text-to-video and image-to-video generation.Recently, latent diffusion models trained for 2D image synthesis have beenturned into generative video models by inserting temporal layers and finetuningthem on small, high-quality video datasets. However, training methods in theliterature vary widely, and the field has yet to agree on a unified strategyfor curating video data. In this paper, we identify and evaluate threedifferent stages for successful training of video LDMs: text-to-imagepretraining, video pretraining, and high-quality video finetuning. Furthermore,we demonstrate the necessity of a well-curated pretraining dataset forgenerating high-quality videos and present a systematic curation process totrain a strong base model, including captioning and filtering strategies. Wethen explore the impact of finetuning our base model on high-quality data andtrain a text-to-video model that is competitive with closed-source videogeneration. We also show that our base model provides a powerful motionrepresentation for downstream tasks such as image-to-video generation andadaptability to camera motion-specific LoRA modules. Finally, we demonstratethat our model provides a strong multi-view 3D-prior and can serve as a base tofinetune a multi-view diffusion model that jointly generates multiple views ofobjects in a feedforward fashion, outperforming image-based methods at afraction of their compute budget. We release code and model weights athttps://github.com/Stability-AI/generative-models .\n",
            "preserve your own correlation: a noise prior for video diffusion models\n",
            " Despite tremendous progress in generating high-quality images using diffusionmodels, synthesizing a sequence of animated frames that are both photorealisticand temporally coherent is still in its infancy. While off-the-shelfbillion-scale datasets for image generation are available, collecting similarvideo data of the same scale is still challenging. Also, training a videodiffusion model is computationally much more expensive than its imagecounterpart. In this work, we explore finetuning a pretrained image diffusionmodel with video data as a practical solution for the video synthesis task. Wefind that naively extending the image noise prior to video noise prior in videodiffusion leads to sub-optimal performance. Our carefully designed video noiseprior leads to substantially better performance. Extensive experimentalvalidation shows that our model, Preserve Your Own Correlation (PYoCo), attainsSOTA zero-shot text-to-video results on the UCF-101 and MSR-VTT benchmarks. Italso achieves SOTA video generation quality on the small-scale UCF-101benchmark with a $10\\times$ smaller model using significantly less computationthan the prior art.\n",
            "efficient video diffusion models via content-frame motion-latent decomposition\n",
            " Video diffusion models have recently made great progress in generationquality, but are still limited by the high memory and computationalrequirements. This is because current video diffusion models often attempt toprocess high-dimensional videos directly. To tackle this issue, we proposecontent-motion latent diffusion model (CMD), a novel efficient extension ofpretrained image diffusion models for video generation. Specifically, wepropose an autoencoder that succinctly encodes a video as a combination of acontent frame (like an image) and a low-dimensional motion latentrepresentation. The former represents the common content, and the latterrepresents the underlying motion in the video, respectively. We generate thecontent frame by fine-tuning a pretrained image diffusion model, and wegenerate the motion latent representation by training a new lightweightdiffusion model. A key innovation here is the design of a compact latent spacethat can directly utilizes a pretrained image diffusion model, which has notbeen done in previous latent video diffusion models. This leads to considerablybetter quality generation and reduced computational costs. For instance, CMDcan sample a video 7.7$\\times$ faster than prior approaches by generating avideo of 512$\\times$1024 resolution and length 16 in 3.1 seconds. Moreover, CMDachieves an FVD score of 212.7 on WebVid-10M, 27.3% better than the previousstate-of-the-art of 292.4.\n",
            "video diffusion models\n",
            " Generating temporally coherent high fidelity video is an important milestonein generative modeling research. We make progress towards this milestone byproposing a diffusion model for video generation that shows very promisinginitial results. Our model is a natural extension of the standard imagediffusion architecture, and it enables jointly training from image and videodata, which we find to reduce the variance of minibatch gradients and speed upoptimization. To generate long and higher resolution videos we introduce a newconditional sampling technique for spatial and temporal video extension thatperforms better than previously proposed methods. We present the first resultson a large text-conditioned video generation task, as well as state-of-the-artresults on established benchmarks for video prediction and unconditional videogeneration. Supplementary material is available athttps://video-diffusion.github.io/\n",
            "diffusion models for video prediction and infilling\n",
            " Predicting and anticipating future outcomes or reasoning about missinginformation in a sequence are critical skills for agents to be able to makeintelligent decisions. This requires strong, temporally coherent generativecapabilities. Diffusion models have shown remarkable success in severalgenerative tasks, but have not been extensively explored in the video domain.We present Random-Mask Video Diffusion (RaMViD), which extends image diffusionmodels to videos using 3D convolutions, and introduces a new conditioningtechnique during training. By varying the mask we condition on, the model isable to perform video prediction, infilling, and upsampling. Due to our simpleconditioning scheme, we can utilize the same architecture as used forunconditional training, which allows us to train the model in a conditional andunconditional fashion at the same time. We evaluate RaMViD on two benchmarkdatasets for video prediction, on which we achieve state-of-the-art results,and one for video generation. High-resolution videos are provided athttps://sites.google.com/view/video-diffusion-prediction.\n",
            "diffusion art or digital forgery? investigating data replication in diffusion models\n",
            " Cutting-edge diffusion models produce images with high quality andcustomizability, enabling them to be used for commercial art and graphic designpurposes. But do diffusion models create unique works of art, or are theyreplicating content directly from their training sets? In this work, we studyimage retrieval frameworks that enable us to compare generated images withtraining samples and detect when content has been replicated. Applying ourframeworks to diffusion models trained on multiple datasets including Oxfordflowers, Celeb-A, ImageNet, and LAION, we discuss how factors such as trainingset size impact rates of content replication. We also identify cases wherediffusion models, including the popular Stable Diffusion model, blatantly copyfrom their training data.\n",
            "echoreel: enhancing action generation of existing video diffusion models\n",
            " Recent large-scale video datasets have facilitated the generation of diverseopen-domain videos of Video Diffusion Models (VDMs). Nonetheless, the efficacyof VDMs in assimilating complex knowledge from these datasets remainsconstrained by their inherent scale, leading to suboptimal comprehension andsynthesis of numerous actions. In this paper, we introduce EchoReel, a novelapproach to augment the capability of VDMs in generating intricate actions byemulating motions from pre-existing videos, which are readily accessible fromdatabases or online repositories. EchoReel seamlessly integrates with existingVDMs, enhancing their ability to produce realistic motions without compromisingtheir fundamental capabilities. Specifically, the Action Prism (AP), isintroduced to distill motion information from reference videos, which requirestraining on only a small dataset. Leveraging the knowledge from pre-trainedVDMs, EchoReel incorporates new action features into VDMs through theadditional layers, eliminating the need for any further fine-tuning ofuntrained actions. Extensive experiments demonstrate that EchoReel is notmerely replicating the whole content from references, and it significantlyimproves the generation of realistic actions, even in situations where existingVDMs might directly fail.\n"
          ]
        }
      ],
      "source": [
        "for retrieve_result in paper_query_engine.retrieve(\"What are some research about video diffusion\"):\n",
        "    print(retrieve_result.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0akSApZqPVa"
      },
      "source": [
        "**Let's test our prompt!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLYPrQMfqPVb",
        "outputId": "ad2521c0-244a-4ea4-8788-e8baa25d8697"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The provided text does not contain any information regarding papers that discuss integrating social data analytics in fake news detection, so I am unable to answer this query from the given context."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 107,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from IPython.display import Markdown\n",
        "\n",
        "Markdown(paper_query_engine.query(\"What are some papers about integrating social data analytics in fake news detection?\").response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOvX9I4CqPVb"
      },
      "source": [
        "## **1.5 Gemma Inference at super speed**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3AMNVo3qPVb"
      },
      "source": [
        "Inferencing LLM is super slow, especially running LLM on free infrastructure like kaggle. Kaggle is great, however it could be too slow for complex RAG apps. In this notebook, I will demonstrate the use of the Groq API service for Gemma inference. Here's why I've chosen Groq for this task:\n",
        "\n",
        "- Groq offers incredibly fast inference speeds, approximately 700-800 tokens per second. This high performance allows for the execution of complex RAG pipelines without concerns about speed limitations.\n",
        "\n",
        "- Enhanced performance: On platforms like Kaggle, we are restricted to using only the quantized Gemma model, which yields poorer and less consistent results. Utilizing an API service like Groq simplifies the process and enhances the outcomes.\n",
        "\n",
        "- Generous free tier: Groq provides a substantial number of requests on its free tier, making it an accessible option for extensive testing and development."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-12T21:17:56.758439Z",
          "iopub.status.busy": "2024-05-12T21:17:56.757700Z",
          "iopub.status.idle": "2024-05-12T21:17:56.906906Z",
          "shell.execute_reply": "2024-05-12T21:17:56.905842Z",
          "shell.execute_reply.started": "2024-05-12T21:17:56.758407Z"
        },
        "id": "PSYdbMrDqPVb"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.groq import Groq\n",
        "# from kaggle_secrets import UserSecretsClient\n",
        "# from llama_index.core import Settings\n",
        "\n",
        "secret_label = 'groq'\n",
        "groq_token = \"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\"\n",
        "\n",
        "\n",
        "groq_llm = Groq(\"gemma-7b-it\", api_key=groq_token)\n",
        "llm = groq_llm\n",
        "Settings.llm = groq_llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYo_ruOnqPVb",
        "outputId": "918df457-04ed-41da-e6ca-ba3d636a6d90"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "**Linear regression** is a statistical method used to model the relationship between a dependent variable and one or more independent variables. It is a simple and widely used technique for **predictive modeling** and **understanding the influence** of independent variables on the dependent variable.\n",
              "\n",
              "**How it works:**\n",
              "\n",
              "- Linear regression finds a straight line that best fits the data points.\n",
              "- The line is represented by the equation:\n",
              "\n",
              "$$y = a + bx$$\n",
              "\n",
              "Where:\n",
              "\n",
              "- **y** is the dependent variable\n",
              "- **a** is the intercept (the value of y when x is 0)\n",
              "- **b** is the slope (the change in y for a one-unit change in x)\n",
              "\n",
              "**Assumptions of linear regression:**\n",
              "\n",
              "- Linearity: The relationship between the dependent and independent variables is straight.\n",
              "- Homoscedasticity: The variance of the residuals (the difference between the observed values and the predicted values) is constant.\n",
              "- Normality: The residuals are normally distributed.\n",
              "- Independence: The observations are independent of each other.\n",
              "\n",
              "**Process:**\n",
              "\n",
              "1. Data collection and exploration\n",
              "2. Model fitting: The linear equation is estimated using least squares method.\n",
              "3. Model evaluation: The goodness of fit and significance of the coefficients are assessed.\n",
              "4. Interpretation: The coefficients and the overall model are interpreted to understand the influence of the independent variables on the dependent variable.\n",
              "\n",
              "**Applications of linear regression:**\n",
              "\n",
              "- **Predictive modeling:** Forecasting future values based on historical data.\n",
              "- **Quality control:** Identifying factors that influence product quality.\n",
              "- **Marketing and sales:** Understanding customer behavior and market trends.\n",
              "- **Finance:** Modeling financial performance and risk assessment.\n",
              "- **Engineering:** Optimizing processes and designing systems.\n",
              "\n",
              "**Advantages of linear regression:**\n",
              "\n",
              "- Simple and easy to understand.\n",
              "- Widely applicable.\n",
              "- Provides insights into the relationship between variables.\n",
              "- Relatively easy to implement.\n",
              "\n",
              "**Disadvantages of linear regression:**\n",
              "\n",
              "- Not suitable for non-linear relationships.\n",
              "- Can be biased if assumptions are not met.\n",
              "- May not capture complex interactions between variables."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from IPython.display import Markdown\n",
        "\n",
        "Markdown(groq_llm.complete(\"What is linear regression?\").text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6Isds71qPVe"
      },
      "outputs": [],
      "source": [
        "paper_query_engine = paper_index.as_query_engine(\n",
        "    similarity_top_k=5,\n",
        "    llm=groq_llm\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WASbFg5AqPVe",
        "outputId": "e8638e1e-6c3e-4b31-d0fc-26aad61b9df0"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The provided text contains several research papers related to video diffusion models. Some of the highlighted research topics include:\n",
              "\n",
              "- **Sample Replication in Video Diffusion Models:** This paper investigates the phenomenon of sample replication in video diffusion models, identifying strategies to mitigate this issue.\n",
              "\n",
              "\n",
              "- **Stable Video Diffusion:** This paper proposes Stable Video Diffusion, a latent video diffusion model for high-resolution video generation, focusing on data curation and training strategies.\n",
              "\n",
              "\n",
              "- **VidProM:** This paper introduces VidProM, a large-scale dataset of text-to-video prompts and generated videos, designed to aid the development of text-to-video diffusion models.\n",
              "\n",
              "\n",
              "- **Video Diffusion Models:** This paper proposes a diffusion model for video generation, addressing the challenges of generating temporally coherent and high-fidelity videos."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Markdown(paper_query_engine.query(\"What are some research about video diffusion?\").response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2a6lxYKqPVe"
      },
      "source": [
        "# **2. Graph-based paper relationship search**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJuwjjvPqPVe"
      },
      "source": [
        "In this section, we delve into constructing a knowledge graph that illustrates the relationships between papers. This graph can be used for interactive visualization and searching relationships between papers (e.g., 'How is paper A related to paper B?') or to find specific relationships within a paper (e.g., 'What works did paper A build upon?'). Moreover, understanding the impact of a paper typically requires examining not just the paper itself but also the works that cite it. This graph serves as a tool for this purpose; it not only displays the papers that cited a specific paper but also explains WHY they cited it. The steps for constructing this knowledge graph are as follows:\n",
        "\n",
        "- Step 1: arXiv Data Extraction: The process starts with academic papers from the arXiv database, which undergo OCR (Optical Character Recognition) and PDF parsing, which organizes the content into structured data such as the title, abstract, sections, and references of the papers.\n",
        "\n",
        "- Step 2: Text Splitter: The text in each section is then processed by a Text Splitter, which split the paper section into smaller chunks, which could be easier for LLMs to process.  \n",
        "\n",
        "- Step 3 GPT-3.5 Processing: Gemma couldn't generate the knowledge graph out-of-the-box. So we need knowledge distillation from a bigger model, which I choose GPT-3.5. The structured data is passed to GPT-3.5 to extract citation relationships, such as \"Data Source\", \"Extension\", or \"Theoretical Foundation\", etc. Each relationship is paired with a dense explanation. I extracted a total of ~300 papers, which cost around 4$.\n",
        "\n",
        "- Step 4 Training Gemma - 7B: The distilled knowledge data are then used to train Gemma-7b. Then I use this model to generate citation relationships for as many papers as I can. In total, I extracted 7k papers, with around 150K triplets! Crazy!!\n",
        "\n",
        "- Step 5 Graph Store: Finally, a Graph Store is created containing 7K papers and 586K triplets. This could then be used for searching relationships or visualization.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUT0AzlgqPVf"
      },
      "source": [
        "## **2.0 Extracting Data + Knowledge distillation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gb59LosPqPVf"
      },
      "source": [
        "**Knowledge distillation** involves training a smaller, less complex model (termed the \"student\") to mimic the behavior of a larger, more intricate model (referred to as the \"teacher\"). The objective is to impart the teacher model's knowledge to the student model so that the student can perform comparably but more efficiently and with reduced computational demands. To achieve this, the student model learns not only from the direct outputs, such as class labels in classification tasks, but also from the softer, indirect outputs like the probability distributions or the feature representations that the teacher model produces. This technique is especially valuable for deploying advanced AI models on devices with constrained computational resources, such as smartphones or embedded systems.\n",
        "\n",
        "In this project, I initially utilized OCR and PDF reader tools to extract textual information from PDF files. The extracted text was then segmented and input into GPT-3.5 to create a dataset for distilling knowledge into Gemma-7B. This dataset is accessible here: [citation data](https://www.kaggle.com/datasets/bachngoh/citation-generation-train)\n",
        "\n",
        "Here are some training samples:\n",
        "```json\n",
        "\n",
        "{\n",
        "            \"Input\": \"Additionally, generalizing over objects, actions and events allow users to make predictions of upcoming Effects even for unseen events (Radinsky, Davidovich, and Markovitch 2012). Therefore, in our work, we condense our graphs by grouping nodes that refer to the same topic together using previous topic modelling solutions (Sia, Dalmia, and Mielke 2020;Zhang et al. 2022).\"\n",
        "            \"Output\":[\n",
        "    {\n",
        "        \"Citation\": \"(Radinsky, Davidovich, and Markovitch 2012)\"\n",
        "        \"Category\": \"Theoretical Foundation\"\n",
        "        \"Explanation\": \"The cited work establishes the importance of generalizing over objects, actions, and events to make predictions, which forms the theoretical basis for the approach taken in the citing paper.\"\n",
        "    },\n",
        "    {\n",
        "        \"Citation\":\"(Sia, Dalmia, and Mielke 2020; Zhang et al. 2022)\"\n",
        "        \"Category\":\"Methodological Basis\"\n",
        "        \"Explanation\":\"The cited works introduce the method of condensing graphs by grouping nodes based on topic modeling solutions, which is adopted by the citing paper to structure its analysis.\"}]\n",
        "}\n",
        "\n",
        "```\n",
        "\n",
        "```json\n",
        "\n",
        "{\n",
        "        \"Input\":\"The proposed GIFT was implemented into three PLMs. • BERT (Devlin et al., 2019) was pre-trained to learn universal language representations on a large amount of general corpora with the self-supervised tasks of MLM and NSP. • SA-BERT (Gu et al., 2020) added speaker embeddings and further pre-trained BERT on a domain-specific corpus to incorporate domain knowledge. We re-implemented SA-BERT on the same pre-training corpus used in this paper to ensure fair comparison. • MPC-BERT (Gu et al., 2021) was pre-trained with two major types of self-supervised tasks for modeling interlocutor structures and utterance semantics in a unified framework. (2016). For models with GIFT, numbers marked with ‡ denoted larger performance improvement or less performance drop compared with the corresponding models without GIFT.\"\n",
        "        \"Output\":[\n",
        "    {\n",
        "        \"Citation\":\"(Devlin et al., 2019)\"\n",
        "        \"Category\":\"Methodological Basis\"\n",
        "        \"Explanation\":\"The cited work, BERT, serves as the basis for pre-training language representations using self-supervised tasks of MLM and NSP, which is adopted by the citing paper for implementing the proposed GIFT into PLMs.\"\n",
        "    }\n",
        "    {\n",
        "        \"Citation\":\"(Gu et al., 2020)\"\n",
        "        \"Category\":\"Extension or Continuation\"\n",
        "        \"Explanation\":\"SA-BERT, as cited, extends the pre-training of BERT by incorporating speaker embeddings and domain-specific knowledge, which is further explored and re-implemented in the citing paper for a fair comparison.\"\n",
        "    }\n",
        "    {\n",
        "        \"Citation\":\"(Gu et al., 2021)\"\n",
        "        \"Category\":\"Extension or Continuation\"\n",
        "        \"Explanation\":\"MPC-BERT, referenced in the text, extends the pre-training of BERT by incorporating self-supervised tasks for modeling interlocutor structures and utterance semantics, which is further explored in the citing paper for enhancing language representations.\"}]\n",
        "}\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcq2yca_qPVg"
      },
      "source": [
        "## **2.1 Download pre-extracted citation data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6tG25goqPVg",
        "outputId": "5634b4d1-e238-4cf9-85e6-53c5be50b2f6",
        "colab": {
          "referenced_widgets": [
            "1403417ee8924727b1648d90588811dc",
            "a02923abd43a45ce9b4b02a145be43c8",
            "561114acb2f847ad92f1ff094d919866",
            "c224ef45b37b442aabda990666f604db",
            "6e0c4da372cb4bffa32927a0a9c87a74"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1403417ee8924727b1648d90588811dc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/1.36k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a02923abd43a45ce9b4b02a145be43c8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/191M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "561114acb2f847ad92f1ff094d919866",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/176M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c224ef45b37b442aabda990666f604db",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/170M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6e0c4da372cb4bffa32927a0a9c87a74",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/19454 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "parsed_article = load_dataset(\"BachNgoH/ParsedArxivPapers\")['train']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59n4A7rCqPVg",
        "outputId": "61470b2d-7e49-40f9-bcff-0d64a71550d1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'2.19.1'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import datasets\n",
        "datasets.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SRihR3zqPVh"
      },
      "outputs": [],
      "source": [
        "parsed_article = parsed_article.to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqcBFtU1qPVh"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "for article in parsed_article:\n",
        "    if article['citation_data'] != None:\n",
        "        article['citation_data'] = json.loads(article['citation_data'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8z9NHcGqPVh"
      },
      "outputs": [],
      "source": [
        "# parsed_article[1]['citation_data']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAHUqIxdqPVh",
        "outputId": "885099f1-9b34-417b-9300-2ee148f48b12"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "19454"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(parsed_article)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vplbrnvhqPVi"
      },
      "source": [
        "Let's see the number of annotated papers for now!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VyFBMOuqqPVi"
      },
      "outputs": [],
      "source": [
        "annotated_article = [x for x in parsed_article if x['citation_data'] is not None]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwPzo1sEqPVi",
        "outputId": "81b10e89-9da4-4cf0-d6be-ca7262d2e980"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Annotated Papers:  7243\n"
          ]
        }
      ],
      "source": [
        "print(\"Annotated Papers: \", len(annotated_article))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tktfjijgqPVi"
      },
      "outputs": [],
      "source": [
        "# annotated_article[1]['citation_data']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzgrxNPkqPVi"
      },
      "source": [
        "## **2.2 Parsing generated data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEogj0caqPVi"
      },
      "source": [
        "From my observation, there are 2 main citation styles in AI papers, Author-year style and Numeric style:\n",
        "\n",
        "Example of Author-year style:\n",
        "- (Bassignana and Plank, 2022a)\n",
        "- (Liu et al., 2021)\n",
        "- (Köksal and Özgür, 2020)\n",
        "\n",
        "Example of Numeric style:\n",
        "- [1], [2], [3]\n",
        "- [2, 56, 67]\n",
        "- [7 - 9]\n",
        "\n",
        "Therefore, we need different strategy to handle each style of citation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGhc85-DqPVj"
      },
      "source": [
        "### **2.2.1 Handle Author-Year citation style**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAd6tKfAqPVj"
      },
      "source": [
        "Handling this citation style can be quite frustrating. Initially, we must separate combined citations like (Liu et al., 2021; Littell et al., 201) into individual entries. Then, we need to identify the first author and publication year. Subsequently, we have to locate the corresponding reference within our reference list based on the author's name and publication year."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-2Fy9l8qPVj",
        "outputId": "99f8b4d0-6a3e-48b0-c219-676855dfa316"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['(Castillo et al., 2015)',\n",
              " '(Castillo et al., 2017)',\n",
              " '(Castillo et al., , 2017)',\n",
              " '(Mihalcea and Tarau, 2004)',\n",
              " '(Hassan and Banea, 2006)',\n",
              " '(Rousseau et al., 2015)',\n",
              " '(Castillo et al., 2015)',\n",
              " '(Arora et al., 2009)',\n",
              " '(Joshi and Rosé, 2009)',\n",
              " '(Dozat and Manning, 2016)',\n",
              " '(Yuan et al., 2021)',\n",
              " '(Yao et al., 2019)',\n",
              " '(Liu et al., 2020)',\n",
              " '(Ragesh et al., 2021)',\n",
              " '(Ding et al., 2020)',\n",
              " '(Huang et al., 2019a)',\n",
              " '(Qian et al., 2019)',\n",
              " '(Li et al., 2015;Cho et al., 2014)',\n",
              " '(Nikolentzos et al., 2020)',\n",
              " '(Gu et al., 2023)',\n",
              " '(Galke and Scherp, 2022)',\n",
              " '(Hassan and Banea, 2006)',\n",
              " '(Castillo et al., 2015)',\n",
              " '(Huang et al., 2019b)',\n",
              " '(Devlin et al., 2018)',\n",
              " '(Beltagy et al., 2020)',\n",
              " '(Grano et al., 2017)',\n",
              " '(Zhang et al., 2015)',\n",
              " '(Maas et al., 2011)',\n",
              " '(Greene and Cunningham, 2006)',\n",
              " '(Kiesel et al., 2018)',\n",
              " '(Kipf and Welling 2016)',\n",
              " '(Xu et al. 2018)',\n",
              " '(Velickovic et al. 2017)',\n",
              " '(see Appendix E)',\n",
              " '(default parameter settings in the original implementation)',\n",
              " '(GloVe Wiki-Gigaword 300-dim.)',\n",
              " '(Pennington et al., 2014)',\n",
              " '(Mikolov et al., 2013)',\n",
              " '(Pennington et al., 2014)',\n",
              " '(Pennington et al., 2014)',\n",
              " '(Pennington et al., 2014)',\n",
              " '(Kingma and Ba, 2014)',\n",
              " '(Xu et al., 2018)',\n",
              " '(Grano et al., 2017)',\n",
              " '(Zhang et al., 2015)',\n",
              " '(Lehmann et al., 2015)',\n",
              " '(Maas et al., 2011)',\n",
              " '(Greene and Cunningham, 2006)',\n",
              " '(Kiesel et al., 2018)',\n",
              " '(Kiesel et al., 2019)']"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Parse annotated articles\n",
        "import re\n",
        "\n",
        "# Function to normalize author names for comparison\n",
        "def normalize_author_name(name):\n",
        "    # Convert to lowercase and remove middle initials\n",
        "    name = name.lower()\n",
        "    name = re.sub(r\"\\s+[a-z]\\.\", \"\", name)  # Remove middle initials\n",
        "    return name\n",
        "\n",
        "\n",
        "citation_names = [c['Citation'] for c in annotated_article[1]['citation_data']]\n",
        "citation_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSs9n94ZqPVj"
      },
      "outputs": [],
      "source": [
        "# Refined function to identify and normalize the first author from a citation\n",
        "def identify_and_normalize_first_author(citation_authors):\n",
        "    # Check for 'et al.' and 'and' to find the first author\n",
        "    if 'et al.' in citation_authors:\n",
        "        first_author = citation_authors.split('et al.')[0].strip()\n",
        "    elif ' and ' in citation_authors:\n",
        "        first_author = citation_authors.rsplit(' and ', 1)[0].split(',')[0].strip()\n",
        "    else:\n",
        "        first_author = citation_authors.split(',')[0].strip()\n",
        "    # Normalize the first author's name for comparison\n",
        "    return first_author.lower()\n",
        "\n",
        "\n",
        "# Function to split and parse citations in cases of citation\n",
        "# like (Culotta and Sorensen 2004; Bunescu and Mooney 2005; Ittoo and Bouma 2013)\n",
        "def split_and_parse_citation(citation):\n",
        "\n",
        "    # Remove outer parentheses\n",
        "    citation = citation.strip(\"()\")\n",
        "    # Split on semicolon if it's present, indicating multiple citations within one\n",
        "    if ';' in citation:\n",
        "        sub_citations = citation.split(';')\n",
        "    else:\n",
        "        sub_citations = [citation]\n",
        "\n",
        "    # Parse each sub-citation for author names and year\n",
        "    for sub_citation in sub_citations:\n",
        "        # Splitting based on the last occurrence of space which is assumed to be before the year\n",
        "        *authors, year = sub_citation.rsplit(' ', 1)\n",
        "        authors = ' '.join(authors)  # Joining back the authors in case there are multiple names\n",
        "        parsed_citation = {'Author': identify_and_normalize_first_author(authors), 'Year': year}\n",
        "\n",
        "    return parsed_citation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbWJXxumqPVj",
        "outputId": "a72b19e6-c476-49e5-fe8f-8d1d7fa088fe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'authors': 'Shilpa Arora; Mahesh Joshi; Carolyn Rosé',\n",
              "  'journal': '',\n",
              "  'ref_id': 'b0',\n",
              "  'title': 'Identifying types of claims in online customer reviews',\n",
              "  'year': '2009'},\n",
              " {'authors': 'Iz Beltagy; Matthew E Peters; Arman Cohan',\n",
              "  'journal': '',\n",
              "  'ref_id': 'b1',\n",
              "  'title': 'Longformer: The long-document transformer',\n",
              "  'year': '2020'},\n",
              " {'authors': 'Esteban Castillo; Ofelia Cervantes; Darnes Vilari;  David',\n",
              "  'journal': 'International Journal of Computer Applications',\n",
              "  'ref_id': 'b2',\n",
              "  'title': 'Author verification using a graphbased representation',\n",
              "  'year': '2015'},\n",
              " {'authors': 'Esteban Castillo; Ofelia Cervantes; Darnes Vilarino',\n",
              "  'journal': 'Computación y Sistemas',\n",
              "  'ref_id': 'b3',\n",
              "  'title': 'Text analysis using different graph-based representations',\n",
              "  'year': '2017'},\n",
              " {'authors': 'Kyunghyun Cho; Bart Van Merriënboer; Caglar Gulcehre; Dzmitry Bahdanau; Fethi Bougares; Holger Schwenk; Yoshua Bengio',\n",
              "  'journal': '',\n",
              "  'ref_id': 'b4',\n",
              "  'title': 'Learning phrase representations using rnn encoder-decoder for statistical machine translation',\n",
              "  'year': '2014'}]"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "references = annotated_article[1]['references'][:5]\n",
        "references"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MDeCZHkqPVj"
      },
      "outputs": [],
      "source": [
        "# Function to normalize and extract the first author's name\n",
        "def get_first_author(authors_str):\n",
        "    first_author = authors_str.split(';')[0].strip()\n",
        "    # Normalize the first author's name for comparison\n",
        "    return first_author.lower()\n",
        "\n",
        "# Generalized regular expression for detecting years in various date formats and standalone years\n",
        "\n",
        "# Function to detect various year patterns and extract the year\n",
        "def extract_years(string):\n",
        "    general_year_pattern = re.compile(r'(?:\\b|\\D)(\\d{4})(?:\\b|\\D)')\n",
        "    # Find all matches for the general year pattern\n",
        "\n",
        "    matches = general_year_pattern.findall(string)\n",
        "    # Add all unique years found in this string\n",
        "    year = matches[0] if matches else None\n",
        "    return year\n",
        "\n",
        "# Function to match citations with references\n",
        "def match_citations_with_references(citation, references):\n",
        "    match = None\n",
        "    citation_first_author = citation['Author']\n",
        "    citation_year = citation['Year'].strip()\n",
        "    for ref in references:\n",
        "        ref_first_author = get_first_author(ref['authors'])\n",
        "        ref_year = extract_years(ref['year']) if ref['year'] is not None else None\n",
        "        # Check for match by first author and year\n",
        "        if citation_first_author in ref_first_author: #and (citation_year == ref_year or ref_year is None):\n",
        "            match = {\n",
        "                'ref_id': ref['ref_id']\n",
        "            }\n",
        "    return match"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzifY0O6qPVk"
      },
      "outputs": [],
      "source": [
        "# test with the first sample\n",
        "for citation in annotated_article[1]['citation_data']:\n",
        "    parsed_name = split_and_parse_citation(citation['Citation'])\n",
        "    match = match_citations_with_references(parsed_name, references)\n",
        "    citation['ref_id'] = match['ref_id'] if match else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYmm2GPlqPVk"
      },
      "source": [
        "Now we need to group the citation data by ref_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ci5eZ-knqPVk",
        "outputId": "39d3fffe-e4bb-4e9f-8f0f-0bcfe6a3e889"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'Category': 'Methodological Basis',\n",
              "  'Citation': '(Castillo et al., 2015)',\n",
              "  'Explanation': 'The cited work provides a foundation for the use of graphs in text classification tasks, as it discusses the applicability and effectiveness of graphs in broader settings.',\n",
              "  'ref_id': 'b3'},\n",
              " {'Category': 'Extension or Continuation',\n",
              "  'Citation': '(Castillo et al., 2017)',\n",
              "  'Explanation': 'The cited work is a continuation of the research on graph representations in text classification, as it further explores the use of graphs in more diverse scenarios.',\n",
              "  'ref_id': 'b3'},\n",
              " {'Category': 'Data Source',\n",
              "  'Citation': '(Castillo et al., , 2017)',\n",
              "  'Explanation': 'The cited work is a data source for the text classification tasks used in the study conducted in the citing paper.',\n",
              "  'ref_id': 'b3'},\n",
              " {'Category': 'Methodological Basis',\n",
              "  'Citation': '(Mihalcea and Tarau, 2004)',\n",
              "  'Explanation': 'The cited work by Mihalcea and Tarau (2004) provides a method of using co-occurrence graphs for keyword extraction, which the citing paper adopts in their research on text representation.',\n",
              "  'ref_id': None},\n",
              " {'Category': 'Methodological Basis',\n",
              "  'Citation': '(Hassan and Banea, 2006)',\n",
              "  'Explanation': 'The cited work by Hassan and Banea (2006) uses a co-occurrence graph with N = 2 and TextRank to replace term frequency weights, which the citing paper builds upon in their text classification research.',\n",
              "  'ref_id': None},\n",
              " {'Category': 'Methodological Basis',\n",
              "  'Citation': '(Rousseau et al., 2015)',\n",
              "  'Explanation': 'The cited work by Rousseau et al. (2015) uses a graph-of-words approach to cast text classification as a classification problem, which the citing paper adopts in their research on text representation.',\n",
              "  'ref_id': None},\n",
              " {'Category': 'Methodological Basis',\n",
              "  'Citation': '(Castillo et al., 2015)',\n",
              "  'Explanation': 'The cited work by Castillo et al. (2015) uses sequence graphs to reflect the original order of words in text, which the citing paper builds upon in their research on text representation.',\n",
              "  'ref_id': 'b3'},\n",
              " {'Category': 'Methodological Basis',\n",
              "  'Citation': '(Arora et al., 2009)',\n",
              "  'Explanation': 'The cited work by Arora et al. provides a method of analyzing individual sentences using tree and graph-structured formalisms, which the citing paper adopts in its research to build document-level representations.',\n",
              "  'ref_id': 'b0'},\n",
              " {'Category': 'Methodological Basis',\n",
              "  'Citation': '(Joshi and Rosé, 2009)',\n",
              "  'Explanation': 'The cited work by Joshi and Rosé offers a method of analyzing individual sentences using tree and graph-structured formalisms, which the citing paper utilizes in its research to build document-level representations.',\n",
              "  'ref_id': None},\n",
              " {'Category': 'Methodological Basis',\n",
              "  'Citation': '(Dozat and Manning, 2016)',\n",
              "  'Explanation': 'The cited work by Dozat and Manning presents a method of inferring word dependencies to obtain syntactic dependency trees, which the citing paper employs in its research to build document-level representations.',\n",
              "  'ref_id': None},\n",
              " {'Category': 'Methodological Basis',\n",
              "  'Citation': '(Yuan et al., 2021)',\n",
              "  'Explanation': 'The cited work by Yuan et al. provides a method of inferring word dependencies to obtain syntactic dependency trees, which the citing paper adopts in its research to build document-level representations.',\n",
              "  'ref_id': None},\n",
              " {'Category': 'Methodological Basis',\n",
              "  'Citation': '(Yao et al., 2019)',\n",
              "  'Explanation': 'TextGCN proposes a heterogeneous graph construction using words and documents as nodes, which serves as a methodological basis for the research conducted in the citing paper.',\n",
              "  'ref_id': None},\n",
              " {'Category': 'Data Source',\n",
              "  'Citation': '(Liu et al., 2020)',\n",
              "  'Explanation': 'TensorGCN is a data source for the research conducted in the citing paper, as it is one of the proposals that integrate heterogeneous contextual information.',\n",
              "  'ref_id': None},\n",
              " {'Category': 'Data Source',\n",
              "  'Citation': '(Ragesh et al., 2021)',\n",
              "  'Explanation': 'Het-eGCN is another data source for the research conducted in the citing paper, as it is another proposal that integrates heterogeneous contextual information.',\n",
              "  'ref_id': None},\n",
              " {'Category': 'Data Source',\n",
              "  'Citation': '(Ding et al., 2020)',\n",
              "  'Explanation': 'HyperGAT is a data source for the research conducted in the citing paper, as it is a proposal that integrates heterogeneous contextual information.',\n",
              "  'ref_id': None},\n",
              " {'Category': 'Methodological Basis',\n",
              "  'Citation': '(Huang et al., 2019a)',\n",
              "  'Explanation': 'TextLevelGCN creates one graph per input text, which serves as a methodological basis for the research conducted in the citing paper.',\n",
              "  'ref_id': None},\n",
              " {'Category': 'Methodological Basis',\n",
              "  'Citation': '(Qian et al., 2019)',\n",
              "  'Explanation': 'The cited work by Qian et al. introduces a GCN-based method for tagging and information extraction tasks, which the citing paper adopts in their research.',\n",
              "  'ref_id': None},\n",
              " {'Category': 'Methodological Basis',\n",
              "  'Citation': '(Li et al., 2015;Cho et al., 2014)',\n",
              "  'Explanation': 'The cited works by Li et al. and Cho et al. use a Gated Recurrent Unit-based message passing function for updating node feature vectors, which the citing paper adapts in their research.',\n",
              "  'ref_id': 'b4'},\n",
              " {'Category': 'Extension or Continuation',\n",
              "  'Citation': '(Nikolentzos et al., 2020)',\n",
              "  'Explanation': 'The cited work by Nikolentzos et al. introduces the master node concept in the graph construction method, which the citing paper extends by including a master node in their research.',\n",
              "  'ref_id': None},\n",
              " {'Category': 'Methodological Basis',\n",
              "  'Citation': '(Gu et al., 2023)',\n",
              "  'Explanation': 'The cited work by Gu et al. proposes a heterogeneous graph construction method with topic nodes for class-aware representation learning, which the citing paper builds upon in their research.',\n",
              "  'ref_id': None},\n",
              " {'Category': 'Methodological Basis',\n",
              "  'Citation': '(Galke and Scherp, 2022)',\n",
              "  'Explanation': 'The cited work by Galke and Scherp (2022) provides a comparison of different text classification approaches, including Bag of Words (BoW), sequence, and graph models. The citing paper adopts this analysis to evaluate the necessity of text-graphs in text classification.',\n",
              "  'ref_id': None},\n",
              " {'Category': 'Methodological Basis',\n",
              "  'Citation': '(Hassan and Banea, 2006)',\n",
              "  'Explanation': 'The cited work by Hassan and Banea (2006) provides the basis for the window-based method of graph construction used in the citing paper.',\n",
              "  'ref_id': None},\n",
              " {'Category': 'Methodological Basis',\n",
              "  'Citation': '(Castillo et al., 2015)',\n",
              "  'Explanation': 'The cited work by Castillo et al. (2015) serves as the methodological basis for the sequence-weighted method of graph construction in the citing paper.',\n",
              "  'ref_id': 'b3'},\n",
              " {'Category': 'Methodological Basis',\n",
              "  'Citation': '(Huang et al., 2019b)',\n",
              "  'Explanation': 'The cited work introduces the TextLevelGCN model, which the citing paper adopts to create a more sophisticated graph-based text representation strategy that considers each word token occurrence as a separate node and uses weighted information from neighbors to determine the in-context meaning.',\n",
              "  'ref_id': None},\n",
              " {'Category': 'Methodological Basis',\n",
              "  'Citation': '(Devlin et al., 2018)',\n",
              "  'Explanation': 'The cited work introduces the BERT Transformer as a powerful masked language model-based encoder that the citing paper adopts for comparison in the study of text representation schemes.',\n",
              "  'ref_id': None},\n",
              " {'Category': 'Methodological Basis',\n",
              "  'Citation': '(Beltagy et al., 2020)',\n",
              "  'Explanation': 'The cited work presents the Longformer Transformer as a modified attention mechanism that extends the maximum input length of the BERT model, providing a basis for comparison in the study of text representation schemes.',\n",
              "  'ref_id': 'b1'},\n",
              " {'Category': 'Data Source',\n",
              "  'Citation': '(Grano et al., 2017)',\n",
              "  'Explanation': 'The cited work provides the App Reviews dataset for fine-grained sentiment analysis in an imbalanced setting, which the citing paper utilizes in their research on assessing the generalizability of graph strategies in text classification.',\n",
              "  'ref_id': None},\n",
              " {'Category': 'Data Source',\n",
              "  'Citation': '(Zhang et al., 2015)',\n",
              "  'Explanation': 'The cited work provides the DBpedia dataset for topic classification based on DBpedia 2014 classes, which the citing paper utilizes in their research on assessing the generalizability of graph strategies in text classification.',\n",
              "  'ref_id': None},\n",
              " {'Category': 'Data Source',\n",
              "  'Citation': '(Maas et al., 2011)',\n",
              "  'Explanation': 'The cited work provides the IMDB dataset for movie reviews, which the citing paper utilizes in its research for binary sentiment classification.',\n",
              "  'ref_id': None},\n",
              " {'Category': 'Data Source',\n",
              "  'Citation': '(Greene and Cunningham, 2006)',\n",
              "  'Explanation': 'The BBC News dataset is cited as a source of English documents for topic classification research in the cited work.',\n",
              "  'ref_id': None},\n",
              " {'Category': 'Data Source',\n",
              "  'Citation': '(Kiesel et al., 2018)',\n",
              "  'Explanation': 'The HND dataset is referenced as a source of news articles for hyperpartisan news detection research in the cited work.',\n",
              "  'ref_id': None},\n",
              " {'Category': 'Methodological Basis',\n",
              "  'Citation': '(Kipf and Welling 2016)',\n",
              "  'Explanation': 'The cited work provides the traditional graph convolutional neural layer (GCN) that the citing paper uses in their experiments on Intuitive Graphs.',\n",
              "  'ref_id': None},\n",
              " {'Category': 'Methodological Basis',\n",
              "  'Citation': '(Xu et al. 2018)',\n",
              "  'Explanation': 'The cited work introduces the graph isomorphism operator (GIN) that the citing paper uses in their experiments to improve structural discriminative power in GNNs.',\n",
              "  'ref_id': None},\n",
              " {'Category': 'Methodological Basis',\n",
              "  'Citation': '(Velickovic et al. 2017)',\n",
              "  'Explanation': 'The cited work includes the graph attentional operator (GAT) with 4 attention heads that the citing paper uses in their experiments to improve the performance of GNNs.',\n",
              "  'ref_id': None},\n",
              " {'Category': 'Data Source',\n",
              "  'Citation': '(see Appendix E)',\n",
              "  'Explanation': 'The cited work is a PyTorch Geometric implementation that the citing paper uses in their experiments on Intuitive Graphs.',\n",
              "  'ref_id': None},\n",
              " {'Category': 'Methodological Basis',\n",
              "  'Citation': '(default parameter settings in the original implementation)',\n",
              "  'Explanation': 'The cited work provides the default parameter settings for TextLevelGCN that the citing paper uses in their experiments.',\n",
              "  'ref_id': None},\n",
              " {'Category': 'Data Source',\n",
              "  'Citation': '(GloVe Wiki-Gigaword 300-dim.)',\n",
              "  'Explanation': 'The cited work is a node vector initialization strategy that the citing paper uses in their experiments to compare different node vector initialization strategies in TextLevelGCN.',\n",
              "  'ref_id': None},\n",
              " {'Category': 'Methodological Basis',\n",
              "  'Citation': '(Pennington et al., 2014)',\n",
              "  'Explanation': 'The cited work by Pennington et al. provides the embeddings used in the study conducted in the citing paper, which serves as a methodological basis for the text classification task.',\n",
              "  'ref_id': None},\n",
              " {'Category': 'Methodological Basis',\n",
              "  'Citation': '(Mikolov et al., 2013)',\n",
              "  'Explanation': 'The cited work by Mikolov et al. provides the Word2Vec Google News 300-dim. embeddings used in the study, which is a methodological basis for the text classification task.',\n",
              "  'ref_id': None},\n",
              " {'Category': 'Methodological Basis',\n",
              "  'Citation': '(Pennington et al., 2014)',\n",
              "  'Explanation': 'The cited work by Pennington et al. provides the static BERT pre-trained embeddings used in the study, which is a methodological basis for the text classification task.',\n",
              "  'ref_id': None},\n",
              " {'Category': 'Methodological Basis',\n",
              "  'Citation': '(Pennington et al., 2014)',\n",
              "  'Explanation': 'The cited work by Pennington et al. provides the contextualized BERT embeddings used in the study, which is a methodological basis for the text classification task.',\n",
              "  'ref_id': None},\n",
              " {'Category': 'Data Source',\n",
              "  'Citation': '(Pennington et al., 2014)',\n",
              "  'Explanation': 'The cited work by Pennington et al. serves as the data source for the BoW vocabulary used in the study, which is a foundational element for the text classification task.',\n",
              "  'ref_id': None},\n",
              " {'Category': 'Methodological Basis',\n",
              "  'Citation': '(Kingma and Ba, 2014)',\n",
              "  'Explanation': 'The cited work by Kingma and Ba provides the Adam optimization method used in the study, which is a methodological basis for the text classification task.',\n",
              "  'ref_id': None},\n",
              " {'Category': 'Methodological Basis',\n",
              "  'Citation': '(Xu et al., 2018)',\n",
              "  'Explanation': 'The cited work by Xu et al. (2018) introduces the GCN model, which the citing paper adopts as a method for improving discriminative power in GNN message passing for certain tasks.',\n",
              "  'ref_id': None},\n",
              " {'Category': 'Data Source',\n",
              "  'Citation': '(Grano et al., 2017)',\n",
              "  'Explanation': 'The App Reviews dataset is a collection of user reviews of Android applications that is used in the text classification experiments conducted in the citing paper.',\n",
              "  'ref_id': None},\n",
              " {'Category': 'Data Source',\n",
              "  'Citation': '(Zhang et al., 2015)',\n",
              "  'Explanation': 'The DBpedia ontology classification dataset is a collection of Wikipedia articles that is used for topic classification in the text classification experiments.',\n",
              "  'ref_id': None},\n",
              " {'Category': 'Data Source',\n",
              "  'Citation': '(Lehmann et al., 2015)',\n",
              "  'Explanation': 'The DBpedia ontology classification dataset is based on the DBpedia 2014 knowledge base, which is a multilingual knowledge base that is referenced in the original DBpedia.',\n",
              "  'ref_id': None},\n",
              " {'Category': 'Data Source',\n",
              "  'Citation': '(Maas et al., 2011)',\n",
              "  'Explanation': 'The cited work provides a dataset of English language movie reviews for binary sentiment classification that the citing paper uses in its research.',\n",
              "  'ref_id': None},\n",
              " {'Category': 'Data Source',\n",
              "  'Citation': '(Greene and Cunningham, 2006)',\n",
              "  'Explanation': 'The cited work provides a dataset of English documents from the BBC News website that the citing paper uses in its research.',\n",
              "  'ref_id': None},\n",
              " {'Category': 'Data Source',\n",
              "  'Citation': '(Kiesel et al., 2018)',\n",
              "  'Explanation': 'The cited work provides a dataset of English samples for hyperpartisan news detection that the citing paper uses in its research.',\n",
              "  'ref_id': None},\n",
              " {'Category': 'Supporting Evidence',\n",
              "  'Citation': '(Kiesel et al., 2019)',\n",
              "  'Explanation': 'The cited work by Kiesel et al. provides a detailed analysis of the characteristics of hyperpartisan language, which serves as a foundational basis for the citing paper in understanding the nature of the task and the challenges involved in detecting hyperpartisan language.',\n",
              "  'ref_id': None}]"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "annotated_article[1]['citation_data']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nAdRvfmqPVk",
        "outputId": "1227fcf3-2507-4391-cc86-58252884874a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'b3': [{'Category': 'Methodological Basis', 'Citation': '(Castillo et al., 2015)', 'Explanation': 'The cited work provides a foundation for the use of graphs in text classification tasks, as it discusses the applicability and effectiveness of graphs in broader settings.'}, {'Category': 'Extension or Continuation', 'Citation': '(Castillo et al., 2017)', 'Explanation': 'The cited work is a continuation of the research on graph representations in text classification, as it further explores the use of graphs in more diverse scenarios.'}, {'Category': 'Data Source', 'Citation': '(Castillo et al., , 2017)', 'Explanation': 'The cited work is a data source for the text classification tasks used in the study conducted in the citing paper.'}, {'Category': 'Methodological Basis', 'Citation': '(Castillo et al., 2015)', 'Explanation': 'The cited work by Castillo et al. (2015) uses sequence graphs to reflect the original order of words in text, which the citing paper builds upon in their research on text representation.'}, {'Category': 'Methodological Basis', 'Citation': '(Castillo et al., 2015)', 'Explanation': 'The cited work by Castillo et al. (2015) serves as the methodological basis for the sequence-weighted method of graph construction in the citing paper.'}], None: [{'Category': 'Methodological Basis', 'Citation': '(Mihalcea and Tarau, 2004)', 'Explanation': 'The cited work by Mihalcea and Tarau (2004) provides a method of using co-occurrence graphs for keyword extraction, which the citing paper adopts in their research on text representation.'}, {'Category': 'Methodological Basis', 'Citation': '(Hassan and Banea, 2006)', 'Explanation': 'The cited work by Hassan and Banea (2006) uses a co-occurrence graph with N = 2 and TextRank to replace term frequency weights, which the citing paper builds upon in their text classification research.'}, {'Category': 'Methodological Basis', 'Citation': '(Rousseau et al., 2015)', 'Explanation': 'The cited work by Rousseau et al. (2015) uses a graph-of-words approach to cast text classification as a classification problem, which the citing paper adopts in their research on text representation.'}, {'Category': 'Methodological Basis', 'Citation': '(Joshi and Rosé, 2009)', 'Explanation': 'The cited work by Joshi and Rosé offers a method of analyzing individual sentences using tree and graph-structured formalisms, which the citing paper utilizes in its research to build document-level representations.'}, {'Category': 'Methodological Basis', 'Citation': '(Dozat and Manning, 2016)', 'Explanation': 'The cited work by Dozat and Manning presents a method of inferring word dependencies to obtain syntactic dependency trees, which the citing paper employs in its research to build document-level representations.'}, {'Category': 'Methodological Basis', 'Citation': '(Yuan et al., 2021)', 'Explanation': 'The cited work by Yuan et al. provides a method of inferring word dependencies to obtain syntactic dependency trees, which the citing paper adopts in its research to build document-level representations.'}, {'Category': 'Methodological Basis', 'Citation': '(Yao et al., 2019)', 'Explanation': 'TextGCN proposes a heterogeneous graph construction using words and documents as nodes, which serves as a methodological basis for the research conducted in the citing paper.'}, {'Category': 'Data Source', 'Citation': '(Liu et al., 2020)', 'Explanation': 'TensorGCN is a data source for the research conducted in the citing paper, as it is one of the proposals that integrate heterogeneous contextual information.'}, {'Category': 'Data Source', 'Citation': '(Ragesh et al., 2021)', 'Explanation': 'Het-eGCN is another data source for the research conducted in the citing paper, as it is another proposal that integrates heterogeneous contextual information.'}, {'Category': 'Data Source', 'Citation': '(Ding et al., 2020)', 'Explanation': 'HyperGAT is a data source for the research conducted in the citing paper, as it is a proposal that integrates heterogeneous contextual information.'}, {'Category': 'Methodological Basis', 'Citation': '(Huang et al., 2019a)', 'Explanation': 'TextLevelGCN creates one graph per input text, which serves as a methodological basis for the research conducted in the citing paper.'}, {'Category': 'Methodological Basis', 'Citation': '(Qian et al., 2019)', 'Explanation': 'The cited work by Qian et al. introduces a GCN-based method for tagging and information extraction tasks, which the citing paper adopts in their research.'}, {'Category': 'Extension or Continuation', 'Citation': '(Nikolentzos et al., 2020)', 'Explanation': 'The cited work by Nikolentzos et al. introduces the master node concept in the graph construction method, which the citing paper extends by including a master node in their research.'}, {'Category': 'Methodological Basis', 'Citation': '(Gu et al., 2023)', 'Explanation': 'The cited work by Gu et al. proposes a heterogeneous graph construction method with topic nodes for class-aware representation learning, which the citing paper builds upon in their research.'}, {'Category': 'Methodological Basis', 'Citation': '(Galke and Scherp, 2022)', 'Explanation': 'The cited work by Galke and Scherp (2022) provides a comparison of different text classification approaches, including Bag of Words (BoW), sequence, and graph models. The citing paper adopts this analysis to evaluate the necessity of text-graphs in text classification.'}, {'Category': 'Methodological Basis', 'Citation': '(Hassan and Banea, 2006)', 'Explanation': 'The cited work by Hassan and Banea (2006) provides the basis for the window-based method of graph construction used in the citing paper.'}, {'Category': 'Methodological Basis', 'Citation': '(Huang et al., 2019b)', 'Explanation': 'The cited work introduces the TextLevelGCN model, which the citing paper adopts to create a more sophisticated graph-based text representation strategy that considers each word token occurrence as a separate node and uses weighted information from neighbors to determine the in-context meaning.'}, {'Category': 'Methodological Basis', 'Citation': '(Devlin et al., 2018)', 'Explanation': 'The cited work introduces the BERT Transformer as a powerful masked language model-based encoder that the citing paper adopts for comparison in the study of text representation schemes.'}, {'Category': 'Data Source', 'Citation': '(Grano et al., 2017)', 'Explanation': 'The cited work provides the App Reviews dataset for fine-grained sentiment analysis in an imbalanced setting, which the citing paper utilizes in their research on assessing the generalizability of graph strategies in text classification.'}, {'Category': 'Data Source', 'Citation': '(Zhang et al., 2015)', 'Explanation': 'The cited work provides the DBpedia dataset for topic classification based on DBpedia 2014 classes, which the citing paper utilizes in their research on assessing the generalizability of graph strategies in text classification.'}, {'Category': 'Data Source', 'Citation': '(Maas et al., 2011)', 'Explanation': 'The cited work provides the IMDB dataset for movie reviews, which the citing paper utilizes in its research for binary sentiment classification.'}, {'Category': 'Data Source', 'Citation': '(Greene and Cunningham, 2006)', 'Explanation': 'The BBC News dataset is cited as a source of English documents for topic classification research in the cited work.'}, {'Category': 'Data Source', 'Citation': '(Kiesel et al., 2018)', 'Explanation': 'The HND dataset is referenced as a source of news articles for hyperpartisan news detection research in the cited work.'}, {'Category': 'Methodological Basis', 'Citation': '(Kipf and Welling 2016)', 'Explanation': 'The cited work provides the traditional graph convolutional neural layer (GCN) that the citing paper uses in their experiments on Intuitive Graphs.'}, {'Category': 'Methodological Basis', 'Citation': '(Xu et al. 2018)', 'Explanation': 'The cited work introduces the graph isomorphism operator (GIN) that the citing paper uses in their experiments to improve structural discriminative power in GNNs.'}, {'Category': 'Methodological Basis', 'Citation': '(Velickovic et al. 2017)', 'Explanation': 'The cited work includes the graph attentional operator (GAT) with 4 attention heads that the citing paper uses in their experiments to improve the performance of GNNs.'}, {'Category': 'Data Source', 'Citation': '(see Appendix E)', 'Explanation': 'The cited work is a PyTorch Geometric implementation that the citing paper uses in their experiments on Intuitive Graphs.'}, {'Category': 'Methodological Basis', 'Citation': '(default parameter settings in the original implementation)', 'Explanation': 'The cited work provides the default parameter settings for TextLevelGCN that the citing paper uses in their experiments.'}, {'Category': 'Data Source', 'Citation': '(GloVe Wiki-Gigaword 300-dim.)', 'Explanation': 'The cited work is a node vector initialization strategy that the citing paper uses in their experiments to compare different node vector initialization strategies in TextLevelGCN.'}, {'Category': 'Methodological Basis', 'Citation': '(Pennington et al., 2014)', 'Explanation': 'The cited work by Pennington et al. provides the embeddings used in the study conducted in the citing paper, which serves as a methodological basis for the text classification task.'}, {'Category': 'Methodological Basis', 'Citation': '(Mikolov et al., 2013)', 'Explanation': 'The cited work by Mikolov et al. provides the Word2Vec Google News 300-dim. embeddings used in the study, which is a methodological basis for the text classification task.'}, {'Category': 'Methodological Basis', 'Citation': '(Pennington et al., 2014)', 'Explanation': 'The cited work by Pennington et al. provides the static BERT pre-trained embeddings used in the study, which is a methodological basis for the text classification task.'}, {'Category': 'Methodological Basis', 'Citation': '(Pennington et al., 2014)', 'Explanation': 'The cited work by Pennington et al. provides the contextualized BERT embeddings used in the study, which is a methodological basis for the text classification task.'}, {'Category': 'Data Source', 'Citation': '(Pennington et al., 2014)', 'Explanation': 'The cited work by Pennington et al. serves as the data source for the BoW vocabulary used in the study, which is a foundational element for the text classification task.'}, {'Category': 'Methodological Basis', 'Citation': '(Kingma and Ba, 2014)', 'Explanation': 'The cited work by Kingma and Ba provides the Adam optimization method used in the study, which is a methodological basis for the text classification task.'}, {'Category': 'Methodological Basis', 'Citation': '(Xu et al., 2018)', 'Explanation': 'The cited work by Xu et al. (2018) introduces the GCN model, which the citing paper adopts as a method for improving discriminative power in GNN message passing for certain tasks.'}, {'Category': 'Data Source', 'Citation': '(Grano et al., 2017)', 'Explanation': 'The App Reviews dataset is a collection of user reviews of Android applications that is used in the text classification experiments conducted in the citing paper.'}, {'Category': 'Data Source', 'Citation': '(Zhang et al., 2015)', 'Explanation': 'The DBpedia ontology classification dataset is a collection of Wikipedia articles that is used for topic classification in the text classification experiments.'}, {'Category': 'Data Source', 'Citation': '(Lehmann et al., 2015)', 'Explanation': 'The DBpedia ontology classification dataset is based on the DBpedia 2014 knowledge base, which is a multilingual knowledge base that is referenced in the original DBpedia.'}, {'Category': 'Data Source', 'Citation': '(Maas et al., 2011)', 'Explanation': 'The cited work provides a dataset of English language movie reviews for binary sentiment classification that the citing paper uses in its research.'}, {'Category': 'Data Source', 'Citation': '(Greene and Cunningham, 2006)', 'Explanation': 'The cited work provides a dataset of English documents from the BBC News website that the citing paper uses in its research.'}, {'Category': 'Data Source', 'Citation': '(Kiesel et al., 2018)', 'Explanation': 'The cited work provides a dataset of English samples for hyperpartisan news detection that the citing paper uses in its research.'}, {'Category': 'Supporting Evidence', 'Citation': '(Kiesel et al., 2019)', 'Explanation': 'The cited work by Kiesel et al. provides a detailed analysis of the characteristics of hyperpartisan language, which serves as a foundational basis for the citing paper in understanding the nature of the task and the challenges involved in detecting hyperpartisan language.'}], 'b0': [{'Category': 'Methodological Basis', 'Citation': '(Arora et al., 2009)', 'Explanation': 'The cited work by Arora et al. provides a method of analyzing individual sentences using tree and graph-structured formalisms, which the citing paper adopts in its research to build document-level representations.'}], 'b4': [{'Category': 'Methodological Basis', 'Citation': '(Li et al., 2015;Cho et al., 2014)', 'Explanation': 'The cited works by Li et al. and Cho et al. use a Gated Recurrent Unit-based message passing function for updating node feature vectors, which the citing paper adapts in their research.'}], 'b1': [{'Category': 'Methodological Basis', 'Citation': '(Beltagy et al., 2020)', 'Explanation': 'The cited work presents the Longformer Transformer as a modified attention mechanism that extends the maximum input length of the BERT model, providing a basis for comparison in the study of text representation schemes.'}]}\n"
          ]
        }
      ],
      "source": [
        "# Function to regroup citations by ref_id\n",
        "def regroup_citations_by_ref_id(citations):\n",
        "    grouped_citations = {}\n",
        "    for citation in citations:\n",
        "        if 'ref_id' in citation.keys():\n",
        "            ref_id = citation['ref_id']\n",
        "            # Create a copy of the citation without the ref_id\n",
        "            citation_copy = {k: v for k, v in citation.items() if k != 'ref_id'}\n",
        "            # Append the citation to the list associated with its ref_id\n",
        "            if ref_id in grouped_citations:\n",
        "                grouped_citations[ref_id].append(citation_copy)\n",
        "            else:\n",
        "                grouped_citations[ref_id] = [citation_copy]\n",
        "    return grouped_citations\n",
        "\n",
        "\n",
        "# Regroup the citationb list by ref_id\n",
        "grouped_citations = regroup_citations_by_ref_id(annotated_article[1]['citation_data'])\n",
        "print(grouped_citations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "li39UK6WqPVk"
      },
      "source": [
        "Let's combine all the steps together into one function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6db6SOqoqPVk"
      },
      "outputs": [],
      "source": [
        "def preprocess_citation_author_year(article):\n",
        "    for citation in article['citation_data']:\n",
        "        try:\n",
        "            parsed_name = split_and_parse_citation(citation['Citation'])\n",
        "            match = match_citations_with_references(parsed_name, article['references'])\n",
        "            citation['ref_id'] = match['ref_id'] if match else None\n",
        "        except:\n",
        "            citation['ref_id'] = None\n",
        "    return article"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ncLhEdYqPVl"
      },
      "source": [
        "Now we have a grouped citation data for author-year citation style, let's start solving cases with numeric-style."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whgNhpyZqPVl"
      },
      "source": [
        "### **2.2.2 Handle Numeric Citation Style**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPCWUtwpqPVl"
      },
      "source": [
        "This style of citation seems simple at first, but there are many edge cases that we have to deal with. From my observation, there are 3 main types:\n",
        "\n",
        "- Singular citations such as [1] or [4]: These are processed conventionally, where the reference ID equals the citation number minus one.\n",
        "- Lists, for instance [1, 4, 6]: In this scenario, the citations are split into individual entries: [1], [4], and [6].\n",
        "- Ranges, like [1 - 5]: Here, the citation is divided into separate entries: [1], [2], [3], [4], [5].\n",
        "- Mixed ranges, such as [1] - [5]: These are split into distinct citations: [1] and [5]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wl53UOkqPVl"
      },
      "outputs": [],
      "source": [
        "def split_numeric_citations(citations):\n",
        "    # Helper function to parse ranges and individual numbers\n",
        "    def parse_part(part):\n",
        "        if '-' in part:  # Handle ranges\n",
        "            start, end = map(int, part.split('-'))\n",
        "            return list(range(start, end + 1))\n",
        "        else:  # Handle individual numbers\n",
        "            return [int(part)]\n",
        "\n",
        "    # Initialize the result list\n",
        "    result = []\n",
        "\n",
        "    # Find all parts of the input that match the patterns\n",
        "    parts = re.findall(r'\\[([^]]+)]', citations)\n",
        "\n",
        "    for part in parts:\n",
        "        # For each part, remove spaces, split by commas and extend the result list\n",
        "        for subpart in part.replace(' ', '').split(','):\n",
        "            try:\n",
        "                result.extend(parse_part(subpart))\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "    return [f\"[{num}]\" for num in result]\n",
        "\n",
        "# Function to apply citation splitting to a list of citation entries\n",
        "def split_citations_in_entries(citation_entries):\n",
        "    expanded_citation_entries = []\n",
        "    for entry in citation_entries:\n",
        "        try:\n",
        "        # Use the split_citations function to get a list of individual citations from the Citation field\n",
        "            split_citations_list = split_numeric_citations(entry['Citation'])\n",
        "            for citation in split_citations_list:\n",
        "                # Create a new citation entry for each split citation, keeping other fields the same\n",
        "\n",
        "                new_entry = {\n",
        "                    'Citation': citation,\n",
        "                    'Category': entry['Category'],\n",
        "                    'Explanation': entry['Explanation']\n",
        "                }\n",
        "                expanded_citation_entries.append(new_entry)\n",
        "        except:\n",
        "            continue\n",
        "    return expanded_citation_entries\n",
        "\n",
        "\n",
        "def match_numeric_citation(citations):\n",
        "    for citation in citations:\n",
        "        # Regular expression to find single numbers inside square brackets\n",
        "        pattern = re.compile(r'\\[\\(?(?P<number>\\d+)\\)?\\]')\n",
        "        try:\n",
        "            #Find all matches in the text and convert them to integers\n",
        "            reference_num = [int(match.group('number')) for match in pattern.finditer(citation['Citation'])][0]\n",
        "            citation['ref_id'] = f\"b{reference_num -1}\"\n",
        "\n",
        "        except:\n",
        "            continue\n",
        "    return citations\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BhXUgv9qPVm"
      },
      "source": [
        "Before parsing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pgSP5jbqPVm",
        "outputId": "a1785358-07d7-4342-f77d-c261633908f3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'Category': 'Methodological Basis',\n",
              "  'Citation': '[29,72]',\n",
              "  'Explanation': 'The cited works provide a method of using the features learned by predicting different auxiliary maps to assist in predicting saliency maps, which the citing paper adopts in its research on saliency object detection.'},\n",
              " {'Category': 'Methodological Basis',\n",
              "  'Citation': '[52]',\n",
              "  'Explanation': 'The cited work uses auxiliary maps as input to guide the training process, which the citing paper adopts in its research on saliency object detection.'},\n",
              " {'Category': 'Methodological Basis',\n",
              "  'Citation': '[46,14]',\n",
              "  'Explanation': 'The cited works introduce a boundary-aware loss to make the models pay more attention to edge pixels, which the citing paper adopts in its research on saliency object detection.'},\n",
              " {'Category': 'Methodological Basis',\n",
              "  'Citation': '[58,60]',\n",
              "  'Explanation': 'The cited works provide a method of using multiple heads in a single encoder to learn different semantic information, which the citing paper adopts as a basis for their research on saliency map prediction.'},\n",
              " {'Category': 'Methodological Basis',\n",
              "  'Citation': '[74]',\n",
              "  'Explanation': 'The cited work highlights the inefficiency of using multiple branches in parallel for saliency map prediction, which the citing paper uses to guide their method design for efficient saliency map prediction.'}]"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "annotated_article[108]['citation_data'][:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3v5un4eqPVm"
      },
      "source": [
        "After parsing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CprQULkIqPVm"
      },
      "outputs": [],
      "source": [
        "annotated_article[108]['citation_data'] = split_citations_in_entries(annotated_article[108]['citation_data'])\n",
        "annotated_article[108]['citation_data'] =  match_numeric_citation(annotated_article[108]['citation_data'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XIS3F02qPVm",
        "outputId": "16dee40b-696c-4b5b-c4b3-b2e2991a825a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'Citation': '[29]',\n",
              "  'Category': 'Methodological Basis',\n",
              "  'Explanation': 'The cited works provide a method of using the features learned by predicting different auxiliary maps to assist in predicting saliency maps, which the citing paper adopts in its research on saliency object detection.',\n",
              "  'ref_id': 'b28'},\n",
              " {'Citation': '[72]',\n",
              "  'Category': 'Methodological Basis',\n",
              "  'Explanation': 'The cited works provide a method of using the features learned by predicting different auxiliary maps to assist in predicting saliency maps, which the citing paper adopts in its research on saliency object detection.',\n",
              "  'ref_id': 'b71'},\n",
              " {'Citation': '[52]',\n",
              "  'Category': 'Methodological Basis',\n",
              "  'Explanation': 'The cited work uses auxiliary maps as input to guide the training process, which the citing paper adopts in its research on saliency object detection.',\n",
              "  'ref_id': 'b51'},\n",
              " {'Citation': '[46]',\n",
              "  'Category': 'Methodological Basis',\n",
              "  'Explanation': 'The cited works introduce a boundary-aware loss to make the models pay more attention to edge pixels, which the citing paper adopts in its research on saliency object detection.',\n",
              "  'ref_id': 'b45'},\n",
              " {'Citation': '[14]',\n",
              "  'Category': 'Methodological Basis',\n",
              "  'Explanation': 'The cited works introduce a boundary-aware loss to make the models pay more attention to edge pixels, which the citing paper adopts in its research on saliency object detection.',\n",
              "  'ref_id': 'b13'},\n",
              " {'Citation': '[58]',\n",
              "  'Category': 'Methodological Basis',\n",
              "  'Explanation': 'The cited works provide a method of using multiple heads in a single encoder to learn different semantic information, which the citing paper adopts as a basis for their research on saliency map prediction.',\n",
              "  'ref_id': 'b57'}]"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "annotated_article[108]['citation_data'][:6]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cF2pb3jmqPVm"
      },
      "source": [
        "As you can see, single citation like [14], [58] will be parsed normaly. But for citation like [29, 72], they will get split to 2 separated citations [29] and [72].\n",
        "\n",
        "Now, let's combine the steps together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJC4C7t3qPVn"
      },
      "outputs": [],
      "source": [
        "def proprocess_citation_numeric(article):\n",
        "\n",
        "    article['citation_data'] = split_citations_in_entries(article['citation_data'])\n",
        "    article['citation_data'] = match_numeric_citation(article['citation_data'])\n",
        "    return article"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUat-VWDqPVn"
      },
      "source": [
        "### **2.2.3 Process 2 citation style**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMkw2KKRqPVn"
      },
      "source": [
        "First we need to detect the citation style"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPgD_SDLqPVn"
      },
      "outputs": [],
      "source": [
        "def detect_citation_style(text):\n",
        "    # Pattern to match numeric citations like [1], [1, 2], [1-6], [1, 2-6], [1, 2, 3-6], etc.\n",
        "    numeric_pattern = re.compile(r'\\[\\d+(-\\d+)?(,\\s*\\d+(-\\d+)?)*\\]')\n",
        "    # Pattern for \"Author-Year\" citations like (Author, Year)\n",
        "    author_year_pattern = re.compile(r'\\([A-Za-z]+,\\s*\\d{4}\\)')\n",
        "\n",
        "    # Check for numeric citation style\n",
        "    if numeric_pattern.search(text):\n",
        "        return \"Numeric\"\n",
        "    # Check for author-year citation style\n",
        "    elif author_year_pattern.search(text):\n",
        "        return \"Author-Year\"\n",
        "    else:\n",
        "        return \"Author-Year\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFWeYBqUqPVn",
        "outputId": "f1d6b8c2-5bbe-4c41-86b1-3a291aab8648"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Author-Year'"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "detect_citation_style(\"(Amin et al., 2019)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10QACFRzqPVn",
        "outputId": "8cf202f9-cfe6-49d6-b3df-367b85444e4c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Numeric'"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "detect_citation_style(\"[1,6]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2gaveUrqPVn",
        "outputId": "840cf36f-1db3-4bae-e470-ddf7faf131b0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7243/7243 [00:12<00:00, 576.70it/s]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "for article in tqdm(annotated_article, total=len(annotated_article)):\n",
        "    references = article['references']\n",
        "    if len(article['citation_data']) == 0:\n",
        "        continue\n",
        "    citation_style = detect_citation_style(article['citation_data'][0][\"Citation\"])\n",
        "    # try:\n",
        "    if citation_style == \"Author-Year\":\n",
        "        article = preprocess_citation_author_year(article)\n",
        "    elif citation_style == \"Numeric\":\n",
        "        article = proprocess_citation_numeric(article)\n",
        "    else:\n",
        "        print(f\"Uncertain citation style: {citation_style}\")\n",
        "        continue\n",
        "    # except Exception as e:\n",
        "    #     print(article['citation_data'])\n",
        "    #     print(e)\n",
        "    #     break\n",
        "\n",
        "    grouped_citations = regroup_citations_by_ref_id(article['citation_data'])\n",
        "    article['grouped_citations'] = grouped_citations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MrJbVKBFqPVo",
        "outputId": "8c33bb87-ed22-40d6-ec07-40b2935ce960"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'GENTLE: A Genre-Diverse Multilayer Challenge Set for English NLP and Linguistic Evaluation'"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "annotated_article[1000]['title']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BBTLH95qPVo",
        "outputId": "c46e01ea-16d8-43eb-ba0b-a0148646bd63"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'b3': [{'Category': 'Methodological Basis',\n",
              "   'Citation': '(Castillo et al., 2015)',\n",
              "   'Explanation': 'The cited work provides a foundation for the use of graphs in text classification tasks, as it discusses the applicability and effectiveness of graphs in broader settings.'},\n",
              "  {'Category': 'Extension or Continuation',\n",
              "   'Citation': '(Castillo et al., 2017)',\n",
              "   'Explanation': 'The cited work is a continuation of the research on graph representations in text classification, as it further explores the use of graphs in more diverse scenarios.'},\n",
              "  {'Category': 'Data Source',\n",
              "   'Citation': '(Castillo et al., , 2017)',\n",
              "   'Explanation': 'The cited work is a data source for the text classification tasks used in the study conducted in the citing paper.'},\n",
              "  {'Category': 'Methodological Basis',\n",
              "   'Citation': '(Castillo et al., 2015)',\n",
              "   'Explanation': 'The cited work by Castillo et al. (2015) uses sequence graphs to reflect the original order of words in text, which the citing paper builds upon in their research on text representation.'},\n",
              "  {'Category': 'Methodological Basis',\n",
              "   'Citation': '(Castillo et al., 2015)',\n",
              "   'Explanation': 'The cited work by Castillo et al. (2015) serves as the methodological basis for the sequence-weighted method of graph construction in the citing paper.'}],\n",
              " 'b25': [{'Category': 'Methodological Basis',\n",
              "   'Citation': '(Mihalcea and Tarau, 2004)',\n",
              "   'Explanation': 'The cited work by Mihalcea and Tarau (2004) provides a method of using co-occurrence graphs for keyword extraction, which the citing paper adopts in their research on text representation.'}],\n",
              " 'b12': [{'Category': 'Methodological Basis',\n",
              "   'Citation': '(Hassan and Banea, 2006)',\n",
              "   'Explanation': 'The cited work by Hassan and Banea (2006) uses a co-occurrence graph with N = 2 and TextRank to replace term frequency weights, which the citing paper builds upon in their text classification research.'},\n",
              "  {'Category': 'Methodological Basis',\n",
              "   'Citation': '(Hassan and Banea, 2006)',\n",
              "   'Explanation': 'The cited work by Hassan and Banea (2006) provides the basis for the window-based method of graph construction used in the citing paper.'}],\n",
              " 'b32': [{'Category': 'Methodological Basis',\n",
              "   'Citation': '(Rousseau et al., 2015)',\n",
              "   'Explanation': 'The cited work by Rousseau et al. (2015) uses a graph-of-words approach to cast text classification as a classification problem, which the citing paper adopts in their research on text representation.'}],\n",
              " 'b0': [{'Category': 'Methodological Basis',\n",
              "   'Citation': '(Arora et al., 2009)',\n",
              "   'Explanation': 'The cited work by Arora et al. provides a method of analyzing individual sentences using tree and graph-structured formalisms, which the citing paper adopts in its research to build document-level representations.'}],\n",
              " 'b16': [{'Category': 'Methodological Basis',\n",
              "   'Citation': '(Joshi and Rosé, 2009)',\n",
              "   'Explanation': 'The cited work by Joshi and Rosé offers a method of analyzing individual sentences using tree and graph-structured formalisms, which the citing paper utilizes in its research to build document-level representations.'}],\n",
              " 'b7': [{'Category': 'Methodological Basis',\n",
              "   'Citation': '(Dozat and Manning, 2016)',\n",
              "   'Explanation': 'The cited work by Dozat and Manning presents a method of inferring word dependencies to obtain syntactic dependency trees, which the citing paper employs in its research to build document-level representations.'}],\n",
              " 'b37': [{'Category': 'Methodological Basis',\n",
              "   'Citation': '(Yuan et al., 2021)',\n",
              "   'Explanation': 'The cited work by Yuan et al. provides a method of inferring word dependencies to obtain syntactic dependency trees, which the citing paper adopts in its research to build document-level representations.'}],\n",
              " 'b36': [{'Category': 'Methodological Basis',\n",
              "   'Citation': '(Yao et al., 2019)',\n",
              "   'Explanation': 'TextGCN proposes a heterogeneous graph construction using words and documents as nodes, which serves as a methodological basis for the research conducted in the citing paper.'}],\n",
              " 'b23': [{'Category': 'Data Source',\n",
              "   'Citation': '(Liu et al., 2020)',\n",
              "   'Explanation': 'TensorGCN is a data source for the research conducted in the citing paper, as it is one of the proposals that integrate heterogeneous contextual information.'}],\n",
              " 'b31': [{'Category': 'Data Source',\n",
              "   'Citation': '(Ragesh et al., 2021)',\n",
              "   'Explanation': 'Het-eGCN is another data source for the research conducted in the citing paper, as it is another proposal that integrates heterogeneous contextual information.'}],\n",
              " 'b6': [{'Category': 'Data Source',\n",
              "   'Citation': '(Ding et al., 2020)',\n",
              "   'Explanation': 'HyperGAT is a data source for the research conducted in the citing paper, as it is a proposal that integrates heterogeneous contextual information.'}],\n",
              " 'b15': [{'Category': 'Methodological Basis',\n",
              "   'Citation': '(Huang et al., 2019a)',\n",
              "   'Explanation': 'TextLevelGCN creates one graph per input text, which serves as a methodological basis for the research conducted in the citing paper.'},\n",
              "  {'Category': 'Methodological Basis',\n",
              "   'Citation': '(Huang et al., 2019b)',\n",
              "   'Explanation': 'The cited work introduces the TextLevelGCN model, which the citing paper adopts to create a more sophisticated graph-based text representation strategy that considers each word token occurrence as a separate node and uses weighted information from neighbors to determine the in-context meaning.'}],\n",
              " 'b30': [{'Category': 'Methodological Basis',\n",
              "   'Citation': '(Qian et al., 2019)',\n",
              "   'Explanation': 'The cited work by Qian et al. introduces a GCN-based method for tagging and information extraction tasks, which the citing paper adopts in their research.'}],\n",
              " 'b4': [{'Category': 'Methodological Basis',\n",
              "   'Citation': '(Li et al., 2015;Cho et al., 2014)',\n",
              "   'Explanation': 'The cited works by Li et al. and Cho et al. use a Gated Recurrent Unit-based message passing function for updating node feature vectors, which the citing paper adapts in their research.'}],\n",
              " 'b28': [{'Category': 'Extension or Continuation',\n",
              "   'Citation': '(Nikolentzos et al., 2020)',\n",
              "   'Explanation': 'The cited work by Nikolentzos et al. introduces the master node concept in the graph construction method, which the citing paper extends by including a master node in their research.'}],\n",
              " 'b11': [{'Category': 'Methodological Basis',\n",
              "   'Citation': '(Gu et al., 2023)',\n",
              "   'Explanation': 'The cited work by Gu et al. proposes a heterogeneous graph construction method with topic nodes for class-aware representation learning, which the citing paper builds upon in their research.'}],\n",
              " 'b8': [{'Category': 'Methodological Basis',\n",
              "   'Citation': '(Galke and Scherp, 2022)',\n",
              "   'Explanation': 'The cited work by Galke and Scherp (2022) provides a comparison of different text classification approaches, including Bag of Words (BoW), sequence, and graph models. The citing paper adopts this analysis to evaluate the necessity of text-graphs in text classification.'}],\n",
              " 'b5': [{'Category': 'Methodological Basis',\n",
              "   'Citation': '(Devlin et al., 2018)',\n",
              "   'Explanation': 'The cited work introduces the BERT Transformer as a powerful masked language model-based encoder that the citing paper adopts for comparison in the study of text representation schemes.'}],\n",
              " 'b1': [{'Category': 'Methodological Basis',\n",
              "   'Citation': '(Beltagy et al., 2020)',\n",
              "   'Explanation': 'The cited work presents the Longformer Transformer as a modified attention mechanism that extends the maximum input length of the BERT model, providing a basis for comparison in the study of text representation schemes.'}],\n",
              " 'b9': [{'Category': 'Data Source',\n",
              "   'Citation': '(Grano et al., 2017)',\n",
              "   'Explanation': 'The cited work provides the App Reviews dataset for fine-grained sentiment analysis in an imbalanced setting, which the citing paper utilizes in their research on assessing the generalizability of graph strategies in text classification.'},\n",
              "  {'Category': 'Data Source',\n",
              "   'Citation': '(Grano et al., 2017)',\n",
              "   'Explanation': 'The App Reviews dataset is a collection of user reviews of Android applications that is used in the text classification experiments conducted in the citing paper.'}],\n",
              " 'b39': [{'Category': 'Data Source',\n",
              "   'Citation': '(Zhang et al., 2015)',\n",
              "   'Explanation': 'The cited work provides the DBpedia dataset for topic classification based on DBpedia 2014 classes, which the citing paper utilizes in their research on assessing the generalizability of graph strategies in text classification.'},\n",
              "  {'Category': 'Data Source',\n",
              "   'Citation': '(Zhang et al., 2015)',\n",
              "   'Explanation': 'The DBpedia ontology classification dataset is a collection of Wikipedia articles that is used for topic classification in the text classification experiments.'}],\n",
              " 'b24': [{'Category': 'Data Source',\n",
              "   'Citation': '(Maas et al., 2011)',\n",
              "   'Explanation': 'The cited work provides the IMDB dataset for movie reviews, which the citing paper utilizes in its research for binary sentiment classification.'},\n",
              "  {'Category': 'Data Source',\n",
              "   'Citation': '(Maas et al., 2011)',\n",
              "   'Explanation': 'The cited work provides a dataset of English language movie reviews for binary sentiment classification that the citing paper uses in its research.'}],\n",
              " 'b10': [{'Category': 'Data Source',\n",
              "   'Citation': '(Greene and Cunningham, 2006)',\n",
              "   'Explanation': 'The BBC News dataset is cited as a source of English documents for topic classification research in the cited work.'},\n",
              "  {'Category': 'Data Source',\n",
              "   'Citation': '(Greene and Cunningham, 2006)',\n",
              "   'Explanation': 'The cited work provides a dataset of English documents from the BBC News website that the citing paper uses in its research.'}],\n",
              " 'b18': [{'Category': 'Data Source',\n",
              "   'Citation': '(Kiesel et al., 2018)',\n",
              "   'Explanation': 'The HND dataset is referenced as a source of news articles for hyperpartisan news detection research in the cited work.'},\n",
              "  {'Category': 'Data Source',\n",
              "   'Citation': '(Kiesel et al., 2018)',\n",
              "   'Explanation': 'The cited work provides a dataset of English samples for hyperpartisan news detection that the citing paper uses in its research.'},\n",
              "  {'Category': 'Supporting Evidence',\n",
              "   'Citation': '(Kiesel et al., 2019)',\n",
              "   'Explanation': 'The cited work by Kiesel et al. provides a detailed analysis of the characteristics of hyperpartisan language, which serves as a foundational basis for the citing paper in understanding the nature of the task and the challenges involved in detecting hyperpartisan language.'}],\n",
              " None: [{'Category': 'Methodological Basis',\n",
              "   'Citation': '(Kipf and Welling 2016)',\n",
              "   'Explanation': 'The cited work provides the traditional graph convolutional neural layer (GCN) that the citing paper uses in their experiments on Intuitive Graphs.'},\n",
              "  {'Category': 'Data Source',\n",
              "   'Citation': '(see Appendix E)',\n",
              "   'Explanation': 'The cited work is a PyTorch Geometric implementation that the citing paper uses in their experiments on Intuitive Graphs.'},\n",
              "  {'Category': 'Methodological Basis',\n",
              "   'Citation': '(default parameter settings in the original implementation)',\n",
              "   'Explanation': 'The cited work provides the default parameter settings for TextLevelGCN that the citing paper uses in their experiments.'},\n",
              "  {'Category': 'Data Source',\n",
              "   'Citation': '(GloVe Wiki-Gigaword 300-dim.)',\n",
              "   'Explanation': 'The cited work is a node vector initialization strategy that the citing paper uses in their experiments to compare different node vector initialization strategies in TextLevelGCN.'},\n",
              "  {'Category': 'Methodological Basis',\n",
              "   'Citation': '(Kingma and Ba, 2014)',\n",
              "   'Explanation': 'The cited work by Kingma and Ba provides the Adam optimization method used in the study, which is a methodological basis for the text classification task.'}],\n",
              " 'b35': [{'Category': 'Methodological Basis',\n",
              "   'Citation': '(Xu et al. 2018)',\n",
              "   'Explanation': 'The cited work introduces the graph isomorphism operator (GIN) that the citing paper uses in their experiments to improve structural discriminative power in GNNs.'},\n",
              "  {'Category': 'Methodological Basis',\n",
              "   'Citation': '(Xu et al., 2018)',\n",
              "   'Explanation': 'The cited work by Xu et al. (2018) introduces the GCN model, which the citing paper adopts as a method for improving discriminative power in GNN message passing for certain tasks.'}],\n",
              " 'b33': [{'Category': 'Methodological Basis',\n",
              "   'Citation': '(Velickovic et al. 2017)',\n",
              "   'Explanation': 'The cited work includes the graph attentional operator (GAT) with 4 attention heads that the citing paper uses in their experiments to improve the performance of GNNs.'}],\n",
              " 'b29': [{'Category': 'Methodological Basis',\n",
              "   'Citation': '(Pennington et al., 2014)',\n",
              "   'Explanation': 'The cited work by Pennington et al. provides the embeddings used in the study conducted in the citing paper, which serves as a methodological basis for the text classification task.'},\n",
              "  {'Category': 'Methodological Basis',\n",
              "   'Citation': '(Pennington et al., 2014)',\n",
              "   'Explanation': 'The cited work by Pennington et al. provides the static BERT pre-trained embeddings used in the study, which is a methodological basis for the text classification task.'},\n",
              "  {'Category': 'Methodological Basis',\n",
              "   'Citation': '(Pennington et al., 2014)',\n",
              "   'Explanation': 'The cited work by Pennington et al. provides the contextualized BERT embeddings used in the study, which is a methodological basis for the text classification task.'},\n",
              "  {'Category': 'Data Source',\n",
              "   'Citation': '(Pennington et al., 2014)',\n",
              "   'Explanation': 'The cited work by Pennington et al. serves as the data source for the BoW vocabulary used in the study, which is a foundational element for the text classification task.'}],\n",
              " 'b26': [{'Category': 'Methodological Basis',\n",
              "   'Citation': '(Mikolov et al., 2013)',\n",
              "   'Explanation': 'The cited work by Mikolov et al. provides the Word2Vec Google News 300-dim. embeddings used in the study, which is a methodological basis for the text classification task.'}],\n",
              " 'b21': [{'Category': 'Data Source',\n",
              "   'Citation': '(Lehmann et al., 2015)',\n",
              "   'Explanation': 'The DBpedia ontology classification dataset is based on the DBpedia 2014 knowledge base, which is a multilingual knowledge base that is referenced in the original DBpedia.'}]}"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "annotated_article[1]['grouped_citations']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBMgzKlVqPVo"
      },
      "outputs": [],
      "source": [
        "# annotated_article[1000]['references']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWY5l-zoqPVo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rESD2XYqPVo"
      },
      "source": [
        "## **2.3 Building citation graph**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMfmBL5yqPVo"
      },
      "source": [
        "### **2.3.1 Parsing annotated triplets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3FP4PlsqPVo"
      },
      "outputs": [],
      "source": [
        "relationships_dict = {\n",
        "    \"Supporting Evidence\": \"Is Evidence For\",\n",
        "    \"Methodological Basis\": \"Is Methodological Basis For\",\n",
        "    \"Theoretical Foundation\": \"Is Theoretical Foundation For\",\n",
        "    \"Data Source\": \"Is Data Source For\",\n",
        "    \"Extension or Continuation\": \"Is Extension or Continuation Of\",\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3tglTzDqPVp"
      },
      "source": [
        "We have grouped citation data; now we need to find the papers cited in the arXiv dataset by name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDMaTC0KqPVp",
        "outputId": "580156c3-ff1c-48ce-e6a3-c7fdd8083e6a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7243/7243 [00:01<00:00, 3757.91it/s]\n"
          ]
        }
      ],
      "source": [
        "# df_data['title'] = df_data['title'].str.lower()\n",
        "# titles = df_data['title'].tolist()\n",
        "title_dict = {title.lower(): arxiv_id for title, arxiv_id in zip(df_data['title'].tolist(), df_data['id'].to_list())}\n",
        "\n",
        "\n",
        "def search_paper_by_name(name):\n",
        "    # matches = df_data['title'].str.contains(name, case=False, na=False, regex=False)\n",
        "    # filtered_df = df_data[matches]\n",
        "    # if len(filtered_df) == 0:\n",
        "    #     return None\n",
        "    # return filtered_df.iloc[0]['id']\n",
        "    try:\n",
        "        return title_dict[name]\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "for article_dict in tqdm(annotated_article, total=len(annotated_article)):\n",
        "\n",
        "    article_dict[\"arxiv_id\"] = search_paper_by_name(article_dict['title'].lower())\n",
        "\n",
        "    if \"grouped_citations\" in article_dict.keys():\n",
        "        article_dict[\"mapped_citation\"] = {}\n",
        "        for key,val in article_dict['grouped_citations'].items():\n",
        "            for ref in article_dict[\"references\"]:\n",
        "                if ref[\"ref_id\"] == key:\n",
        "                    title = ref[\"title\"]\n",
        "\n",
        "            title = title.lower()\n",
        "            arxiv_id = search_paper_by_name(title)\n",
        "            article_dict['mapped_citation'][key] = {\"title\": title, 'arxiv_id': arxiv_id, 'citation': val}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRcSC6hJqPVp",
        "outputId": "e3d9ecfc-0d33-40a3-84a9-bbe50d807da5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'GENTLE: A Genre-Diverse Multilayer Challenge Set for English NLP and Linguistic Evaluation'"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "annotated_article[1000]['title']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sCjQDKTqPVp"
      },
      "outputs": [],
      "source": [
        "# annotated_article[1024]['mapped_citation']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLmhUz37qPVp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbRmLzs7qPVp"
      },
      "source": [
        "Let's define a class for a paper node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3G39HUCqPVq"
      },
      "outputs": [],
      "source": [
        "class PaperNode:\n",
        "    title: str\n",
        "    arxiv_id: str\n",
        "\n",
        "    def __init__(self, title, arxiv_id):\n",
        "        self.title = title\n",
        "        self.arxiv_id = arxiv_id\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        return f\"Title: {self.title},\\n Arxiv ID: {self.arxiv_id}\"\n",
        "\n",
        "class PaperEdge:\n",
        "    category: str\n",
        "    explanation: str\n",
        "    verbose = True\n",
        "\n",
        "    def __init__(self, category, explanation):\n",
        "        self.category = category\n",
        "        self.explanation = explanation\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        if self.verbose:\n",
        "            return f\"Category: {self.category},\\n Explanation: {self.explanation}\"\n",
        "        else:\n",
        "            return f\"Category: {self.category}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbPNyfapqPVq",
        "outputId": "7b40369a-a126-4268-d0fb-d8fc1ec294a5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7243/7243 [00:00<00:00, 30806.79it/s]\n"
          ]
        }
      ],
      "source": [
        "paper_dict = {}\n",
        "\n",
        "for article_dict in tqdm(annotated_article, total=len(annotated_article)):\n",
        "    paper_dict[article_dict['title'].lower()] = PaperNode(title=article_dict['title'], arxiv_id=article_dict['arxiv_id'])\n",
        "\n",
        "    if \"mapped_citation\" in article_dict.keys():\n",
        "        for key,val in article_dict['mapped_citation'].items():\n",
        "            title = val['title']\n",
        "            if title not in paper_dict.keys():\n",
        "                paper_node = PaperNode(title=val['title'], arxiv_id=val['arxiv_id'])\n",
        "                paper_dict[title] = paper_node\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vh-_4ZGoqPVt",
        "outputId": "fc73eb06-9a9a-4821-a437-443fb2accff3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "112611"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(paper_dict.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfPdICEiqPVt",
        "outputId": "a52da669-5ad8-4bd3-e1a1-df68c7e6871a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Title: make-a-video: text-to-video generation without text-video data,\n",
            " Arxiv ID: 2209.14792\n"
          ]
        }
      ],
      "source": [
        "print(paper_dict[\"make-a-video: text-to-video generation without text-video data\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_8Lklo1qPVt",
        "outputId": "7d58fdad-7711-4cb1-8c81-21ad273f7714"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'b3': {'title': 'active learning with statistical models',\n",
              "  'arxiv_id': 'cs/9603104',\n",
              "  'citation': [{'Category': 'Methodological Basis',\n",
              "    'Citation': '(Cohn et al., 1996)',\n",
              "    'Explanation': 'The cited work introduces the concept of active learning as a potential solution to the challenge of data labeling in low-resource settings, which the citing paper builds upon in its research on efficient finetuning methods for PLMs.'}]},\n",
              " 'b37': {'title': 'active learning literature survey',\n",
              "  'arxiv_id': None,\n",
              "  'citation': [{'Category': 'Methodological Basis',\n",
              "    'Citation': '(Settles, 2009)',\n",
              "    'Explanation': 'The cited work provides a more in-depth discussion of active learning and its potential benefits in reducing labeling costs, which the citing paper further explores in the context of PLMs and low-resource settings.'}]},\n",
              " 'b4': {'title': 'two faces of active learning',\n",
              "  'arxiv_id': None,\n",
              "  'citation': [{'Category': 'Methodological Basis',\n",
              "    'Citation': '(Dasgupta, 2011)',\n",
              "    'Explanation': 'The cited work highlights the importance of label complexity in active learning and the need to reduce it for efficient model training, which the citing paper addresses in its research on efficient finetuning methods for PLMs in low-resource settings.'}]},\n",
              " 'b11': {'title': \"don't stop pretraining: adapt language models to domains and tasks\",\n",
              "  'arxiv_id': '2004.10964',\n",
              "  'citation': [{'Category': 'Methodological Basis',\n",
              "    'Citation': '(Gururangan et al., 2020)',\n",
              "    'Explanation': 'The cited work introduces the concept of task-adaptive pre-training (TAPT), which the citing paper adopts in their research to further reduce the label complexity in AL research.'}]},\n",
              " 'b14': {'title': 'parameter-efficient transfer learning for nlp',\n",
              "  'arxiv_id': '1902.00751',\n",
              "  'citation': [{'Category': 'Extension or Continuation',\n",
              "    'Citation': '(Houlsby et al., 2019)',\n",
              "    'Explanation': 'The cited work introduces the concept of adapters as compact modules for fine-tuning PLMs, which the citing paper extends by discussing the use of adapters for parameter-efficient fine-tuning (PEFT) in AL research.'},\n",
              "   {'Category': 'Methodological Basis',\n",
              "    'Citation': '(Houlsby et al., 2019)',\n",
              "    'Explanation': 'The cited work introduces the concept of trainable bottleneck layers in Transformer layers, which the citing paper adopts in the development of the Adapter PEFT technique.'}]},\n",
              " 'b34': {'title': 'modular deep learning',\n",
              "  'arxiv_id': '2302.11529',\n",
              "  'citation': [{'Category': 'Data Source',\n",
              "    'Citation': '(Pfeiffer et al., 2023)',\n",
              "    'Explanation': 'The cited work discusses the use of modular learning in PEFT, which the citing paper references as a method for parameter-efficient fine-tuning in AL research.'},\n",
              "   {'Category': 'Methodological Basis',\n",
              "    'Citation': '(Pfeiffer et al., 2020)',\n",
              "    'Explanation': 'The cited work provides the implementation of adapters used in the citing paper, which serves as a methodological basis for the research conducted in the citing paper.'}]},\n",
              " 'b19': {'title': 'parameterefficient multi-task fine-tuning for transformers via shared hypernetworks',\n",
              "  'arxiv_id': None,\n",
              "  'citation': [{'Category': 'Supporting Evidence',\n",
              "    'Citation': '(He et al., 2021;Li and Liang, 2021;Karimi Mahabadi et al., 2021)',\n",
              "    'Explanation': 'The cited works have revealed that PEFT methods outperform full fine-tuning in low-resource settings, which is a key finding that supports the claims made in the citing paper about the potential benefits of PEFT in this context.'}]},\n",
              " 'b42': {'title': 'an empirical study of example forgetting during deep neural network learning',\n",
              "  'arxiv_id': '1812.05159',\n",
              "  'citation': [{'Category': 'Supporting Evidence',\n",
              "    'Citation': '(Toneva et al., 2019)',\n",
              "    'Explanation': 'The cited work by Toneva et al. (2019) provides a method for analyzing the properties of PEFT and FFT, which the citing paper uses to understand the reason for the improved performance of PEFT in low-resource AL scenarios.'},\n",
              "   {'Category': 'Methodological Basis',\n",
              "    'Citation': '(Toneva et al., 2019)',\n",
              "    'Explanation': 'The cited work by Toneva et al. (2019) provides a methodology for analyzing forgetting dynamics in training examples, which the citing paper adopts to study the occurrence of forgetting events in adapters and their impact on AL data selection.'}]},\n",
              " 'b7': {'title': 'active learning for bert: an empirical study',\n",
              "  'arxiv_id': None,\n",
              "  'citation': [{'Category': 'Methodological Basis',\n",
              "    'Citation': '(Ein-Dor et al., 2020)',\n",
              "    'Explanation': 'The cited work by Ein-Dor et al. (2020) provides a conventional approach for integrating PLMs with AL, which the citing paper adopts in their research to investigate the use of PEFT techniques in low-resource settings.'}]},\n",
              " 'b29': {'title': 'active learning by acquiring contrastive examples',\n",
              "  'arxiv_id': '2109.03764',\n",
              "  'citation': [{'Category': 'Methodological Basis',\n",
              "    'Citation': '(Margatina et al., 2021)',\n",
              "    'Explanation': 'The cited work by Margatina et al. (2021) also contributes to the research on combining PLMs with AL, providing a method for fine-tuning the model in each AL step.'},\n",
              "   {'Category': 'Extension or Continuation',\n",
              "    'Citation': '(Margatina et al., 2022)',\n",
              "    'Explanation': 'The cited work by Margatina et al. (2022) extends the research on the effectiveness of TAPT in enhancing AL performance by providing further insights and data.'}]},\n",
              " 'b38': {'title': 'active learning for sequence tagging with deep pre-trained models and bayesian uncertainty estimates',\n",
              "  'arxiv_id': '2101.08133',\n",
              "  'citation': [{'Category': 'Methodological Basis',\n",
              "    'Citation': '(Shelmanov et al., 2021)',\n",
              "    'Explanation': 'The cited work by Shelmanov et al. (2021) further adds to the research on integrating PLMs with AL, by discussing the use of fine-tuning in each AL step.'}]},\n",
              " 'b18': {'title': 'mind your outliers! investigating the negative impact of outliers on active learning for visual question answering',\n",
              "  'arxiv_id': '2107.02331',\n",
              "  'citation': [{'Category': 'Methodological Basis',\n",
              "    'Citation': '(Karamcheti et al., 2021)',\n",
              "    'Explanation': 'The cited work by Karamcheti et al. (2021) also contributes to the research on combining PLMs with AL, by exploring the use of fine-tuning in each AL step.'}]},\n",
              " 'b35': {'title': 'revisiting uncertainty-based query strategies for active learning with transformers',\n",
              "  'arxiv_id': '2107.05687',\n",
              "  'citation': [{'Category': 'Methodological Basis',\n",
              "    'Citation': '(Schröder et al., 2022)',\n",
              "    'Explanation': 'The cited work by Schröder et al. (2022) further adds to the research on integrating PLMs with AL, by discussing the use of fine-tuning in each AL step.'},\n",
              "   {'Category': 'Methodological Basis',\n",
              "    'Citation': '(Schröder et al., 2022)',\n",
              "    'Explanation': 'The cited work provides a recommendation for using AUC as a suitable approximation of AL feasibility, which the citing paper adopts in their research to evaluate the performance of AL methods.'}]},\n",
              " 'b30': {'title': 'on the stability of fine-tuning bert: misconceptions, explanations, and strong baselines',\n",
              "  'arxiv_id': '2006.04884',\n",
              "  'citation': [{'Category': 'Extension or Continuation',\n",
              "    'Citation': '(Mosbach et al., 2021)',\n",
              "    'Explanation': 'The cited work by Mosbach et al. (2021) extends the research on fine-tuning in low-resource settings, by discussing the instability of the process and its impact on AL.'}]},\n",
              " 'b46': {'title': 'character-level convolutional networks for text classification',\n",
              "  'arxiv_id': '1509.01626',\n",
              "  'citation': [{'Category': 'Extension or Continuation',\n",
              "    'Citation': '(Zhang et al., 2021)',\n",
              "    'Explanation': 'The cited work by Zhang et al. (2021) also extends the research on fine-tuning in low-resource settings, by discussing the instability of the process and its impact on AL.'},\n",
              "   {'Category': 'Data Source',\n",
              "    'Citation': '(Zhang et al., 2015)',\n",
              "    'Explanation': 'The cited work by Zhang et al. is the data source for the AGN dataset used in the single-text classification task in the citing paper.'}]},\n",
              " 'b6': {'title': 'fine-tuning pretrained language models: weight initializations, data orders, and early stopping',\n",
              "  'arxiv_id': '2002.06305',\n",
              "  'citation': [{'Category': 'Data Source',\n",
              "    'Citation': '(Dodge et al., 2020)',\n",
              "    'Explanation': 'The cited work by Dodge et al. (2020) provides a data source for the research on fine-tuning in low-resource settings, by discussing the sensitivity of the process to weight initialization and data ordering.'}]},\n",
              " 'b10': {'title': 'fine-tuning bert for low-resource natural language understanding via active learning',\n",
              "  'arxiv_id': '2012.02462',\n",
              "  'citation': [{'Category': 'Supporting Evidence',\n",
              "    'Citation': '(Grießhaber et al., 2020)',\n",
              "    'Explanation': 'The cited work by Grießhaber et al. (2020) provides evidence that the choice of training regime is more critical than the choice of the AL method in improving AL performance.'}]},\n",
              " 'b44': {'title': 'cold-start active learning through selfsupervised language modeling',\n",
              "  'arxiv_id': None,\n",
              "  'citation': [{'Category': 'Supporting Evidence',\n",
              "    'Citation': '(Yuan et al., 2020)',\n",
              "    'Explanation': 'The cited work by Yuan et al. (2020) further supports the claim that the training regime is more important than the AL method in enhancing AL performance.'},\n",
              "   {'Category': 'Supporting Evidence',\n",
              "    'Citation': '(Yu et al., 2022)',\n",
              "    'Explanation': 'The cited work by Yu et al. (2022) provides additional evidence that the training regime is a critical factor in improving AL performance.'},\n",
              "   {'Category': 'Supporting Evidence',\n",
              "    'Citation': '(He et al., 2021)',\n",
              "    'Explanation': 'The cited work by He et al. (2021) provides evidence on the stability and generalization capabilities of adapter-based tuning in monolingual settings with scarce data.'},\n",
              "   {'Category': 'Methodological Basis',\n",
              "    'Citation': '(He et al., 2021)',\n",
              "    'Explanation': 'The cited work by He et al. further builds upon the research on the use of adapters in low-resource settings in the citing paper.'},\n",
              "   {'Category': 'Extension or Continuation',\n",
              "    'Citation': '(He et al., 2021)',\n",
              "    'Explanation': \"The citing paper continues the research on the use of adapters in low-resource settings by looking into how the models' performance changes as the training set increases.\"},\n",
              "   {'Category': 'Methodological Basis',\n",
              "    'Citation': '(He et al., 2021)',\n",
              "    'Explanation': 'The cited work by He et al. (2021) provides the inspiration for the layerwise examination of similarity in the citing paper, which is used to analyze the effect of PEFT and FFT on AL selection with respect to their layerwise similarity to the base model.'}]},\n",
              " 'b17': {'title': 'smooth sailing: improving active learning for pre-trained language models with representation smoothness analysis',\n",
              "  'arxiv_id': '2212.11680',\n",
              "  'citation': [{'Category': 'Extension or Continuation',\n",
              "    'Citation': '(Jukić and Šnajder, 2023)',\n",
              "    'Explanation': 'The cited work by Jukić and Šnajder (2023) continues the research on TAPT by exploring new dimensions and variables in enhancing AL performance.'},\n",
              "   {'Category': 'Methodological Basis',\n",
              "    'Citation': '(Jukić and Šnajder, 2023)',\n",
              "    'Explanation': 'The cited work also recommends using AUC as a summary numeric score in AL, which the citing paper adopts in their research to evaluate the performance of AL methods.'}]},\n",
              " 'b0': {'title': 'mad-g: multilingual adapter generation for efficient cross-lingual transfer',\n",
              "  'arxiv_id': None,\n",
              "  'citation': [{'Category': 'Supporting Evidence',\n",
              "    'Citation': '(Ansell et al., 2021)',\n",
              "    'Explanation': 'The cited work by Ansell et al. (2021) provides evidence on the effectiveness of cross-lingual transfer for low-resource languages in the context of adapters.'}]},\n",
              " 'b23': {'title': 'fad-x: fusing adapters for cross-lingual transfer to low-resource languages',\n",
              "  'arxiv_id': None,\n",
              "  'citation': [{'Category': 'Supporting Evidence',\n",
              "    'Citation': '(Lee et al., 2022)',\n",
              "    'Explanation': 'The cited work by Lee et al. (2022) further supports the research on the use of adapters in low-resource settings for cross-lingual transfer.'}]},\n",
              " 'b32': {'title': 'bad-x: bilingual adapters improve zero-shot cross-lingual transfer',\n",
              "  'arxiv_id': None,\n",
              "  'citation': [{'Category': 'Supporting Evidence',\n",
              "    'Citation': '(Parović et al., 2022)',\n",
              "    'Explanation': 'The cited work by Parović et al. (2022) provides additional insights on the use of adapters in low-resource settings for cross-lingual transfer.'}]},\n",
              " 'b26': {'title': 'learning question classifiers',\n",
              "  'arxiv_id': None,\n",
              "  'citation': [{'Category': 'Supporting Evidence',\n",
              "    'Citation': '(Li and Liang, 2021)',\n",
              "    'Explanation': 'The cited work by Li and Liang (2021) supports the research on the use of adapters in monolingual settings with scarce data.'},\n",
              "   {'Category': 'Data Source',\n",
              "    'Citation': '(Li and Roth, 2002)',\n",
              "    'Explanation': 'The cited work by Li and Roth is the data source for the TREC dataset used in the single-text classification task in the citing paper.'},\n",
              "   {'Category': 'Methodological Basis',\n",
              "    'Citation': '(Li and Liang, 2021)',\n",
              "    'Explanation': 'The cited work presents the Prefix-tuning PEFT technique, which the citing paper incorporates in the development of the UniPELT method by adding new parameters in the multi-head attention blocks of Transformer layers.'},\n",
              "   {'Category': 'Methodological Basis',\n",
              "    'Citation': '(Li and Liang, 2021)',\n",
              "    'Explanation': 'The cited work by Li and Liang provides the basis for the use of adapters in low-resource settings in the citing paper.'},\n",
              "   {'Category': 'Extension or Continuation',\n",
              "    'Citation': '(Li and Liang, 2021)',\n",
              "    'Explanation': 'The citing paper extends the research on the use of adapters in low-resource settings by conducting a more nuanced analysis and comparing multiple adapter variants with FFT under the passive learning setup.'},\n",
              "   {'Category': 'Methodological Basis',\n",
              "    'Citation': '(Li and Liang, 2021)',\n",
              "    'Explanation': 'The cited work by Li and Liang (2021) is used to bolster the findings of the citing paper by exploring the stability of representations in scenarios with limited resources.'}]},\n",
              " 'b27': {'title': 'unipelt: a unified framework for parameter-efficient language model tuning',\n",
              "  'arxiv_id': '2110.07577',\n",
              "  'citation': [{'Category': 'Supporting Evidence',\n",
              "    'Citation': '(Mao et al., 2022)',\n",
              "    'Explanation': 'The cited work by Mao et al. (2022) further supports the research on the use of adapters in monolingual settings with scarce data.'},\n",
              "   {'Category': 'Methodological Basis',\n",
              "    'Citation': '(Mao et al., 2022)',\n",
              "    'Explanation': 'The cited work presents the UniPELT PEFT method, which the citing paper considers as a combination of multiple PEFT approaches, including LoRA, Prefix-tuning, and Adapter, in a single unified setup with gating mechanisms for effective activation.'},\n",
              "   {'Category': 'Methodological Basis',\n",
              "    'Citation': '(Mao et al., 2022)',\n",
              "    'Explanation': 'The cited work by Mao et al. contributes to the understanding of the use of adapters in low-resource settings in the citing paper.'},\n",
              "   {'Category': 'Extension or Continuation',\n",
              "    'Citation': '(Mao et al., 2022)',\n",
              "    'Explanation': 'The citing paper further extends the research on the use of adapters in low-resource settings by generating detailed learning curves to facilitate the comparison of multiple adapters with FFT in the passive learning setup.'},\n",
              "   {'Category': 'Methodological Basis',\n",
              "    'Citation': '(Mao et al., 2022)',\n",
              "    'Explanation': 'The cited work by Mao et al. (2022) contributes to the analysis of the stability of representations in the citing paper, providing insights into the use of adapters in scenarios with limited resources.'}]},\n",
              " 'b20': {'title': 'revisiting pretraining with adapters',\n",
              "  'arxiv_id': None,\n",
              "  'citation': [{'Category': 'Supporting Evidence',\n",
              "    'Citation': '(Kim et al., 2021)',\n",
              "    'Explanation': 'The cited work by Kim et al. (2021) provides evidence that the benefits of integrating TAPT with adapters tend to taper off as the amount of data increases, which is relevant to the discussion in the citing paper about the limitations of using adapters in low-resource setups.'}]},\n",
              " 'b31': {'title': 'a sentimental education: sentiment analysis using subjectivity summarization based on minimum cuts',\n",
              "  'arxiv_id': 'cs/0409058',\n",
              "  'citation': [{'Category': 'Data Source',\n",
              "    'Citation': '(Pang and Lee, 2004)',\n",
              "    'Explanation': 'The cited work by Pang and Lee serves as the data source for the SUBJ dataset used in the citing paper for the single-text classification task.'}]},\n",
              " 'b39': {'title': 'parsing with compositional vector grammars',\n",
              "  'arxiv_id': None,\n",
              "  'citation': [{'Category': 'Data Source',\n",
              "    'Citation': '(Socher et al., 2013)',\n",
              "    'Explanation': 'The cited work by Socher et al. is the data source for the SST dataset used in the single-text classification task in the citing paper.'}]},\n",
              " None: {'title': 'parsing with compositional vector grammars',\n",
              "  'arxiv_id': None,\n",
              "  'citation': [{'Category': 'Methodological Basis',\n",
              "    'Citation': '(Hu et al., 2022)',\n",
              "    'Explanation': 'The cited work introduces the LoRA PEFT technique, which the citing paper incorporates in the development of the UniPELT method by representing an additive method that incorporates trainable low-rank decomposition matrices in the layers of a pre-trained model.'},\n",
              "   {'Category': 'Methodological Basis',\n",
              "    'Citation': '(Lewis and Gale, 1994)',\n",
              "    'Explanation': 'The cited work by Lewis and Gale (1994) provides the maximum entropy (ENT) strategy for sampling instances in the field of uncertainty strategies, which the citing paper adopts as a method for instance selection.'}]},\n",
              " 'b5': {'title': 'bert: pre-training of deep bidirectional transformers for language understanding',\n",
              "  'arxiv_id': '1810.04805',\n",
              "  'citation': [{'Category': 'Methodological Basis',\n",
              "    'Citation': '(Devlin et al., 2019)',\n",
              "    'Explanation': 'The cited work by Devlin et al. (2019) provides the base PLM (BERT) that the citing paper uses as the foundation for their research on adapters.'}]},\n",
              " 'b8': {'title': 'dropout as a bayesian approximation: representing model uncertainty in deep learning',\n",
              "  'arxiv_id': '1506.02142',\n",
              "  'citation': [{'Category': 'Methodological Basis',\n",
              "    'Citation': '(Gal and Ghahramani, 2016)',\n",
              "    'Explanation': 'The cited work by Gal and Ghahramani (2016) introduces the Monte Carlo dropout (MC) method for instance selection based on the stochasticity of forward passes with dropout layers, which the citing paper utilizes in the field of uncertainty strategies.'}]},\n",
              " 'b40': {'title': 'dropout: a simple way to prevent neural networks from overfitting',\n",
              "  'arxiv_id': None,\n",
              "  'citation': [{'Category': 'Methodological Basis',\n",
              "    'Citation': '(Srivastava et al., 2014)',\n",
              "    'Explanation': 'The cited work by Srivastava et al. (2014) presents the use of dropout layers in forward passes, which the citing paper references in the context of the Monte Carlo dropout (MC) method for instance selection in the field of uncertainty strategies.'}]},\n",
              " 'b36': {'title': 'active learning for convolutional neural networks: a core-set approach',\n",
              "  'arxiv_id': '1708.00489',\n",
              "  'citation': [{'Category': 'Methodological Basis',\n",
              "    'Citation': '(Sener and Savarese, 2018)',\n",
              "    'Explanation': 'The cited work by Sener and Savarese (2018) introduces the core-set (CS) method for instance selection in the field of learning representations of the acquisition model, which the citing paper adopts as a method for encouraging instance diversity.'}]},\n",
              " 'b41': {'title': 'on the geometry of generalization and memorization in deep neural networks',\n",
              "  'arxiv_id': '2105.14602',\n",
              "  'citation': [{'Category': 'Data Source',\n",
              "    'Citation': '(Stephenson et al., 2021)',\n",
              "    'Explanation': 'The data source cited by Stephenson et al. (2021) is used to draw inspiration for the layerwise examination of similarity in the citing paper, which is conducted to analyze the effect of PEFT and FFT on AL selection with respect to their layerwise similarity to the base model.'}]},\n",
              " 'b1': {'title': 'deep learning through the lens of example difficulty',\n",
              "  'arxiv_id': '2106.09647',\n",
              "  'citation': [{'Category': 'Data Source',\n",
              "    'Citation': '(Baldock et al., 2021)',\n",
              "    'Explanation': 'The data source cited by Baldock et al. (2021) is used in the citing paper to support the claim that different layers of networks specialize in different features, with earlier layers acquiring more generalized knowledge and deeper layers focusing on task-specific information.'}]}}"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "annotated_article[0]['mapped_citation']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSV3dfByqPVt",
        "outputId": "9178d76d-d00f-4fd5-96b5-e941d9bffc46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Causal Laws and Multi-Valued Fluents\n"
          ]
        }
      ],
      "source": [
        "triplets = []\n",
        "error_count = 0\n",
        "\n",
        "for article_dict in annotated_article:\n",
        "    if \"mapped_citation\" not in article_dict.keys():\n",
        "        print(article_dict['title'])\n",
        "        continue\n",
        "    for key, val in article_dict['mapped_citation'].items():\n",
        "        title = val['title']\n",
        "        citation = val['citation']\n",
        "\n",
        "        # Use a dictionary to group explanations by category\n",
        "        category_explanations = {}\n",
        "        for rel in citation:\n",
        "            # try:\n",
        "            if 'Category' in rel.keys() and 'Explanation' in rel.keys():\n",
        "                category = rel['Category']\n",
        "                explanation = rel['Explanation']\n",
        "                if category not in category_explanations:\n",
        "                    category_explanations[category] = []\n",
        "                category_explanations[category].append(explanation)\n",
        "            else:\n",
        "                error_count += 1\n",
        "\n",
        "        source_node = paper_dict[article_dict['title'].lower()]\n",
        "        target_node = paper_dict[title]\n",
        "\n",
        "        # Construct triplets with aggregated explanations for each category\n",
        "        if len(category_explanations.items()) > 0:\n",
        "            for category, explanations in category_explanations.items():\n",
        "                if category not in relationships_dict.keys():\n",
        "                    relationships_dict[category] = f\"Is {category} Of\"\n",
        "\n",
        "                aggregated_explanation = \"; \".join(set(explanations))  # Remove duplicates and join explanations\n",
        "                rel = PaperEdge(category=category, explanation=aggregated_explanation)\n",
        "                reverse_rel = PaperEdge(category=relationships_dict[category], explanation=aggregated_explanation)\n",
        "\n",
        "                # Add the relationship in both directions\n",
        "                triplets.append((source_node, rel, target_node))\n",
        "                triplets.append((target_node, reverse_rel, source_node))\n",
        "        else:\n",
        "            rel = PaperEdge(category=\"Unk\", explanation=\"Unk\")\n",
        "            triplets.append((source_node, rel, target_node))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vl98ttqqPVu",
        "outputId": "6ce4e650-5b91-482e-d11f-79e829b6231c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "149"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "error_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aRirTHzqPVu",
        "outputId": "d7ea4dc5-e6f0-476a-aed1-5ff8d6ec4ce1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "586957"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(triplets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAAmR5S9qPVu"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "\n",
        "# Assuming 'triplets' is your list of relationships,\n",
        "# and each PaperNode object in the triplets has an 'arxiv_id' attribute\n",
        "\n",
        "G = nx.DiGraph()\n",
        "\n",
        "# Add nodes and edges\n",
        "for source_node, relationship, target_node in triplets:\n",
        "    # Add nodes if they are not already in the graph\n",
        "    if source_node.arxiv_id not in G:\n",
        "        G.add_node(source_node.title, title=str(source_node), arxiv_id=source_node.arxiv_id)\n",
        "    if target_node.arxiv_id not in G:\n",
        "        G.add_node(target_node.title, title=str(target_node), arxiv_id=target_node.arxiv_id)\n",
        "\n",
        "    # Add edge with relationship details\n",
        "    G.add_edge(source_node.title, target_node.title, title=str(relationship), category=relationship.category, explanation=relationship.explanation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tA5DdSCqPVu",
        "outputId": "d15ad4f8-9a86-4112-ace3-98ff0947e6d7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "112610"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(G.nodes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbadQvSQqPVu"
      },
      "source": [
        "### **2.3.2 Visualizing citation graph**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGOzZmQ_qPVu"
      },
      "outputs": [],
      "source": [
        "def find_connected_nodes(graph, node, relationship=None):\n",
        "    \"\"\"\n",
        "    Find nodes connected to the given node with an optional filter on the type of relationship.\n",
        "    \"\"\"\n",
        "    connected_nodes = []\n",
        "    for n, nbrs in graph.adj.items():\n",
        "        if n == node:\n",
        "            for nbr, eattr in nbrs.items():\n",
        "                if relationship is None or eattr['label'] == relationship:\n",
        "                    connected_nodes.append(nbr)\n",
        "    return connected_nodes\n",
        "\n",
        "# Function to search for a node by arxiv_id and return its details\n",
        "def find_nodes_by_arxiv_id(graph, arxiv_id):\n",
        "    for node, data in graph.nodes(data=True):\n",
        "        if data.get('arxiv_id') == arxiv_id:\n",
        "            return data  # or return data['paper_node'] to return the PaperNode object itself\n",
        "    return \"Paper not found in the graph.\"\n",
        "\n",
        "\n",
        "def find_shortest_path(graph, source, target):\n",
        "    \"\"\"\n",
        "    Find the shortest path between two nodes.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        path = nx.shortest_path(graph, source=source, target=target)\n",
        "        return path\n",
        "    except nx.NetworkXNoPath:\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wegMMpmbqPVu",
        "outputId": "517f0fb2-f61d-4123-86c8-ba17efa4cc05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Topics related to cogview paper:\n",
            " ['T2TD: Text-3D Generation Model based on Prior Knowledge Guidance', 'Prompt-Free Diffusion: Taking \"Text\" out of Text-to-Image Diffusion Models', 'RAPHAEL: Text-to-Image Generation via Large Mixture of Diffusion Paths', 'Cones 2: Customizable Image Synthesis with Multiple Subjects', 'JourneyDB: A Benchmark for Generative Image Understanding', 'Planting a SEED of Vision in Large Language Model', 'Enhancing Visually-Rich Document Understanding via Layout Structure Modeling', 'Likelihood-Based Text-to-Image Evaluation with Patch-Level Perceptual and Semantic Credit Assignment', 'DiffCloth: Diffusion Based Garment Synthesis and Manipulation via Structural Cross-modal Semantic Alignment', 'AI-Generated Content (AIGC) for Various Data Modalities: A Survey', 'Exploring Sparse MoE in GANs for Text-conditioned Image Synthesis', 'Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation', 'Looking at words and points with attention: a benchmark for text-to-shape coherence', 'OpenBA: An Open-Sourced 15B Bilingual Asymmetric Seq2Seq Model Pre-trained from Scratch', 'TextCLIP: Text-Guided Face Image Generation And Manipulation Without Adversarial Training', 'Towards Accurate Image Coding: Improved Autoregressive Image Generation with Dynamic Vector Quantization']\n"
          ]
        }
      ],
      "source": [
        "# Example Usage\n",
        "phenaki_related_topics = find_connected_nodes(G, 'cogview: mastering text-to-image generation via transformers')\n",
        "print(\"Topics related to cogview paper:\\n\", phenaki_related_topics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbqb55wfqPVv",
        "outputId": "7711e6d5-e39d-48e5-90bb-d4a752f5cd8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'title': 'Title: make-a-video: text-to-video generation without text-video data,\\n Arxiv ID: 2209.14792', 'arxiv_id': '2209.14792'}\n"
          ]
        }
      ],
      "source": [
        "# Example search\n",
        "search_result = find_nodes_by_arxiv_id(G, \"2209.14792\")\n",
        "print(search_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDORz0pfqPVv",
        "outputId": "75e2ecfc-e14c-45dd-d6a7-13b1a9470250"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['make-a-video: text-to-video generation without text-video data']\n"
          ]
        }
      ],
      "source": [
        "def find_nodes_by_keyword(graph, keyword):\n",
        "    \"\"\"\n",
        "    Find nodes that contain the given keyword in their name and retrieve their connected nodes and relationships.\n",
        "    \"\"\"\n",
        "    keyword = keyword.lower()  # Convert keyword to lowercase for case-insensitive matching\n",
        "    matching_nodes = [node for node in graph.nodes if keyword in node.lower()]\n",
        "\n",
        "    # related_nodes = {}\n",
        "    # for node in matching_nodes:\n",
        "    #     connections = []\n",
        "    #     for neighbor, details in graph[node].items():\n",
        "    #         connections.append((neighbor, details['title'].split('\\n')[0]))\n",
        "    #     related_nodes[node] = connections\n",
        "\n",
        "    return matching_nodes\n",
        "\n",
        "# Example Usage\n",
        "keyword = \"make-a-video\"\n",
        "related_nodes = find_nodes_by_keyword(G, keyword)\n",
        "# for node, connections in phenaki_related.items():\n",
        "#     print(f\"Node: {node}\")\n",
        "#     for conn in connections:\n",
        "#         print(f\"  Connected to: {conn[0]} via {conn[1]}\")\n",
        "print(related_nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Je5mUW41qPVv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Q3RW2zGqPVv",
        "outputId": "2692d705-e67e-4979-ab96-fa9eac674863"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Local cdn resources have problems on chrome/safari when used in jupyter-notebook. \n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"100%\"\n",
              "            height=\"600px\"\n",
              "            src=\"nx.html\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "            \n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7fb288077d30>"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import networkx as nx\n",
        "from pyvis.network import Network\n",
        "\n",
        "# Assuming G is your original graph\n",
        "# Step 1: Create the subgraph for \"Node1\" and its neighbors\n",
        "subgraph = nx.ego_graph(G, 'cogview: mastering text-to-image generation via transformers', radius=1, center=True, undirected=True)\n",
        "# Nodes to be removed because they have a degree of 1 in the full graph\n",
        "# nodes_to_remove = [node for node in subgraph if subgraph.degree(node) < 3]\n",
        "\n",
        "# Remove the nodes from the ego graph\n",
        "# subgraph.remove_nodes_from(nodes_to_remove)\n",
        "\n",
        "nt = Network(notebook=True, font_color='#10000000')\n",
        "nt.from_nx(subgraph)\n",
        "nt.show(\"nx.html\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDYseJrEqPVv"
      },
      "source": [
        "From this graph, we can see that the paper 'CogView: Mastering Text-to-Image Generation via Transformers' is referenced in several papers such as 'TextCLIP', with an explanation like, 'The cited work, CogView, is similar to DALL-E in training a transformer with 4 billion network parameters...' Additionally, the paper 'T2TD: Text-3D Generation Models' describes it as an 'approach that is not based on GAN structure but achieves favorable generation performance.' CogView also serves as the theoretical foundation for papers like 'Toward Accurate Image Encoding' and 'DiffCloth.'\n",
        "\n",
        "Based on these relationships, we can discern a lot about the paper CogView, which is invaluable for those seeking to understand a paper more deeply.\n",
        "\n",
        "A really cool use case of this graph is using it with a paper retriever, we retrieve a list of paper about a topic and see how they are related to each other"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFMMQXUeqPVv"
      },
      "outputs": [],
      "source": [
        "paper_retriever = paper_index.as_retriever(\n",
        "    similarity_top_k=15,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BQv_eJ8qPVv"
      },
      "outputs": [],
      "source": [
        "results = paper_retriever.retrieve(\"Give me some paper about Video diffusion models\")\n",
        "# print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2AZt6B5VqPVv",
        "outputId": "b56ee133-cf5a-4841-f94e-3939bb15967d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a survey on video diffusion models\n",
            "frame by familiar frame: understanding replication in video diffusion models\n",
            "efficient video diffusion models via content-frame motion-latent decomposition\n",
            "stable video diffusion: scaling latent video diffusion models to large datasets\n",
            "preserve your own correlation: a noise prior for video diffusion models\n",
            "diffusion models: a comprehensive survey of methods and applications\n",
            "diffusion probabilistic modeling for video generation\n",
            "diffusion models for time series applications: a survey\n",
            "video probabilistic diffusion models in projected latent space\n",
            "video diffusion models\n",
            "medm: mediating image diffusion models for video-to-video translation with temporal correspondence guidance\n",
            "dreamix: video diffusion models are general video editors\n",
            "diffusion models for video prediction and infilling\n",
            "sinfusion: training diffusion models on a single image or video\n",
            "state of the art on diffusion models for visual computing\n"
          ]
        }
      ],
      "source": [
        "all_nodes = []\n",
        "for r in results:\n",
        "    title = r.text.split(\"\\n\")[0]\n",
        "    print(title)\n",
        "    nodes = find_nodes_by_keyword(G, title)\n",
        "    if len(nodes) > 0:\n",
        "        all_nodes += nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vH56piuIqPVw",
        "outputId": "9eec6f5b-c552-4bfb-89d0-d740a2077111"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['diffusion models: a comprehensive survey of methods and applications',\n",
              " 'diffusion probabilistic modeling for video generation',\n",
              " 'video probabilistic diffusion models in projected latent space',\n",
              " 'fleet. video diffusion models',\n",
              " 'latent video diffusion models for high-fidelity video generation with arbitrary lengths',\n",
              " 'video diffusion models',\n",
              " 'dreamix: video diffusion models are general video editors',\n",
              " 'latent video diffusion models for high-fidelity long video generation',\n",
              " 'Video Diffusion Models with Local-Global Context Guidance',\n",
              " 'llm-grounded video diffusion models',\n",
              " 'MeDM: Mediating Image Diffusion Models for Video-to-Video Translation with Temporal Correspondence Guidance',\n",
              " 'dreamix: video diffusion models are general video editors',\n",
              " 'diffusion models for video prediction and infilling',\n",
              " 'sinfusion: training diffusion models on a single image or video',\n",
              " 'state of the art on diffusion models for visual computing']"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qq1pTQZrqPVw"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "# Generate ego graphs\n",
        "ego_graphs = [nx.ego_graph(G, node, radius=1) for node in all_nodes]\n",
        "\n",
        "# Combine all ego graphs into one graph\n",
        "combined_ego_graph = nx.compose_all(ego_graphs)\n",
        "nodes_to_remove = [node for node in combined_ego_graph if combined_ego_graph.degree(node) < 2]\n",
        "\n",
        "# Remove the nodes from the ego graph\n",
        "combined_ego_graph.remove_nodes_from(nodes_to_remove)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDY3UUEWqPVw",
        "outputId": "42c5f84f-eb34-4691-fb97-f1bf952826dd"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Draw the combined ego graph\n",
        "plt.figure(figsize=(8, 6))\n",
        "# Define positions using a layout\n",
        "pos = nx.spring_layout(combined_ego_graph)\n",
        "\n",
        "# Draw nodes\n",
        "nx.draw_networkx_nodes(combined_ego_graph, pos, node_color='orange')\n",
        "\n",
        "# Draw edges\n",
        "nx.draw_networkx_edges(combined_ego_graph, pos)\n",
        "\n",
        "# Draw node labels with transparency\n",
        "labels = {node: node for node in combined_ego_graph.nodes()}\n",
        "alpha_value = 0.0  # Set transparency for labels\n",
        "nx.draw_networkx_labels(combined_ego_graph, pos, labels, font_size=12, font_color='black', alpha=alpha_value)\n",
        "\n",
        "# Optionally add a title with transparency\n",
        "plt.title(\"Video diffusion graph\", alpha=0.5)  # Set transparency for title text\n",
        "\n",
        "plt.axis('off')  # Hide axes\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-PwqMb7KqPVw",
        "outputId": "f570fc5a-1825-4eb5-f64a-5dd2ca228d79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Local cdn resources have problems on chrome/safari when used in jupyter-notebook. \n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"100%\"\n",
              "            height=\"600px\"\n",
              "            src=\"nx.html\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "            \n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7fb2881fe340>"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Assign colors: highlighted nodes in red, others in blue\n",
        "highlight_color = \"orange\"\n",
        "for node in combined_ego_graph.nodes():\n",
        "    if node in all_nodes:\n",
        "        combined_ego_graph.nodes[node]['color'] = highlight_color\n",
        "\n",
        "nt = Network(notebook=True, font_color='#10000000')\n",
        "nt.from_nx(combined_ego_graph)\n",
        "nt.show(\"nx.html\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOMaqN9TqPVw"
      },
      "source": [
        "Even with a radius set to 1, which essentially creates a graph with a depth of 1, nearly all the nodes are interconnected. This tool is incredibly effective for vividly visualizing the relationships between various research papers within the same field! You can play around with different parameters to create comprehensive graph!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Tz8ivDRqPVw"
      },
      "source": [
        "<div style=\"color: #D5896F; font-size:120%\">\n",
        "Based on this graph, we can see that the paper 'MeDM: Mediating Image Diffusion Models for Video-to-Video Translation with Temporal Correspondence Guidance' is extensively interconnected. It utilizes data sources from the papers 'CelebV-HQ' and 'A Benchmark Dataset and Evaluation Methodology for Video Object Segmentation.' This paper employs the point tracking method presented in 'TAPIR: Tracking Any Point with Per-Frame...,' which is based on 'Video Diffusion Models.' These two papers share another common link through the paper 'Make-Your-Video,' which itself is based on 'Video Diffusion Models'—the same foundational paper used in 'MeDM.'\n",
        "\n",
        "Additionally, 'Video Diffusion Models' serves as the methodological basis for numerous papers in video generation and video editing, such as 'InstructEdit', which cites 'Diffusion Probabilistic Modelling for Video Generation' and 'Dreamix'. Another example is 'StableVideo: Text-driven Consistency-aware Diffusion Video Editing', which adopts the method introduced in 'Video Diffusion Models' and focuses more on appearance editing.\n",
        "\n",
        "There are several fascinating relationships like this to explore further within this graph. From the initial set of 15 papers, we can uncover several significant works on the topic and trace their connections!\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMH5wdxTqPVx",
        "outputId": "88518517-5fac-483d-9cd4-eda1154a6da8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vision transformers: state of the art and research challenges\n",
            "transformers in vision: a survey\n",
            "a survey on visual transformer\n",
            "a survey of vision transformers in autonomous driving: current trends and future directions\n",
            "transformers meet visual learning understanding: a comprehensive review\n",
            "recent advances in vision transformer: a survey and outlook of recent work\n",
            "what do vision transformers learn? a visual exploration\n",
            "vision transformers for computer go\n",
            "a survey of the vision transformers and its cnn-transformer based variants\n",
            "adventures of trustworthy vision-language models: a survey\n",
            "intriguing properties of vision transformers\n",
            "three things everyone should know about vision transformers\n",
            "flatten transformer: vision transformer using focused linear attention\n",
            "gaze estimation using transformer\n",
            "explainability of vision transformers: a comprehensive review and new perspectives\n"
          ]
        }
      ],
      "source": [
        "results = paper_retriever.retrieve(\"What are some research about transformers in vision tasks?\")\n",
        "all_nodes = []\n",
        "for r in results:\n",
        "    title = r.text.split(\"\\n\")[0]\n",
        "    print(title)\n",
        "    nodes = find_nodes_by_keyword(G, title)\n",
        "    if len(nodes) > 0:\n",
        "        all_nodes += nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSe-bBfmqPVx",
        "outputId": "cde2044d-2156-46b3-81f6-4eac7f68a4bd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['transformers in vision: a survey',\n",
              " 'a survey on visual transformer',\n",
              " ') recent advances in vision transformer: a survey and outlook of recent work islam ma',\n",
              " 'what do vision transformers learn? a visual exploration',\n",
              " 'A survey of the Vision Transformers and its CNN-Transformer based Variants',\n",
              " 'intriguing properties of vision transformers',\n",
              " 'three things everyone should know about vision transformers',\n",
              " 'FLatten Transformer: Vision Transformer using Focused Linear Attention',\n",
              " 'gaze estimation using transformer']"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9lnWPPQqPVy"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "# Generate ego graphs\n",
        "ego_graphs = [nx.ego_graph(G, node, radius=1) for node in all_nodes]\n",
        "\n",
        "# Combine all ego graphs into one graph\n",
        "combined_ego_graph = nx.compose_all(ego_graphs)\n",
        "# nodes_to_remove = [node for node in combined_ego_graph if combined_ego_graph.degree(node) < 2]\n",
        "\n",
        "# Remove the nodes from the ego graph\n",
        "# combined_ego_graph.remove_nodes_from(nodes_to_remove)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07DvzFq7qPVy",
        "outputId": "fafa0e6e-a29c-4a5f-ffeb-40fc3aaec521"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Local cdn resources have problems on chrome/safari when used in jupyter-notebook. \n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"100%\"\n",
              "            height=\"600px\"\n",
              "            src=\"nx.html\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "            \n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7fb2869d7340>"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Assign colors: highlighted nodes in red, others in blue\n",
        "highlight_color = \"orange\"\n",
        "for node in combined_ego_graph.nodes():\n",
        "    if node in all_nodes:\n",
        "        combined_ego_graph.nodes[node]['color'] = highlight_color\n",
        "\n",
        "nt = Network(notebook=True, font_color='#10000000')\n",
        "nt.from_nx(combined_ego_graph)\n",
        "nt.show(\"nx.html\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "CQrK_5-KqPVy",
        "outputId": "c4f113e9-98c5-415f-e2fb-293088f09eb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "effective pre-training objectives for transformer-based autoencoders\n",
            "don't sweep your learning rate under the rug: a closer look at cross-modal transfer of pretrained transformers\n",
            "generative pre-trained transformer for design concept generation: an exploration\n",
            "transformer models: an introduction and catalog\n",
            "learning to grow pretrained models for efficient transformer training\n",
            "model-generated pretraining signals improves zero-shot generalization of text-to-text transformers\n",
            "subformer: exploring weight sharing for parameter efficiency in generative transformers\n",
            "adding recurrence to pretrained transformers for improved efficiency and context size\n",
            "an introduction to transformers\n",
            "efficient pre-training objectives for transformers\n",
            "pretrained transformers as universal computation engines\n",
            "huggingface's transformers: state-of-the-art natural language processing\n",
            "opt: open pre-trained transformer language models\n",
            "are pre-trained convolutions better than pre-trained transformers?\n",
            "birth of a transformer: a memory viewpoint\n",
            "Local cdn resources have problems on chrome/safari when used in jupyter-notebook. \n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"100%\"\n",
              "            height=\"600px\"\n",
              "            src=\"nx.html\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "            \n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7fb2880a1c10>"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results = paper_retriever.retrieve(\"Give me some papers about generative pretrained transformers\")\n",
        "all_nodes = []\n",
        "for r in results:\n",
        "    title = r.text.split(\"\\n\")[0]\n",
        "    print(title)\n",
        "    nodes = find_nodes_by_keyword(G, title)\n",
        "    if len(nodes) > 0:\n",
        "        all_nodes += nodes\n",
        "\n",
        "# Generate ego graphs\n",
        "ego_graphs = [nx.ego_graph(G, node, radius=1) for node in all_nodes]\n",
        "\n",
        "# Combine all ego graphs into one graph\n",
        "combined_ego_graph = nx.compose_all(ego_graphs)\n",
        "# nodes_to_remove = [node for node in combined_ego_graph if combined_ego_graph.degree(node) < 2]\n",
        "\n",
        "# Remove the nodes from the ego graph\n",
        "# combined_ego_graph.remove_nodes_from(nodes_to_remove)\n",
        "# Assign colors: highlighted nodes in red, others in blue\n",
        "highlight_color = \"orange\"\n",
        "for node in combined_ego_graph.nodes():\n",
        "    if node in all_nodes:\n",
        "        combined_ego_graph.nodes[node]['color'] = highlight_color\n",
        "\n",
        "nt = Network(notebook=True, font_color='#10000000')\n",
        "nt.from_nx(combined_ego_graph)\n",
        "nt.show(\"nx.html\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rsfq5aUBqPVz"
      },
      "outputs": [],
      "source": [
        "# results = paper_retriever.retrieve(\"Give me some papers about multimodal large language model?\")\n",
        "# all_nodes = []\n",
        "# for r in results:\n",
        "#     title = r.text.split(\"\\n\")[0]\n",
        "#     print(title)\n",
        "#     nodes = find_nodes_by_keyword(G, title)\n",
        "#     if len(nodes) > 0:\n",
        "#         all_nodes += nodes\n",
        "\n",
        "# # Generate ego graphs\n",
        "# ego_graphs = [nx.ego_graph(G, node, radius=1) for node in all_nodes]\n",
        "\n",
        "# # Combine all ego graphs into one graph\n",
        "# combined_ego_graph = nx.compose_all(ego_graphs)\n",
        "# # nodes_to_remove = [node for node in combined_ego_graph if combined_ego_graph.degree(node) < 2]\n",
        "\n",
        "# # Remove the nodes from the ego graph\n",
        "# # combined_ego_graph.remove_nodes_from(nodes_to_remove)\n",
        "# # Assign colors: highlighted nodes in red, others in blue\n",
        "# highlight_color = \"orange\"\n",
        "# for node in combined_ego_graph.nodes():\n",
        "#     if node in all_nodes:\n",
        "#         combined_ego_graph.nodes[node]['color'] = highlight_color\n",
        "\n",
        "# nt = Network(notebook=True, font_color='#10000000')\n",
        "# nt.from_nx(combined_ego_graph)\n",
        "# nt.show(\"nx.html\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYAMwTraqPVz"
      },
      "source": [
        "## **2.4 Building Graph Query Engine**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aniJLMgkqPVz"
      },
      "source": [
        "The graph query mechanism is designed to efficiently process user queries by leveraging LLM to retrieve information from a graph database. Here's a detailed breakdown of the process:\n",
        "\n",
        "- Query Parsing: Initially, when a user submits a query, the LLM parses the query to extract key elements, typically in the form of (paper_title, relationship) pairs.\n",
        "\n",
        "- Graph Database Querying: Once the necessary information is extracted, the next step involves querying the graph database. The process begins with locating the node corresponding to the 'paper_title'.\n",
        "\n",
        "- Connection Edge Retrieval: After identifying the relevant paper node, the system then searches for connection edges. It specifically looks for the edge that has the highest similarity score with the extracted 'relationship' text, using a vector search. This step ensures that the most relevant connections based on the user's query are identified.\n",
        "\n",
        "- Result Compilation: The final step involves returning the retrieved information to the LLM. The LLM then uses this data to generate a comprehensive answer tailored to the user's query, providing insights based on the connections and data found in the graph database.\n",
        "\n",
        "For a more visual understanding of this process, refer to the diagram below,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZC2qAVZqPVz"
      },
      "outputs": [],
      "source": [
        "Image(\"/kaggle/input/gemmarag-figures/Graph-Mechanism.jpg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8GuEr-uqPVz"
      },
      "source": [
        "### **2.4.1 Prepare data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFe_suYIqPV0",
        "outputId": "a0518d8c-550f-41e2-ad5e-037ffbcaf93b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "135"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(relationships_dict.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcCDnmT_qPV0"
      },
      "outputs": [],
      "source": [
        "relationships_prompt = {\n",
        "    \"Supporting Evidence\": \"The paper \\\"{source}\\\" provides supporting evidence for the paper \\\"{target}\\\". \\nExplanation: {explanation}\",\n",
        "    \"Is Evidence For\": \"The paper \\\"{source}\\\" is supported by the paper \\\"{target}\\\". \\nExplanation: {explanation}\",\n",
        "    \"Methodological Basis\": \"The paper \\\"{source}\\\" is a methodological basis for the paper \\\"{target}\\\". \\nExplanation: {explanation}\",\n",
        "    \"Is Methodological Basis For\": \"The paper \\\"{source}\\\" is based on the methodology of the paper \\\"{target}\\\". \\nExplanation: {explanation}\",\n",
        "    \"Theoretical Foundation\": \"The paper \\\"{source}\\\" is a theoretical foundation for the paper \\\"{target}\\\". \\nExplanation: {explanation}\",\n",
        "    \"Is Theoretical Foundation For\": \"The paper \\\"{source}\\\" is based on the theoretical foundation of the paper \\\"{target}\\\". \\nExplanation: {explanation}\",\n",
        "    \"Data Source\": \"The paper \\\"{source}\\\" uses the data from the paper \\\"{target}\\\". \\nExplanation: {explanation}\",\n",
        "    \"Is Data Source For\": \"The paper \\\"{source}\\\" provides data for the paper \\\"{target}\\\". \\nExplanation: {explanation}\",\n",
        "    \"Extension or Continuation\": \"The paper \\\"{source}\\\" is an extension or continuation of the paper \\\"{target}\\\". \\nExplanation: {explanation}\",\n",
        "    \"Is Extension or Continuation Of\": \"The paper \\\"{source}\\\" is extended or continued by the paper \\\"{target}\\\". \\nExplanation: {explanation}\",\n",
        "    \"Unk\": \"The relationship bewteen the 2 papers \\\"{source}\\\", \\\"{target}\\\" is unkown {explanation}\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uKjyIRvqPV0"
      },
      "outputs": [],
      "source": [
        "for key, val in relationships_dict.items():\n",
        "    if key not in relationships_prompt.keys():\n",
        "        relationships_prompt[key] = \"The paper \\\"{source}\\\" provide \" + val + \" for paper \\\"{target}\\\". \\nExplanation: {explanation}\"\n",
        "        relationships_prompt[val] = \"The paper \\\"{source}\\\" is \" + val + \" of paper \\\"{target}\\\". \\nExplanation: {explanation}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UauLI5RcqPV0",
        "outputId": "3b437470-4724-4146-bb1c-7917624dd9b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The paper \"{source}\" provide Is Evaluation Protocol Of for paper \"{target}\". \n",
            "Explanation: {explanation}\n"
          ]
        }
      ],
      "source": [
        "print(relationships_prompt['Evaluation Protocol'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOy6ZtaEqPV1"
      },
      "outputs": [],
      "source": [
        "# triplet_nodes = []\n",
        "\n",
        "# for triplet in tqdm(triplets, total=len(triplets)):\n",
        "#     text_prompt = relationships_prompt[triplet[1].category].format(source=triplet[0].title, target=triplet[2].title, explanation=triplet[1].explanation)\n",
        "#     triplet_nodes.append(Document(text=text_prompt, metadata={\"title\": triplet[0].title, \"arxiv_id\": triplet[0].arxiv_id}))\n",
        "\n",
        "# triplet_nodes[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJ93plryqPV1"
      },
      "source": [
        "Now we extract the embedding of the relationships between nodes. Again, this process takes quite long so I have precomputed it. You can run the below code to extract the embedding from scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROypf2OaqPV1"
      },
      "outputs": [],
      "source": [
        "# ## Extract embedding for the truplets relationships\n",
        "# Settings.llm = None\n",
        "# # Create embed model\n",
        "# device_type = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\", cache_folder=\"../models\", device=device_type)\n",
        "\n",
        "# chroma_client = chromadb.PersistentClient(path=\"../DB/graph\")\n",
        "# chroma_collection = chroma_client.get_or_create_collection(\"gemma_assistant_graph\")\n",
        "\n",
        "# vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "# storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "# index = VectorStoreIndex.from_documents(\n",
        "#     triplet_nodes, storage_context=storage_context, embed_model=embed_model, show_progress=True\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oW4UqQSHqPV1"
      },
      "source": [
        "Now, we load the relationship from the vector store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDD55kEmqPV1",
        "outputId": "a4956800-d4c0-4d29-90c0-9f46e0fabe92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLM is explicitly disabled. Using MockLLM.\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core import VectorStoreIndex, Settings\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "import chromadb\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "from llama_index.core import StorageContext\n",
        "import torch\n",
        "\n",
        "\n",
        "Settings.llm = None # Set this to none to make the index only do retrieval\n",
        "# device_type = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\") # must be the same as the previous stage\n",
        "\n",
        "chroma_client = chromadb.PersistentClient(path=\"/home/mahita/fyp/input/graph\")\n",
        "chroma_collection = chroma_client.get_or_create_collection(\"gemma_assistant_graph\")\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "# load the vectorstore\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "rel_index = VectorStoreIndex.from_vector_store(vector_store, storage_context=storage_context, embed_model=embed_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwd_a4tTqPV2"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.vector_stores.types import MetadataFilters, ExactMatchFilter\n",
        "\n",
        "\n",
        "filters = MetadataFilters(filters=[\n",
        "    ExactMatchFilter(\n",
        "        key=\"title\",\n",
        "        value=\"active learning with statistical models\"\n",
        "    )\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCZALAkWqPV2"
      },
      "outputs": [],
      "source": [
        "graph_rel_query_engine = rel_index.as_retriever(\n",
        "    similarity_top_k=10,\n",
        "    filters=filters\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XkksT7xsqPV2"
      },
      "outputs": [],
      "source": [
        "retrieved_res = graph_rel_query_engine.retrieve(\"is based on the methodology of the paper\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fX3F7s1OqPV2"
      },
      "outputs": [],
      "source": [
        "retrieved_res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZ1FftccqPV2"
      },
      "source": [
        "### **2.4.2 Building the search logic**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pW4MNGmRqPV2"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from typing import Any, Dict, List, Optional\n",
        "from dataclasses import dataclass, field\n",
        "from dataclasses_json import DataClassJsonMixin\n",
        "from collections import defaultdict\n",
        "\n",
        "import fsspec\n",
        "from llama_index.core.graph_stores.types import (\n",
        "    DEFAULT_PERSIST_DIR,\n",
        "    DEFAULT_PERSIST_FNAME,\n",
        "    GraphStore,\n",
        ")\n",
        "import ast\n",
        "import fsspec\n",
        "import logging\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "DEFAULT_PERSIST_DIR = \"../DB/citation_graph\"\n",
        "DEFAULT_PERSIST_FNAME = \"graph_store.json\"\n",
        "\n",
        "\n",
        "class CitationGraphStoreData(DataClassJsonMixin):\n",
        "\n",
        "    \"\"\"Simple Graph Store Data container.\n",
        "\n",
        "    Args:\n",
        "        graph_dict (Optional[dict]): dict mapping subject to\n",
        "    \"\"\"\n",
        "\n",
        "    graph_dict: Dict[str, List[List[str]]] = defaultdict(list)\n",
        "    graph_index: VectorStoreIndex = rel_index\n",
        "\n",
        "\n",
        "    def find_nodes_by_keyword(self, keyword):\n",
        "        \"\"\"\n",
        "        Find all nodes that contain the given keyword in their name.\n",
        "        \"\"\"\n",
        "        keyword = keyword.lower()  # Convert keyword to lowercase for case-insensitive matching\n",
        "        return [node for node in self.graph_dict.keys() if keyword in node.title.lower()]\n",
        "\n",
        "\n",
        "    def get_rel_map(\n",
        "        self, subjs: Optional[List[str]] = None, depth: int = 2, limit: int = 30\n",
        "    ) -> Dict[str, List[List[str]]]:\n",
        "        \"\"\"Get subjects' rel map in max depth.\"\"\"\n",
        "        if subjs is None:\n",
        "            subjs = list(self.graph_dict.keys())\n",
        "        rel_map = {}\n",
        "        for subj in subjs:\n",
        "            rel_map[subj] = self._get_rel_map(subj.title, depth=depth, limit=limit)\n",
        "        # TBD, truncate the rel_map in a spread way, now just truncate based\n",
        "        # on iteration order\n",
        "        rel_count = 0\n",
        "        return_map = {}\n",
        "        for subj in rel_map:\n",
        "            if rel_count + len(rel_map[subj]) > limit:\n",
        "                return_map[subj] = rel_map[subj][: limit - rel_count]\n",
        "                break\n",
        "            else:\n",
        "                return_map[subj] = rel_map[subj]\n",
        "                rel_count += len(rel_map[subj])\n",
        "        return return_map\n",
        "\n",
        "    def _get_rel_map(\n",
        "        self, keyword: str, depth: int = 2, limit: int = 30\n",
        "    ) -> List[List[str]]:\n",
        "        \"\"\"Get one subect's rel map in max depth.\"\"\"\n",
        "        if depth == 0:\n",
        "            return []\n",
        "        rel_map = []\n",
        "        rel_count = 0\n",
        "        subjs = self.find_nodes_by_keyword(keyword)\n",
        "\n",
        "        if len(subjs) > 0:\n",
        "            subj = subjs[0]\n",
        "            for rel, obj in self.graph_dict[subj]:\n",
        "                if rel_count >= limit:\n",
        "                    break\n",
        "                rel_map.append([subj, rel, obj])\n",
        "                rel_map += self._get_rel_map(obj, depth=depth - 1)\n",
        "                rel_count += 1\n",
        "        return rel_map\n",
        "\n",
        "    def search_vector(self, queries):\n",
        "        # Example string that represents a tuple\n",
        "        final_res = []\n",
        "        for query_tuple in queries:\n",
        "\n",
        "            # Converting string to tuple\n",
        "            # result_tuple = ast.literal_eval(query_str)\n",
        "            qr_title = query_tuple[0]\n",
        "            qr_rel = query_tuple[1]\n",
        "            nodes = self.find_nodes_by_keyword(qr_title)\n",
        "            if len(nodes) == 0:\n",
        "                return None\n",
        "            node = nodes[0]\n",
        "\n",
        "            filters = MetadataFilters(filters=[\n",
        "                ExactMatchFilter(\n",
        "                    key=\"title\",\n",
        "                    value=node.title\n",
        "                )\n",
        "            ])\n",
        "            retriever = self.graph_index.as_retriever(\n",
        "                similarity_top_k=3,\n",
        "                filters=filters\n",
        "            )\n",
        "            res = retriever.retrieve(qr_rel)\n",
        "            final_res += [r.text for r in res]\n",
        "\n",
        "        return final_res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsDrP6fmqPV2"
      },
      "outputs": [],
      "source": [
        "class CitationGraphStore(GraphStore):\n",
        "    \"\"\"Simple Graph Store.\n",
        "\n",
        "    In this graph store, triplets are stored within a simple, in-memory dictionary.\n",
        "\n",
        "    Args:\n",
        "        simple_graph_store_data_dict (Optional[dict]): data dict\n",
        "            containing the triplets. See SimpleGraphStoreData\n",
        "            for more details.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        data: Optional[CitationGraphStoreData] = None,\n",
        "        fs: Optional[fsspec.AbstractFileSystem] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> None:\n",
        "        \"\"\"Initialize params.\"\"\"\n",
        "        self._data = data or CitationGraphStoreData()\n",
        "        self._fs = fs or fsspec.filesystem(\"file\")\n",
        "\n",
        "    @classmethod\n",
        "    def from_persist_dir(\n",
        "        cls,\n",
        "        persist_dir: str = DEFAULT_PERSIST_DIR,\n",
        "        fs: Optional[fsspec.AbstractFileSystem] = None,\n",
        "    ) -> \"CitationGraphStore\":\n",
        "        \"\"\"Load from persist dir.\"\"\"\n",
        "        persist_path = os.path.join(persist_dir, DEFAULT_PERSIST_FNAME)\n",
        "        return cls.from_persist_path(persist_path, fs=fs)\n",
        "\n",
        "    @property\n",
        "    def client(self) -> None:\n",
        "        \"\"\"Get client.\n",
        "        Not applicable for this store.\n",
        "        \"\"\"\n",
        "        return\n",
        "\n",
        "    def get(self, subj: str) -> List[List[str]]:\n",
        "        \"\"\"Get triplets.\"\"\"\n",
        "        return self._data.graph_dict.get(subj, [])\n",
        "\n",
        "    def get_rel_map(\n",
        "        self, subjs: Optional[List[str]] = None, depth: int = 2, limit: int = 30\n",
        "    ) -> Dict[str, List[List[str]]]:\n",
        "        \"\"\"Get depth-aware rel map.\"\"\"\n",
        "        return self._data.get_rel_map(subjs=subjs, depth=depth, limit=limit)\n",
        "\n",
        "    def upsert_triplet(self, subj: str, rel: str, obj: str) -> None:\n",
        "        \"\"\"Add triplet.\"\"\"\n",
        "        if subj not in self._data.graph_dict:\n",
        "            self._data.graph_dict[subj] = []\n",
        "        if (rel, obj) not in self._data.graph_dict[subj]:\n",
        "            self._data.graph_dict[subj].append([rel, obj])\n",
        "\n",
        "    def delete(self, subj: str, rel: str, obj: str) -> None:\n",
        "        \"\"\"Delete triplet.\"\"\"\n",
        "        if subj in self._data.graph_dict:\n",
        "            if (rel, obj) in self._data.graph_dict[subj]:\n",
        "                self._data.graph_dict[subj].remove([rel, obj])\n",
        "                if len(self._data.graph_dict[subj]) == 0:\n",
        "                    del self._data.graph_dict[subj]\n",
        "\n",
        "    def persist(\n",
        "        self,\n",
        "        persist_path: str = os.path.join(DEFAULT_PERSIST_DIR, DEFAULT_PERSIST_FNAME),\n",
        "        fs: Optional[fsspec.AbstractFileSystem] = None,\n",
        "    ) -> None:\n",
        "        \"\"\"Persist the SimpleGraphStore to a directory.\"\"\"\n",
        "        fs = fs or self._fs\n",
        "        dirpath = os.path.dirname(persist_path)\n",
        "        if not fs.exists(dirpath):\n",
        "            fs.makedirs(dirpath)\n",
        "\n",
        "        with fs.open(persist_path, \"w\") as f:\n",
        "            json.dump(self._data.to_dict(), f)\n",
        "\n",
        "    def get_schema(self, refresh: bool = False) -> str:\n",
        "        \"\"\"Get schema.\"\"\"\n",
        "        return \"CitationGraphStore\"\n",
        "\n",
        "    def query(self, query: str, param_map: Optional[Dict[str, Any]] = {}) -> Any:\n",
        "        response = []\n",
        "\n",
        "        pattern = r'\\(([^)]+)\\)'\n",
        "        tuples = re.findall(pattern, query)\n",
        "\n",
        "        pairs = [tuple(item.split(\",\")) for item in tuples]\n",
        "\n",
        "        # relmap = self._data.get_rel_map(subjs=pairs, depth=param_map.get(\"depth\") or 1, limit=param_map.get(\"limit\") or 30)\n",
        "        res = self._data.search_vector(pairs)\n",
        "\n",
        "        response.append(res)\n",
        "        return response\n",
        "\n",
        "    @classmethod\n",
        "    def from_persist_path(\n",
        "        cls, persist_path: str, fs: Optional[fsspec.AbstractFileSystem] = None\n",
        "    ) -> \"CitationGraphStore\":\n",
        "        \"\"\"Create a SimpleGraphStore from a persist directory.\"\"\"\n",
        "        fs = fs or fsspec.filesystem(\"file\")\n",
        "        if not fs.exists(persist_path):\n",
        "            logger.warning(\n",
        "                f\"No existing {__name__} found at {persist_path}. \"\n",
        "                \"Initializing a new graph_store from scratch. \"\n",
        "            )\n",
        "            return cls()\n",
        "\n",
        "        logger.debug(f\"Loading {__name__} from {persist_path}.\")\n",
        "        with fs.open(persist_path, \"rb\") as f:\n",
        "            data_dict = json.load(f)\n",
        "            data = CitationGraphStoreData.from_dict(data_dict)\n",
        "        return cls(data)\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls, save_dict: dict) -> \"CitationGraphStore\":\n",
        "        data = CitationGraphStoreData.from_dict(save_dict)\n",
        "        return cls(data)\n",
        "\n",
        "    def to_dict(self) -> dict:\n",
        "        return self._data.to_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVoavLlWqPV3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWB_5VW1qPV3"
      },
      "outputs": [],
      "source": [
        "citation_graph_store = CitationGraphStore()\n",
        "for triplet in triplets:\n",
        "    citation_graph_store.upsert_triplet(triplet[0], triplet[1], triplet[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yoAKfFvrqPV3"
      },
      "outputs": [],
      "source": [
        "input_string = \"\"\"Graph Store Query:\n",
        "- (tune-a-video, methodological basis)\n",
        "- (tune-a-video, data sources)\"\"\"\n",
        "\n",
        "temp_res = citation_graph_store.query(input_string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMlbpaFQqPV3"
      },
      "outputs": [],
      "source": [
        "temp_res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9AOOIieqPV3"
      },
      "outputs": [],
      "source": [
        "type(temp_res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNrquVasqPV3"
      },
      "outputs": [],
      "source": [
        "len(temp_res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lE6m6ycnqPV3"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.query_engine import KnowledgeGraphQueryEngine\n",
        "from llama_index.core import PromptTemplate\n",
        "from llama_index.core.prompts.prompt_type import PromptType\n",
        "\n",
        "\n",
        "QUERY_KEYWORD_EXTRACT_TEMPLATE_TMPL = (\n",
        "    \"A question is provided below. Given the question, extract all the (paper, relationship) pars\"\n",
        "    \"from the text. Focus on extracting the keywords that we can use \"\n",
        "    \"to best lookup answers to the question. Avoid stopwords.\\n\"\n",
        "    \"Example: (vq-vae, continuation), (video diffusion models, data source)\"\n",
        "    \"---------------------\\n\"\n",
        "    \"{query_str}\\n\"\n",
        "    \"---------------------\\n\"\n",
        "    \"Provide pairs in the following comma-separated format: \"\n",
        "    \"Example 1: (('paper name', 'relationship'), ('paper name 2', 'relationship2')) \\n\"\n",
        "    \"Example 2: (('paper name', 'relationship'))\\n\"\n",
        ")\n",
        "\n",
        "QUERY_KEYWORD_EXTRACT_TEMPLATE = PromptTemplate(\n",
        "    QUERY_KEYWORD_EXTRACT_TEMPLATE_TMPL,\n",
        "    prompt_type=PromptType.QUERY_KEYWORD_EXTRACT,\n",
        ")\n",
        "\n",
        "storage_context = StorageContext.from_defaults(graph_store=citation_graph_store)\n",
        "\n",
        "graph_query_engine = KnowledgeGraphQueryEngine(\n",
        "    storage_context=storage_context,\n",
        "    graph_query_synthesis_prompt=QUERY_KEYWORD_EXTRACT_TEMPLATE,\n",
        "    llm=groq_llm,\n",
        "    verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rj6-F0zvqPV4",
        "outputId": "296578bc-cd5d-4911-e2fa-70c272652233"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;3;33mGraph Store Query:\n",
            "- (tune-a-video, methodological basis)\n",
            "- (tune-a-video, data sources)\n",
            "\u001b[0m\u001b[1;3;33mGraph Store Response:\n",
            "[['The paper \"tune-a-video: one-shot tuning of image diffusion models for text-to-video generation\" is based on the methodology of the paper \"SAVE: Spectral-Shift-Aware Adaptation of Image Diffusion Models for Text-driven Video Editing\". \\nExplanation: The cited works on modifying T2I diffusion models to fit the video generation process provide a methodological basis for the citing paper in text-guided video editing.; The cited work, Tune-A-Video, is used as a basis for adding temporal layers to the T2I model in the citing paper to instil temporal awareness in the video generation process.; The cited work provides a computationally efficient sparse-causal attention mechanism with O((mN ) 2 ) complexity that the citing paper uses to improve the self-attention mechanism in the T2I model and enhance the temporal coherence of video generation.', 'The paper \"tune-a-video: one-shot tuning of image diffusion models for text-to-video generation\" is based on the methodology of the paper \"Gen-L-Video: Multi-Text to Long Video Generation via Temporal Co-Denoising\". \\nExplanation: The cited work provides a pipeline for one-shot tuning Text-to-Video, which the citing paper adopts in their research to implement the one-shot tuning process.; Tune-A-Video finetunes the pretrained text-to-image diffusion model on a video, providing a methodological basis for the citing paper to explore the use of finetuning in video generation.; The cited work on sparse causal attention may have been a foundational method for the sparse causal attention mechanism used in the citing paper to generate long videos with improved consistency between initial and subsequent frames.; The cited work A-Video is used as a method to achieve more precise layout generation in video editing, as it is applied in the citing paper to provide a more general approach to the problem of video editing.', 'The paper \"tune-a-video: one-shot tuning of image diffusion models for text-to-video generation\" is based on the methodology of the paper \"A Systematic Survey of Prompt Engineering on Vision-Language Foundation Models\". \\nExplanation: The cited work, Tune-A-Video, offers a one-shot tuning strategy for text-video generation that the citing paper builds upon in their study of T2I models and T2V.', 'The paper \"tune-a-video: one-shot tuning of image diffusion models for text-to-video generation\" provides data for the paper \"Gen-L-Video: Multi-Text to Long Video Generation via Temporal Co-Denoising\". \\nExplanation: The cited work, the TGVE competition, is the source of the video dataset used in the evaluation of the method described in the citing paper.; The cited work is the source of the videos used in the evaluation of the method presented in the citing paper.', 'The paper \"tune-a-video: one-shot tuning of image diffusion models for text-to-video generation\" provides data for the paper \"CoDeF: Content Deformation Fields for Temporally Consistent Video Processing\". \\nExplanation: The cited work, Tune-A-Video, is used as a data source for obtaining the segmentation of video frames in the citing paper.', 'The paper \"tune-a-video: one-shot tuning of image diffusion models for text-to-video generation\" provides data for the paper \"Instruct-Video2Avatar: Video-to-Avatar Generation with Instructions\". \\nExplanation: The cited work by Wu et al. provides a dataset of pre-trained text-to-image models and attention modules that the citing paper utilizes in their research to synthesize videos of realistic scenes.']]\n",
            "\u001b[0m\u001b[1;3;32mFinal Response: Based on the provided context, the paper \"tune-a-video\" relies on several methodological approaches and utilizes various data sources. \n",
            "\n",
            "**Methodological basis:**\n",
            "\n",
            "- **Adapting T2I diffusion models:** The paper adapts existing methods for text-to-image diffusion models to handle video generation, enhancing temporal awareness.\n",
            "- **One-shot tuning:** A novel approach is proposed to fine-tune the model on a single video, eliminating the need for extensive training data.\n",
            "- **Sparse causal attention:** This mechanism improves the self-attention process in the T2I model, leading to more coherent video generation.\n",
            "\n",
            "**Data sources:**\n",
            "\n",
            "- **TGVE competition:** Provides a dataset of videos used to evaluate the proposed method.\n",
            "- **Gen-L-Video:** Offers a pipeline for generating long videos from text descriptions.\n",
            "- **CoDeF:** Dataset used for obtaining video frame segmentation.\n",
            "- **Dataset of pre-trained models:** Data from Wu et al. is utilized to synthesize realistic videos.\n",
            "\u001b[0m"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "Based on the provided context, the paper \"tune-a-video\" relies on several methodological approaches and utilizes various data sources. \n",
              "\n",
              "**Methodological basis:**\n",
              "\n",
              "- **Adapting T2I diffusion models:** The paper adapts existing methods for text-to-image diffusion models to handle video generation, enhancing temporal awareness.\n",
              "- **One-shot tuning:** A novel approach is proposed to fine-tune the model on a single video, eliminating the need for extensive training data.\n",
              "- **Sparse causal attention:** This mechanism improves the self-attention process in the T2I model, leading to more coherent video generation.\n",
              "\n",
              "**Data sources:**\n",
              "\n",
              "- **TGVE competition:** Provides a dataset of videos used to evaluate the proposed method.\n",
              "- **Gen-L-Video:** Offers a pipeline for generating long videos from text descriptions.\n",
              "- **CoDeF:** Dataset used for obtaining video frame segmentation.\n",
              "- **Dataset of pre-trained models:** Data from Wu et al. is utilized to synthesize realistic videos."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 97,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Markdown(graph_query_engine.query(\"what are the methodological basis and data sources used in the paper 'tune-a-video'?\").response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBkK_DjLqPV4"
      },
      "source": [
        "# **3. Basic Data Science Assistant**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJ2PmbnIqPV4"
      },
      "source": [
        "The next part of our chatbot project involves building a Data Science bot, which will be specifically designed to assist users in navigating complex data science queries and analyses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-54dMa-qPV4"
      },
      "source": [
        "## **3.1 Download Wikipedia Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbPjxtCXqPV4"
      },
      "source": [
        "For data science questions, I will use the source from wikipedia. Wikipedia is a valuable resource for addressing data science questions due to its extensive and regularly updated content. The platform is managed by a global community of volunteers who contribute and scrutinize information, ensuring that entries are comprehensive and often cited with reliable sources. For data science topics, which can be complex and multifaceted, Wikipedia provides accessible explanations that are ideal for both beginners and professionals looking to refresh their knowledge. The articles often cover fundamental concepts, methodologies, and the historical context of topics, making it a balanced starting point for deeper research."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Byl1S76zqPV4"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Pre-compile the regular expression pattern for better performance\n",
        "BRACES_PATTERN = re.compile(r'\\{.*?\\}|\\}')\n",
        "\n",
        "def remove_braces_and_content(text):\n",
        "    \"\"\"Remove all occurrences of curly braces and their content from the given text\"\"\"\n",
        "    return BRACES_PATTERN.sub('', text)\n",
        "\n",
        "def clean_string(input_string):\n",
        "    \"\"\"Clean the input string.\"\"\"\n",
        "\n",
        "    # Remove extra spaces by splitting the string by spaces and joining back together\n",
        "    cleaned_string = ' '.join(input_string.split())\n",
        "\n",
        "    # Remove consecutive carriage return characters until there are no more consecutive occurrences\n",
        "    cleaned_string = re.sub(r'\\r+', '\\r', cleaned_string)\n",
        "\n",
        "    # Remove all occurrences of curly braces and their content from the cleaned string\n",
        "    cleaned_string = remove_braces_and_content(cleaned_string)\n",
        "\n",
        "    # Return the cleaned string\n",
        "    return cleaned_string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0g07c6e0qPV5"
      },
      "outputs": [],
      "source": [
        "def extract_wikipedia_pages(wiki_wiki, category_name):\n",
        "    \"\"\"Extract all references from a category on Wikipedia\"\"\"\n",
        "\n",
        "    # Get the Wikipedia page corresponding to the provided category name\n",
        "    category = wiki_wiki.page(\"Category:\" + category_name)\n",
        "\n",
        "    # Initialize an empty list to store page titles\n",
        "    pages = []\n",
        "\n",
        "    # Check if the category exists\n",
        "    if category.exists():\n",
        "        # Iterate through each article in the category and append its title to the list\n",
        "        for article in category.categorymembers.values():\n",
        "            pages.append(article.title)\n",
        "\n",
        "    # Return the list of page titles\n",
        "    return pages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGxvoAXTqPV5"
      },
      "outputs": [],
      "source": [
        "import wikipediaapi\n",
        "from tqdm import tqdm\n",
        "\n",
        "def get_wikipedia_pages(categories):\n",
        "    \"\"\"Retrieve Wikipedia pages from a list of categories and extract their content\"\"\"\n",
        "\n",
        "    # Create a Wikipedia object\n",
        "    wiki_wiki = wikipediaapi.Wikipedia('Kaggle Data Science Assistant with Gemma', 'en')\n",
        "\n",
        "    # Initialize lists to store explored categories and Wikipedia pages\n",
        "    explored_categories = []\n",
        "    wikipedia_pages = []\n",
        "\n",
        "    # Iterate through each category\n",
        "    print(\"- Processing Wikipedia categories:\")\n",
        "    for category_name in categories:\n",
        "        print(f\"\\tExploring {category_name} on Wikipedia\")\n",
        "\n",
        "        # Get the Wikipedia page corresponding to the category\n",
        "        category = wiki_wiki.page(\"Category:\" + category_name)\n",
        "\n",
        "        # Extract Wikipedia pages from the category and extend the list\n",
        "        wikipedia_pages.extend(extract_wikipedia_pages(wiki_wiki, category_name))\n",
        "\n",
        "        # Add the explored category to the list\n",
        "        explored_categories.append(category_name)\n",
        "\n",
        "    # Extract subcategories and remove duplicate categories\n",
        "    categories_to_explore = [item.replace(\"Category:\", \"\") for item in wikipedia_pages if \"Category:\" in item]\n",
        "    wikipedia_pages = list(set([item for item in wikipedia_pages if \"Category:\" not in item]))\n",
        "\n",
        "    # Explore subcategories recursively\n",
        "    while categories_to_explore:\n",
        "        category_name = categories_to_explore.pop()\n",
        "        print(f\"\\tExploring {category_name} on Wikipedia\")\n",
        "\n",
        "        # Extract more references from the subcategory\n",
        "        more_refs = extract_wikipedia_pages(wiki_wiki, category_name)\n",
        "\n",
        "        # Iterate through the references\n",
        "        for ref in more_refs:\n",
        "            # Check if the reference is a category\n",
        "            if \"Category:\" in ref:\n",
        "                new_category = ref.replace(\"Category:\", \"\")\n",
        "                # Add the new category to the explored categories list\n",
        "                if new_category not in explored_categories:\n",
        "                    explored_categories.append(new_category)\n",
        "            else:\n",
        "                # Add the reference to the Wikipedia pages list\n",
        "                if ref not in wikipedia_pages:\n",
        "                    wikipedia_pages.append(ref)\n",
        "\n",
        "    # Initialize a list to store extracted texts\n",
        "    extracted_texts = []\n",
        "\n",
        "    # Iterate through each Wikipedia page\n",
        "    print(\"- Processing Wikipedia pages:\")\n",
        "    for page_title in tqdm(wikipedia_pages, total=len(wikipedia_pages)):\n",
        "        # Get the Wikipedia page\n",
        "        page = wiki_wiki.page(page_title)\n",
        "\n",
        "        # Append the page title and summary to the extracted texts list\n",
        "        if len(page.summary) > len(page.title):\n",
        "            extracted_texts.append(page.title + \" : \" + clean_string(page.summary))\n",
        "\n",
        "        # Iterate through the sections in the page\n",
        "        for section in page.sections:\n",
        "            # Append the page title and section text to the extracted texts list\n",
        "            if len(section.text) > len(page.title):\n",
        "                extracted_texts.append(page.title + \" : \" + clean_string(section.text))\n",
        "\n",
        "    # Return the extracted texts\n",
        "    return extracted_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRivXJb-qPV7",
        "outputId": "65fc7b86-7b31-44b4-be23-e0ac25a4eb78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "- Processing Wikipedia categories:\n",
            "\tExploring Computer security on Wikipedia\n",
            "\tExploring Data_science on Wikipedia\n",
            "\tExploring Data scientists on Wikipedia\n",
            "\tExploring Computer security stubs on Wikipedia\n",
            "\tExploring Works about computer security on Wikipedia\n",
            "\tExploring Trusted computing on Wikipedia\n",
            "\tExploring Computer security standards on Wikipedia\n",
            "\tExploring Software obfuscation on Wikipedia\n",
            "\tExploring Computer security software on Wikipedia\n",
            "\tExploring Security vulnerability databases on Wikipedia\n",
            "\tExploring Computer security qualifications on Wikipedia\n",
            "\tExploring Computer security procedures on Wikipedia\n",
            "\tExploring Information privacy on Wikipedia\n",
            "\tExploring People associated with computer security on Wikipedia\n",
            "\tExploring Computer security organizations on Wikipedia\n",
            "\tExploring Computer network security on Wikipedia\n",
            "\tExploring Computer security models on Wikipedia\n",
            "\tExploring Mobile security on Wikipedia\n",
            "\tExploring IT risk management on Wikipedia\n",
            "\tExploring Internet leaks on Wikipedia\n",
            "\tExploring InfoSec Twitter on Wikipedia\n",
            "\tExploring Computer security hardware on Wikipedia\n",
            "\tExploring Computer forensics on Wikipedia\n",
            "\tExploring Financial cryptography on Wikipedia\n",
            "\tExploring Computer security exploits on Wikipedia\n",
            "\tExploring Electronic identification on Wikipedia\n",
            "\tExploring Data security on Wikipedia\n",
            "\tExploring Cyber Security by country on Wikipedia\n",
            "\tExploring Cryptography on Wikipedia\n",
            "\tExploring Control flow integrity on Wikipedia\n",
            "\tExploring Computer surveillance on Wikipedia\n",
            "\tExploring Computer security conferences on Wikipedia\n",
            "\tExploring Computational trust on Wikipedia\n",
            "\tExploring Cloud infrastructure attacks and failures on Wikipedia\n",
            "\tExploring Computer access control on Wikipedia\n",
            "\tExploring Computer security by country on Wikipedia\n",
            "- Processing Wikipedia pages:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████| 1912/1912 [10:23<00:00,  3.07it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 8776 Wikipedia pages\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "categories = [\"Computer security\", \"Data_science\"]\n",
        "extracted_texts = get_wikipedia_pages(categories)\n",
        "print(\"Found\", len(extracted_texts), \"Wikipedia pages\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aE_eVGYqqPV8"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import Document\n",
        "\n",
        "wiki_documents = [Document(text=extracted_text, doc_id=str(i)) for i, extracted_text in enumerate(extracted_texts)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fv02McTCqPV8"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex, Settings\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "import chromadb\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "from llama_index.core import StorageContext\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bgdo2WKzqPV8",
        "outputId": "d2ef62ad-0f06-4b36-90f0-975c9a6c2eea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLM is explicitly disabled. Using MockLLM.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "Settings.llm = None # Set this to none to make the index only do retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOdpeq51qPV8",
        "outputId": "68c3ac5d-118c-4d7e-efb1-6c6d3d4d537f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
            "Collecting protobuf==3.19.0\n",
            "  Downloading protobuf-3.19.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (806 bytes)\n",
            "Downloading protobuf-3.19.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25h\u001b[33mDEPRECATION: torch-tensorrt 1.1.0a0 has a non-standard dependency specifier torch>=1.10.0+cu113<1.11.0. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of torch-tensorrt or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.3\n",
            "    Uninstalling protobuf-4.25.3:\n",
            "      Successfully uninstalled protobuf-4.25.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf 21.12.0a0+293.g0930f712e6 requires numba>=0.53.1, but you have numba 0.53.0 which is incompatible.\n",
            "googleapis-common-protos 1.63.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 3.19.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-3.19.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# !pip install protobuf==3.19.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "xRZ-FbT0qPV9",
        "outputId": "57f2483a-9af3-4df6-85f8-dda1fe9265eb",
        "colab": {
          "referenced_widgets": [
            "021b02cbd42a40e5a54d7059bde1d8bb",
            "e8a3cd43a68c46ffaae59e189bfc335a",
            "57eeabb93bcc4f8f8d1972243f9bc5ce",
            "76607be3be3141e4949f95bf073eef4b",
            "bb8c113bc83e4656a25901e604473dc0",
            "d63c73657ae34c7d990ab5db15430638"
          ]
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/torch/cuda/__init__.py:141: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11060). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
            "  return torch._C._cuda_getDeviceCount() > 0\n",
            "/opt/conda/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/opt/conda/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "021b02cbd42a40e5a54d7059bde1d8bb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e8a3cd43a68c46ffaae59e189bfc335a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "57eeabb93bcc4f8f8d1972243f9bc5ce",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "76607be3be3141e4949f95bf073eef4b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bb8c113bc83e4656a25901e604473dc0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d63c73657ae34c7d990ab5db15430638",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "\n",
        "# # loads BAAI/bge-small-en\n",
        "# # embed_model = HuggingFaceEmbedding()\n",
        "\n",
        "# # loads BAAI/bge-small-en-v1.5\n",
        "# embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AoYTjFxqPV9"
      },
      "outputs": [],
      "source": [
        "chroma_client = chromadb.PersistentClient(path=\"/home/mahita/fyp/input/wiki\")\n",
        "chroma_collection = chroma_client.get_or_create_collection(\"wiki_assitant\")\n",
        "\n",
        "# Create vector store\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWDA5jO3qPV9"
      },
      "outputs": [],
      "source": [
        "index = VectorStoreIndex.from_documents(\n",
        "    wiki_documents, storage_context=storage_context, embed_model=embed_model, show_progress=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fgh26LEmqPV9"
      },
      "source": [
        "## **3.2 Loading from vector store**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOtmjtJhqPV-"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex, Settings\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "import chromadb\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "from llama_index.core import StorageContext\n",
        "import torch\n",
        "\n",
        "\n",
        "Settings.llm = None # Set this to none to make the index only do retrieval\n",
        "device_type = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\", cache_folder=\"../models\", device=device_type) # must be the same as the previous stage\n",
        "\n",
        "chroma_client = chromadb.PersistentClient(path=\"/home/mahita/fyp/input/wiki\")\n",
        "chroma_collection = chroma_client.get_or_create_collection(\"wiki_assistant\")\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "# load the vectorstore\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "ds_index = VectorStoreIndex.from_vector_store(vector_store, storage_context=storage_context, embed_model=embed_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5U5DaWiLqPV-"
      },
      "outputs": [],
      "source": [
        "data_science_query_engine = ds_index.as_query_engine(\n",
        "    similarity_top_k=5,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFi3AVpsqPV-"
      },
      "outputs": [],
      "source": [
        "print(data_science_query_engine.query(\"What is machine learning\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JA55YafqPV-"
      },
      "source": [
        "## **3.3 Adding a reranker**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NOCb_7KqPV-"
      },
      "source": [
        "Adding a reranker could be beneficial for this task, as the retrieved results often include noise and unrelated context, which can complicate the generation process. But what is a reranker, and how are they different from normal retriever?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LHANdEgqPV_"
      },
      "source": [
        "When a retriever pulls information from the vector store, it's a bit like casting a wide net – you end up with a lot of catches, but not all of them are the fish you're after. Some pieces of context can be way off the mark, leading us down the wrong path. That's where reranking comes into play. Think of reranking as a second round of scrutiny, a fine-tuning of sorts. After the initial haul from the vector search, reranking steps in to sift through the catch, reorganizing the order or ranking of the items (in this case, the documents we've retrieved) based on more specific criteria. It's like making sure the best, most relevant pieces of information are right at the top, ready for us to use. This extra step helps ensure that what we're working with is as relevant and useful as possible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACP4VaaJqPV_"
      },
      "source": [
        "But how are rerankers different from our initial retriever?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "id": "q4GXGr42qPV_"
      },
      "outputs": [],
      "source": [
        "Image(\"/kaggle/input/gemmarag-figures/biencoder.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IljMpZd8qPWA"
      },
      "source": [
        "The conventional embedding model adheres to the Bi-Encoder paradigm, wherein embeddings for source documents are precomputed. During the query phase, the model generates an embedding for the user's query and then calculates the Cosine Similarity score across our database to identify the most relevant documents.\n",
        "\n",
        "For the reranking process, it is essential to input both the source documents and the query concurrently into the model. This allows the model to evaluate the similarity between the two entities. This approach can be considerably time-intensive, as it lacks the advantage of precomputed data. However, the potential for enhanced accuracy is substantial. Therefore, the reranking process is reserved for the top documents initially retrieved by the Bi-Encoder, ensuring a balance between efficiency and precision in the document selection process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsbJlZdgqPWA"
      },
      "source": [
        "**Reranking Cheatsheet**: Here is a useful reranking cheatsheet, originally in this [tweet](https://twitter.com/bclavie/status/1765312881120153659/photo/1). Thanks [@bclavie](https://twitter.com/bclavie)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "id": "SESXjLaDqPWB"
      },
      "outputs": [],
      "source": [
        "Image(\"/kaggle/input/gemmarag-figures/GH-ms_HWcAEYWou.jpeg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1yxCilgqPWB"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
        "\n",
        "rerank_postprocessor = SentenceTransformerRerank(\n",
        "    model='mixedbread-ai/mxbai-rerank-xsmall-v1',\n",
        "    top_n=3, # number of nodes after re-ranking,\n",
        "    keep_retrieval_score=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0rdtp7UqPWB"
      },
      "outputs": [],
      "source": [
        "# re-define our query engine\n",
        "data_science_query_engine = ds_index.as_query_engine(\n",
        "    similarity_top_k=10,\n",
        "    llm=None,\n",
        "    node_postprocessors=[rerank_postprocessor],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhMTLEcuqPWB"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import ChatPromptTemplate, PromptTemplate\n",
        "from llama_index.core.llms import ChatMessage, MessageRole\n",
        "\n",
        "# system_prompt = \"\"\"\n",
        "# You are an expert data science Q&A system\n",
        "# Answer the query using the provided context information, use prior knowledge if needed.\n",
        "# Some rules to follow:\n",
        "# 1. Never directly reference the given context in your answer.\n",
        "# 2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.\n",
        "\n",
        "# \"\"\"\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "You are an expert data science Q&A system\n",
        "Answer the query using the provided context information, use prior knowledge if needed.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "user_prompt = \"\"\"\n",
        "Context information is below.\n",
        "---------------------\n",
        "{context_str}\n",
        "---------------------\n",
        "Given the context information, answer the data science question, use prior knowledge if needed.\n",
        "Query: {query_str}\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "message_template = [\n",
        "    ChatMessage(content=system_prompt, role=MessageRole.SYSTEM),\n",
        "    ChatMessage(content=user_prompt, role=MessageRole.USER)\n",
        "]\n",
        "chat_prompt = ChatPromptTemplate(message_templates=message_template)\n",
        "\n",
        "data_science_query_engine.update_prompts(\n",
        "    {\"response_synthesizer:text_qa_template\": chat_prompt}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2FZke7GqPWC"
      },
      "outputs": [],
      "source": [
        "print(data_science_query_engine.query(\"Tell me about machine learning\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tTnNAnKqPWC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FCYqlhAqPWC"
      },
      "outputs": [],
      "source": [
        "# # re-define our query engine\n",
        "# data_science_query_engine = ds_index.as_query_engine(\n",
        "#     similarity_top_k=10,\n",
        "#     llm=groq_llm,\n",
        "#     node_postprocessors=[rerank_postprocessor],\n",
        "# )\n",
        "\n",
        "# data_science_query_engine.update_prompts(\n",
        "#     {\"response_synthesizer:text_qa_template\": chat_prompt}\n",
        "# )\n",
        "# print(data_science_query_engine.query(\"Tell me about machine learning\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzlUR2VPqPWD"
      },
      "source": [
        "### Citation Query Engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWyx2g-YqPWD"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.query_engine import CitationQueryEngine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZhcYFquqPWD"
      },
      "outputs": [],
      "source": [
        "citation_query_engine = CitationQueryEngine.from_args(\n",
        "    ds_index,\n",
        "    similarity_top_k=3,\n",
        "    # here we can control how granular citation sources are, the default is 512\n",
        "    citation_chunk_size=1024,\n",
        "#     llm=groq_llm,\n",
        "    node_postprocessors=[rerank_postprocessor],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnzA5LR6qPWD"
      },
      "outputs": [],
      "source": [
        "print(citation_query_engine.query(\"Tell me about machine learning\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwjryYSdqPWD"
      },
      "source": [
        "# **4. Python Code Assistant**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ji_lRKJbqPWD"
      },
      "source": [
        "To handle coding questions, we need a specialized Large Language Model (LLM) designed for programming. Google recently launched CodeGemma, a coding-specific LLM available in two versions: a 2 billion and a 7 billion parameter model. Let’s explore how CodeGemma can help us tackle complex coding challenges more effectively. Whether you're a beginner or an experienced developer, this tool could be a valuable addition to our toolkit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yn-D2SwQqPWD"
      },
      "source": [
        "## **4.1 Loading the CodeGemma LLM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-12T21:14:22.009980Z",
          "iopub.status.busy": "2024-05-12T21:14:22.009562Z",
          "iopub.status.idle": "2024-05-12T21:15:21.120809Z",
          "shell.execute_reply": "2024-05-12T21:15:21.119964Z",
          "shell.execute_reply.started": "2024-05-12T21:14:22.009940Z"
        },
        "id": "mXyljSMzqPWD",
        "outputId": "974f2aa5-4c63-47df-be23-35d9b7c187f2",
        "colab": {
          "referenced_widgets": [
            "e9a40319fb21480297ddc29d67cfcc32",
            "bbc38b9fc1f24e3e8f97580cf957ed5a",
            "16bb94f183544eb9b8b3f6308c075e8e",
            "b3e428599fd64c07bf81d09c448d5ffb",
            "0b9cf856af7e4d68aebb601be58ca77a",
            "6911152aff7a4d8fb4de4f3ea90d2a6b",
            "d2aa9d423b45427484425310906a9703"
          ]
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-05-12 21:14:34.201928: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-05-12 21:14:34.202057: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-05-12 21:14:34.365617: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e9a40319fb21480297ddc29d67cfcc32",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/1.13k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth: Fast Gemma patching release 2024.5\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.2.2+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = FALSE. Xformers = 0.0.25.post1. FA = False.\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bbc38b9fc1f24e3e8f97580cf957ed5a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/5.57G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "16bb94f183544eb9b8b3f6308c075e8e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b3e428599fd64c07bf81d09c448d5ffb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/40.5k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0b9cf856af7e4d68aebb601be58ca77a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6911152aff7a4d8fb4de4f3ea90d2a6b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/555 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d2aa9d423b45427484425310906a9703",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/codegemma-7b-it-bnb-4bit\", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-12T21:15:21.124620Z",
          "iopub.status.busy": "2024-05-12T21:15:21.123566Z",
          "iopub.status.idle": "2024-05-12T21:15:23.545228Z",
          "shell.execute_reply": "2024-05-12T21:15:23.544147Z",
          "shell.execute_reply.started": "2024-05-12T21:15:21.124556Z"
        },
        "id": "eSxrCKvhqPWE",
        "outputId": "c80026db-fef7-4580-b5ef-eaa976f2e91e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.10/site-packages/pydantic/_internal/_fields.py:151: UserWarning: Field \"model_id\" has conflict with protected namespace \"model_\".\n",
            "\n",
            "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "\n",
        "max_seq_length = 1024 # Choose any! We auto support RoPE Scaling internally!\n",
        "code_llm = HuggingFaceLLM(model=model, tokenizer=tokenizer, context_window=2048, max_new_tokens=max_seq_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPSL7x5SqPWE"
      },
      "source": [
        "## **4.2 Define a code Assistant**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-12T21:15:23.546993Z",
          "iopub.status.busy": "2024-05-12T21:15:23.546626Z",
          "iopub.status.idle": "2024-05-12T21:15:23.618197Z",
          "shell.execute_reply": "2024-05-12T21:15:23.617417Z",
          "shell.execute_reply.started": "2024-05-12T21:15:23.546957Z"
        },
        "id": "en9ekzqrqPWE"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.query_engine import CustomQueryEngine\n",
        "from llama_index.core import PromptTemplate\n",
        "\n",
        "code_qa_prompt = PromptTemplate(\n",
        "    \"You are a code assistant powered by a large language model. \"\n",
        "    \"Your task is to help users solve programming problems, provide code examples, \"\n",
        "    \"explain programming concepts, and debug code. \"\n",
        "    \"Write python code to answer the question bellow\\n\"\n",
        "    \"---------------------\\n\"\n",
        "    \"{query_str}\\n\"\n",
        "    \"---------------------\\n\"\n",
        "    \"Answer: \"\n",
        ")\n",
        "\n",
        "class CodeQueryEngine(CustomQueryEngine):\n",
        "    llm: HuggingFaceLLM\n",
        "    qa_prompt: PromptTemplate\n",
        "    def custom_query(self , query_str: str):\n",
        "        response = self.llm.complete(\n",
        "            self.qa_prompt.format(query_str=query_str)\n",
        "        )\n",
        "#         program = extract_program(output.text)\n",
        "#         executor = PythonExecutor(get_answer_from_stdout=True)\n",
        "\n",
        "#         exe_result = executor.apply(program)\n",
        "#         response = response.text + f\"\\n\\n**Execution Output**:```output\\n\\n{exe_result[0]}\\n\\n```\\n\"\n",
        "        return str(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-12T21:22:46.768325Z",
          "iopub.status.busy": "2024-05-12T21:22:46.767431Z",
          "iopub.status.idle": "2024-05-12T21:22:46.772776Z",
          "shell.execute_reply": "2024-05-12T21:22:46.771805Z",
          "shell.execute_reply.started": "2024-05-12T21:22:46.768290Z"
        },
        "id": "TkoSITarqPWE"
      },
      "outputs": [],
      "source": [
        "code_query_engine = CodeQueryEngine(llm = code_llm, qa_prompt=code_qa_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-12T21:17:24.851433Z",
          "iopub.status.busy": "2024-05-12T21:17:24.850720Z",
          "iopub.status.idle": "2024-05-12T21:17:39.961780Z",
          "shell.execute_reply": "2024-05-12T21:17:39.960745Z",
          "shell.execute_reply.started": "2024-05-12T21:17:24.851400Z"
        },
        "id": "14bqYmkWqPWE",
        "outputId": "e86e82ea-af9d-4374-ac71-c832c4e480b3"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "\n",
              "```python\n",
              "import torch\n",
              "import torch.nn as nn\n",
              "import torch.optim as optim\n",
              "\n",
              "# Define the model\n",
              "model = nn.Linear(10, 1)\n",
              "\n",
              "# Define the loss function\n",
              "loss_fn = nn.MSELoss()\n",
              "\n",
              "# Define the optimizer\n",
              "optimizer = optim.Adam(model.parameters())\n",
              "\n",
              "# Training loop\n",
              "for epoch in range(num_epochs):\n",
              "    for batch in data_loader:\n",
              "        # Get the input and target data\n",
              "        inputs, targets = batch\n",
              "\n",
              "        # Clear the gradients\n",
              "        optimizer.zero_grad()\n",
              "\n",
              "        # Forward pass\n",
              "        outputs = model(inputs)\n",
              "\n",
              "        # Calculate the loss\n",
              "        loss = loss_fn(outputs, targets)\n",
              "\n",
              "        # Backward pass\n",
              "        loss.backward()\n",
              "\n",
              "        # Update the weights\n",
              "        optimizer.step()\n",
              "\n",
              "# Print the training loss\n",
              "print(f\"Epoch {epoch+1}: {loss.item()}\")\n",
              "```"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "Markdown(code_query_engine.query(\"Give me code for a basic training loop in pytorch\").response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-12T20:42:46.677397Z",
          "iopub.status.busy": "2024-05-12T20:42:46.676504Z",
          "iopub.status.idle": "2024-05-12T20:43:02.434278Z",
          "shell.execute_reply": "2024-05-12T20:43:02.433122Z",
          "shell.execute_reply.started": "2024-05-12T20:42:46.677357Z"
        },
        "id": "py9ugeA6qPWE",
        "outputId": "0058fa15-0f1f-4eac-89a0-277ba6d250b3"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "\n",
              "```python\n",
              "def binary_search(arr, x):\n",
              "    low = 0\n",
              "    high = len(arr) - 1\n",
              "    mid = 0\n",
              "    \n",
              "    while low <= high:\n",
              "        mid = (high + low) // 2\n",
              "        \n",
              "        if arr[mid] == x:\n",
              "            return mid\n",
              "        elif arr[mid] < x:\n",
              "            low = mid + 1\n",
              "        else:\n",
              "            high = mid - 1\n",
              "    \n",
              "    return -1\n",
              "\n",
              "# Test array\n",
              "arr = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
              "x = 10\n",
              "\n",
              "# Function call\n",
              "result = binary_search(arr, x)\n",
              "\n",
              "# Print result\n",
              "if result != -1:\n",
              "    print(\"Element is present at index\", str(result))\n",
              "else:\n",
              "    print(\"Element is not present in array\")\n",
              "```"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Markdown(code_query_engine.query(\"Give me a binary search code\").response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-12T20:43:02.436355Z",
          "iopub.status.busy": "2024-05-12T20:43:02.436027Z",
          "iopub.status.idle": "2024-05-12T20:43:09.184838Z",
          "shell.execute_reply": "2024-05-12T20:43:09.183782Z",
          "shell.execute_reply.started": "2024-05-12T20:43:02.436328Z"
        },
        "id": "T6glopPxqPWE",
        "outputId": "83a9f30c-bd57-4e09-f348-a8ea7f80e220"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "\n",
              "```python\n",
              "class MyDataset(torch.utils.data.Dataset):\n",
              "    def __init__(self, data, labels):\n",
              "        self.data = data\n",
              "        self.labels = labels\n",
              "\n",
              "    def __len__(self):\n",
              "        return len(self.data)\n",
              "\n",
              "    def __getitem__(self, index):\n",
              "        return self.data[index], self.labels[index]\n",
              "```"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Markdown(code_query_engine.query(\"How can I define a dataset class in pytorch?\").response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8R6iyNgqPWE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUFD5DglqPWF"
      },
      "source": [
        "# **5. Combine all of them together**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Gr4G55AqPWF"
      },
      "source": [
        "How can we combine all of the tools together? The answer is by using the Router Engine. The Router Engine effectively combines various AI tools to address complex tasks. At the core of this engine is a LLM, the LLM assesses the context and specific requirements of a task to determine which AI tool is best suited for the job. This intelligent selection process ensures that the most appropriate resources are utilized, enhancing the efficiency and accuracy of the outputs. By integrating different capabilities seamlessly, the Router Engine facilitates a more dynamic and versatile approach to solving problems, streamlining workflows, and optimizing performance across diverse applications. This strategy not only maximizes the potential of individual tools but also leverages their collective strengths in a cohesive and intelligent manner."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "id": "gZkkxDpYqPWF"
      },
      "outputs": [],
      "source": [
        "Image(\"/kaggle/input/gemmarag-figures/RouterEngine.jpg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H11Tc16mqPWF"
      },
      "source": [
        "## **5.1 Define Router Engine**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-12T21:17:40.728609Z",
          "iopub.status.busy": "2024-05-12T21:17:40.728218Z",
          "iopub.status.idle": "2024-05-12T21:17:40.762869Z",
          "shell.execute_reply": "2024-05-12T21:17:40.762111Z",
          "shell.execute_reply.started": "2024-05-12T21:17:40.728560Z"
        },
        "id": "e5-VJlJFqPWF",
        "outputId": "bc6ee4c2-47ed-4358-dc8b-b97428a44be9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/pydantic/_internal/_fields.py:160: UserWarning: Field \"model_id\" has conflict with protected namespace \"model_\".\n",
            "\n",
            "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core.query_engine import RouterQueryEngine\n",
        "from llama_index.core.selectors import LLMSingleSelector\n",
        "from llama_index.core.tools import QueryEngineTool\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from llama_index.llms.groq import Groq\n",
        "# from unsloth import FastLanguageModel\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-12T21:17:41.060234Z",
          "iopub.status.busy": "2024-05-12T21:17:41.059926Z",
          "iopub.status.idle": "2024-05-12T21:17:41.082746Z",
          "shell.execute_reply": "2024-05-12T21:17:41.081657Z",
          "shell.execute_reply.started": "2024-05-12T21:17:41.060209Z"
        },
        "id": "1ekOKeK0qPWF"
      },
      "outputs": [],
      "source": [
        "from typing import Any, Dict, List, Optional, Sequence, cast\n",
        "\n",
        "from llama_index.core.base.base_selector import (\n",
        "    BaseSelector,\n",
        "    SelectorResult,\n",
        "    SingleSelection,\n",
        ")\n",
        "from llama_index.core.output_parsers.base import StructuredOutput\n",
        "from llama_index.core.output_parsers.selection import Answer, SelectionOutputParser\n",
        "from llama_index.core.prompts.mixin import PromptDictType\n",
        "from llama_index.core.prompts.prompt_type import PromptType\n",
        "from llama_index.core.schema import QueryBundle\n",
        "from llama_index.core.selectors.prompts import (\n",
        "    DEFAULT_MULTI_SELECT_PROMPT_TMPL,\n",
        "    DEFAULT_SINGLE_SELECT_PROMPT_TMPL,\n",
        "    MultiSelectPrompt,\n",
        "    SingleSelectPrompt,\n",
        ")\n",
        "from llama_index.core.service_context import ServiceContext\n",
        "from llama_index.core.service_context_elements.llm_predictor import (\n",
        "    LLMPredictorType,\n",
        ")\n",
        "from llama_index.core.settings import Settings, llm_from_settings_or_context\n",
        "from llama_index.core.tools.types import ToolMetadata\n",
        "from llama_index.core.types import BaseOutputParser\n",
        "\n",
        "\n",
        "def _build_choices_text(choices: Sequence[ToolMetadata]) -> str:\n",
        "    \"\"\"Convert sequence of metadata to enumeration text.\"\"\"\n",
        "    texts: List[str] = []\n",
        "    for ind, choice in enumerate(choices):\n",
        "        text = \" \".join(choice.description.splitlines())\n",
        "        text = f\"({ind + 1}) {text}\"  # to one indexing\n",
        "        texts.append(text)\n",
        "    return \"\\n\\n\".join(texts)\n",
        "\n",
        "\n",
        "def _structured_output_to_selector_result(output: Any) -> SelectorResult:\n",
        "    \"\"\"Convert structured output to selector result.\"\"\"\n",
        "    structured_output = cast(StructuredOutput, output)\n",
        "    answers = cast(List[Answer], structured_output.parsed_output)\n",
        "\n",
        "    # adjust for zero indexing\n",
        "    selections = [\n",
        "        SingleSelection(index=answer.choice - 1, reason=answer.reason)\n",
        "        for answer in answers\n",
        "    ]\n",
        "    return SelectorResult(selections=selections)\n",
        "\n",
        "\n",
        "class LLMSingleSelectorCustom(BaseSelector):\n",
        "    \"\"\"LLM single selector.\n",
        "\n",
        "    LLM-based selector that chooses one out of many options.\n",
        "\n",
        "    Args:\n",
        "        LLM (LLM): An LLM.\n",
        "        prompt (SingleSelectPrompt): A LLM prompt for selecting one out of many options.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        llm: LLMPredictorType,\n",
        "        prompt: SingleSelectPrompt,\n",
        "    ) -> None:\n",
        "        self._llm = llm\n",
        "        self._prompt = prompt\n",
        "\n",
        "        if self._prompt.output_parser is None:\n",
        "            raise ValueError(\"Prompt should have output parser.\")\n",
        "\n",
        "    @classmethod\n",
        "    def from_defaults(\n",
        "        cls,\n",
        "        llm: Optional[LLMPredictorType] = None,\n",
        "        service_context: Optional[ServiceContext] = None,\n",
        "        prompt_template_str: Optional[str] = None,\n",
        "        output_parser: Optional[BaseOutputParser] = None,\n",
        "    ) -> \"LLMSingleSelector\":\n",
        "        # optionally initialize defaults\n",
        "        llm = llm or llm_from_settings_or_context(Settings, service_context)\n",
        "        prompt_template_str = prompt_template_str or DEFAULT_SINGLE_SELECT_PROMPT_TMPL\n",
        "        output_parser = output_parser or SelectionOutputParser()\n",
        "\n",
        "        # construct prompt\n",
        "        prompt = SingleSelectPrompt(\n",
        "            template=prompt_template_str,\n",
        "            output_parser=output_parser,\n",
        "            prompt_type=PromptType.SINGLE_SELECT,\n",
        "        )\n",
        "        return cls(llm, prompt)\n",
        "\n",
        "    def _get_prompts(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get prompts.\"\"\"\n",
        "        return {\"prompt\": self._prompt}\n",
        "\n",
        "    def _update_prompts(self, prompts: PromptDictType) -> None:\n",
        "        \"\"\"Update prompts.\"\"\"\n",
        "        if \"prompt\" in prompts:\n",
        "            self._prompt = prompts[\"prompt\"]\n",
        "\n",
        "    def _select(\n",
        "        self, choices: Sequence[ToolMetadata], query: QueryBundle\n",
        "    ) -> SelectorResult:\n",
        "        # prepare input\n",
        "        choices_text = _build_choices_text(choices)\n",
        "\n",
        "        # predict\n",
        "        prediction = self._llm.predict(\n",
        "            prompt=self._prompt,\n",
        "            num_choices=len(choices),\n",
        "            context_list=choices_text,\n",
        "            query_str=query.query_str,\n",
        "        )\n",
        "        print(prediction)\n",
        "\n",
        "        # parse output\n",
        "        assert self._prompt.output_parser is not None\n",
        "        parse = self._prompt.output_parser.parse(prediction)\n",
        "        return _structured_output_to_selector_result(parse)\n",
        "\n",
        "    async def _aselect(\n",
        "        self, choices: Sequence[ToolMetadata], query: QueryBundle\n",
        "    ) -> SelectorResult:\n",
        "        # prepare input\n",
        "        choices_text = _build_choices_text(choices)\n",
        "\n",
        "        # predict\n",
        "        prediction = await self._llm.apredict(\n",
        "            prompt=self._prompt,\n",
        "            num_choices=len(choices),\n",
        "            context_list=choices_text,\n",
        "            query_str=query.query_str,\n",
        "        )\n",
        "\n",
        "        # parse output\n",
        "        assert self._prompt.output_parser is not None\n",
        "        parse = self._prompt.output_parser.parse(prediction)\n",
        "        return _structured_output_to_selector_result(parse)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-12T21:18:04.876103Z",
          "iopub.status.busy": "2024-05-12T21:18:04.875713Z",
          "iopub.status.idle": "2024-05-12T21:18:04.881817Z",
          "shell.execute_reply": "2024-05-12T21:18:04.880897Z",
          "shell.execute_reply.started": "2024-05-12T21:18:04.876073Z"
        },
        "id": "CqcaJr_5qPWG"
      },
      "outputs": [],
      "source": [
        "Settings.llm = Groq(model=\"gemma-7b-it\", api_key=groq_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-12T21:18:08.366286Z",
          "iopub.status.busy": "2024-05-12T21:18:08.365425Z",
          "iopub.status.idle": "2024-05-12T21:18:09.135419Z",
          "shell.execute_reply": "2024-05-12T21:18:09.134401Z",
          "shell.execute_reply.started": "2024-05-12T21:18:08.366250Z"
        },
        "id": "oRtfBL_wqPWG"
      },
      "outputs": [],
      "source": [
        "paper_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=paper_query_engine,\n",
        "    description=\"Useful for search for papers\",\n",
        ")\n",
        "\n",
        "graph_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=graph_query_engine,\n",
        "    description=\"Useful for answering  between papers\",\n",
        ")\n",
        "\n",
        "# ds_tool = QueryEngineTool.from_defaults(\n",
        "#     query_engine=data_science_query_engine,\n",
        "#     description=\"Useful for answering data science concepts\",\n",
        "# )\n",
        "\n",
        "# code_tool = QueryEngineTool.from_defaults(\n",
        "#     query_engine=code_query_engine,\n",
        "#     description=\"Useful for answering coding questions\",\n",
        "# )\n",
        "\n",
        "query_engine = RouterQueryEngine(\n",
        "    selector=LLMSingleSelectorCustom.from_defaults(),\n",
        "    query_engine_tools=[\n",
        "        paper_tool,\n",
        "#         ds_tool,\n",
        "        graph_tool,\n",
        "#         code_tool\n",
        "    ],\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujUYbwN4qPWG"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    print(query_engine.query(\"How to do paper research about video diffusion?\"))\n",
        "except:\n",
        "    pass print(\"404 Not Found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-12T20:45:43.470623Z",
          "iopub.status.busy": "2024-05-12T20:45:43.469855Z",
          "iopub.status.idle": "2024-05-12T20:45:59.552805Z",
          "shell.execute_reply": "2024-05-12T20:45:59.551882Z",
          "shell.execute_reply.started": "2024-05-12T20:45:43.470588Z"
        },
        "id": "rivycSKdqPWG",
        "outputId": "bcdf5cf3-5d0a-4d5b-c485-c3cc6e63caf5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "```json\n",
            "[\n",
            "    {\n",
            "        \"choice\": 1,\n",
            "        \"reason\": \"The summary states that the function is useful for answering coding questions, which aligns with the request for a binary search function code.\"\n",
            "    }\n",
            "]\n",
            "```\n",
            "\u001b[1;3;38;5;200mSelecting query engine 0: The summary states that the function is useful for answering coding questions, which aligns with the request for a binary search function code..\n",
            "\u001b[0m\n",
            "```python\n",
            "def binary_search(arr, x):\n",
            "    low = 0\n",
            "    high = len(arr) - 1\n",
            "    mid = 0\n",
            "    \n",
            "    while low <= high:\n",
            "        mid = (high + low) // 2\n",
            "        \n",
            "        if arr[mid] == x:\n",
            "            return mid\n",
            "        elif arr[mid] < x:\n",
            "            low = mid + 1\n",
            "        else:\n",
            "            high = mid - 1\n",
            "    \n",
            "    return -1\n",
            "\n",
            "# Test array\n",
            "arr = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
            "x = 10\n",
            "\n",
            "# Function call\n",
            "result = binary_search(arr, x)\n",
            "\n",
            "# Print result\n",
            "if result != -1:\n",
            "    print(\"Element is present at index\", str(result))\n",
            "else:\n",
            "    print(\"Element is not present in array\")\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    print(query_engine.query(\"Give me a code for a binary search function?\"))\n",
        "except:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-12T21:23:18.907765Z",
          "iopub.status.busy": "2024-05-12T21:23:18.907139Z",
          "iopub.status.idle": "2024-05-12T21:23:33.665855Z",
          "shell.execute_reply": "2024-05-12T21:23:33.665016Z",
          "shell.execute_reply.started": "2024-05-12T21:23:18.907734Z"
        },
        "id": "ApZrzbvfqPWG",
        "outputId": "7c8c6c8c-763c-4dd7-f995-d86cf1b62e39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "```json\n",
            "[\n",
            "    {\n",
            "        \"choice\": 1,\n",
            "        \"reason\": \"The summary (1) explicitly mentions usefulness for answering coding questions, which aligns with the request for a binary search function code.\"\n",
            "    }\n",
            "]\n",
            "```\n",
            "\u001b[1;3;38;5;200mSelecting query engine 0: The summary (1) explicitly mentions usefulness for answering coding questions, which aligns with the request for a binary search function code..\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "res = query_engine.query(\"Give me a code for a binary search function?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-12T21:23:33.667700Z",
          "iopub.status.busy": "2024-05-12T21:23:33.667397Z",
          "iopub.status.idle": "2024-05-12T21:23:33.672760Z",
          "shell.execute_reply": "2024-05-12T21:23:33.671759Z",
          "shell.execute_reply.started": "2024-05-12T21:23:33.667674Z"
        },
        "id": "184UZxQrqPWG",
        "outputId": "3a32f311-1e8b-482a-b6a4-1cc96dd9d4af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "```python\n",
            "def binary_search(arr, x):\n",
            "    low = 0\n",
            "    high = len(arr) - 1\n",
            "    mid = 0\n",
            "    \n",
            "    while low <= high:\n",
            "        mid = (high + low) // 2\n",
            "        \n",
            "        if arr[mid] == x:\n",
            "            return mid\n",
            "        elif arr[mid] < x:\n",
            "            low = mid + 1\n",
            "        else:\n",
            "            high = mid - 1\n",
            "    \n",
            "    return -1\n",
            "\n",
            "# Test array\n",
            "arr = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
            "x = 10\n",
            "\n",
            "# Function call\n",
            "result = binary_search(arr, x)\n",
            "\n",
            "# Print result\n",
            "if result != -1:\n",
            "    print(\"Element is present at index\", str(result))\n",
            "else:\n",
            "    print(\"Element is not present in array\")\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "print(res.response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmW359I3qPWG"
      },
      "outputs": [],
      "source": [
        "output ="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHyyEQzKqPWG"
      },
      "source": [
        "# **Bonus Section: Building a simple UI**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-12T21:24:29.430631Z",
          "iopub.status.busy": "2024-05-12T21:24:29.430220Z",
          "iopub.status.idle": "2024-05-12T21:24:44.144890Z",
          "shell.execute_reply": "2024-05-12T21:24:44.143788Z",
          "shell.execute_reply.started": "2024-05-12T21:24:29.430594Z"
        },
        "id": "VMzvUDevqPWG",
        "outputId": "8b55b7ac-1e21-4b26-b461-ef97343e4af4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        }
      ],
      "source": [
        "%%capture\n",
        "!pip install --upgrade gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHg2afitqPWH"
      },
      "outputs": [],
      "source": [
        "def query_func(message, history):\n",
        "\n",
        "    return query_engine.query(message).response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-12T21:34:29.127794Z",
          "iopub.status.busy": "2024-05-12T21:34:29.127377Z",
          "iopub.status.idle": "2024-05-12T21:34:29.132693Z",
          "shell.execute_reply": "2024-05-12T21:34:29.131576Z",
          "shell.execute_reply.started": "2024-05-12T21:34:29.127764Z"
        },
        "id": "EcOEzcOLqPWH"
      },
      "outputs": [],
      "source": [
        "output = \"I'm sarvesh\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-12T21:34:31.854750Z",
          "iopub.status.busy": "2024-05-12T21:34:31.853819Z",
          "iopub.status.idle": "2024-05-12T21:34:31.859263Z",
          "shell.execute_reply": "2024-05-12T21:34:31.858261Z",
          "shell.execute_reply.started": "2024-05-12T21:34:31.854717Z"
        },
        "id": "kzLYcH3bqPWH"
      },
      "outputs": [],
      "source": [
        "def query_func(message, history):\n",
        "\n",
        "#     res = query_engine.query(message)\n",
        "#     output = res.response\n",
        "    ans = output\n",
        "\n",
        "    return ans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-12T21:34:35.759166Z",
          "iopub.status.busy": "2024-05-12T21:34:35.758819Z",
          "iopub.status.idle": "2024-05-12T21:34:35.763651Z",
          "shell.execute_reply": "2024-05-12T21:34:35.762541Z",
          "shell.execute_reply.started": "2024-05-12T21:34:35.759140Z"
        },
        "id": "73aKSx4GqPWH"
      },
      "outputs": [],
      "source": [
        "# msg = \"Hello\"\n",
        "# history = \"Hola\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-12T21:34:38.525887Z",
          "iopub.status.busy": "2024-05-12T21:34:38.525482Z",
          "iopub.status.idle": "2024-05-12T21:34:38.531447Z",
          "shell.execute_reply": "2024-05-12T21:34:38.530208Z",
          "shell.execute_reply.started": "2024-05-12T21:34:38.525855Z"
        },
        "id": "QUHkvK-XqPWI"
      },
      "outputs": [],
      "source": [
        "# print(query_func(msg, history))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-12T21:36:19.295635Z",
          "iopub.status.busy": "2024-05-12T21:36:19.295204Z",
          "iopub.status.idle": "2024-05-12T21:36:25.966521Z",
          "shell.execute_reply": "2024-05-12T21:36:25.965372Z",
          "shell.execute_reply.started": "2024-05-12T21:36:19.295597Z"
        },
        "id": "bJPUe9KwqPWJ",
        "outputId": "71c0a65f-3841-472f-ba5c-e103e5cb8ec5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on local URL:  http://127.0.0.1:7861\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on public URL: https://c3cdc9789fe77be447.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://c3cdc9789fe77be447.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 106,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "```json\n",
            "[\n",
            "    {\n",
            "        \"choice\": 1,\n",
            "        \"reason\": \"The first choice suggests it's relevant for searching for papers related to the research topic.\"\n",
            "    }\n",
            "]\n",
            "```\n",
            "\u001b[1;3;38;5;200mSelecting query engine 0: The first choice suggests it's relevant for searching for papers related to the research topic..\n",
            "\u001b[0m```json\n",
            "[\n",
            "    {\n",
            "        \"choice\": 1,\n",
            "        \"reason\": \"The first choice suggests that it is useful for searching for papers, which aligns with the question's intent of finding papers related to the topic.\"\n",
            "    }\n",
            "]\n",
            "```\n",
            "\u001b[1;3;38;5;200mSelecting query engine 0: The first choice suggests that it is useful for searching for papers, which aligns with the question's intent of finding papers related to the topic..\n",
            "\u001b[0m```json\n",
            "[\n",
            "    {\n",
            "        \"choice\": 1,\n",
            "        \"reason\": \"The first choice mentions usefulness for searching papers, which aligns with the question's intent of finding papers related to the topic.\"\n",
            "    }\n",
            "]\n",
            "```\n",
            "\u001b[1;3;38;5;200mSelecting query engine 0: The first choice mentions usefulness for searching papers, which aligns with the question's intent of finding papers related to the topic..\n",
            "\u001b[0m```json\n",
            "[\n",
            "    {\n",
            "        \"choice\": 1,\n",
            "        \"reason\": \"The summary (1) mentions usefulness for searching papers, which aligns with the need to find information about the methodological implementation of the paper in question.\"\n",
            "    }\n",
            "]\n",
            "```\n",
            "\u001b[1;3;38;5;200mSelecting query engine 0: The summary (1) mentions usefulness for searching papers, which aligns with the need to find information about the methodological implementation of the paper in question..\n",
            "\u001b[0m```json\n",
            "[\n",
            "    {\n",
            "        \"choice\": 1,\n",
            "        \"reason\": \"The summary mentions the use of multiple sources and information extraction, which aligns with the search for papers related to paper search.\"\n",
            "    }\n",
            "]\n",
            "```\n",
            "\u001b[1;3;38;5;200mSelecting query engine 0: The summary mentions the use of multiple sources and information extraction, which aligns with the search for papers related to paper search..\n",
            "\u001b[0m```json\n",
            "[\n",
            "    {\n",
            "        \"choice\": 1,\n",
            "        \"reason\": \"The summary mentions the use of multiple sources and multihop reasoning, which aligns with the search for papers related to information extraction and summarization.\"\n",
            "    }\n",
            "]\n",
            "```\n",
            "\u001b[1;3;38;5;200mSelecting query engine 0: The summary mentions the use of multiple sources and multihop reasoning, which aligns with the search for papers related to information extraction and summarization..\n",
            "\u001b[0m```json\n",
            "[\n",
            "    {\n",
            "        \"choice\": 1,\n",
            "        \"reason\": \"The first choice mentions usefulness for searching papers, which aligns with the question's focus on finding papers on multihop reasoning.\"\n",
            "    }\n",
            "]\n",
            "```\n",
            "\u001b[1;3;38;5;200mSelecting query engine 0: The first choice mentions usefulness for searching papers, which aligns with the question's focus on finding papers on multihop reasoning..\n",
            "\u001b[0m```json\n",
            "[\n",
            "    {\n",
            "        \"choice\": 1,\n",
            "        \"reason\": \"The first choice suggests that it is relevant for searching for papers related to the topic of fake news detection.\"\n",
            "    }\n",
            "]\n",
            "```\n",
            "\u001b[1;3;38;5;200mSelecting query engine 0: The first choice suggests that it is relevant for searching for papers related to the topic of fake news detection..\n",
            "\u001b[0m```json\n",
            "[\n",
            "    {\n",
            "        \"choice\": 1,\n",
            "        \"reason\": \"The first choice mentions usefulness for searching papers, which aligns with the question's intent of finding papers related to the topic.\"\n",
            "    }\n",
            "]\n",
            "```\n",
            "\u001b[1;3;38;5;200mSelecting query engine 0: The first choice mentions usefulness for searching papers, which aligns with the question's intent of finding papers related to the topic..\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "gr.ChatInterface(query_func).launch(share=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAyHydAHqPWJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZqJIMNYqPWJ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "databundleVersionId": 7669720,
          "sourceId": 64148,
          "sourceType": "competition"
        },
        {
          "datasetId": 4783808,
          "sourceId": 8100696,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 4783560,
          "sourceId": 8119694,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 612177,
          "sourceId": 8387387,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30683,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}